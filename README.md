# Gradients Analysis 

Created by: 
- Diego Sabajo: https://www.linkedin.com/in/diego-sabajo
- Eitan Sprejer: https://www.linkedin.com/in/eitan-sprejer-574380204
- Matias Zabaljauregui: https://www.linkedin.com/in/zabaljauregui
- Oliver Morris: https://www.linkedin.com/in/olimoz

## Overview
Here's an **overview** section for your `README.md`:

---

## Overview

**Explainable Diagnostic Assistant** is an interactive tool designed to refine and analyze model outputs in a transparent and interpretable way. Built on top of **Goodfire's API** for advanced model evaluation and steering, this project focuses on improving trust and usability in AI systems, particularly in healthcare diagnostics. 

### Key Features:
1. **Chat Interface with Predefined Prompts:**
   - Users can select from predefined medical prompts.
   - Responses are generated by an LLM and compared against ground truth labels for evaluation.

2. **Feature Steering and Analysis:**
   - Adjust feature importance dynamically using sliders.
   - Visualize the impact of these adjustments on model performance and hallucination probability.

3. **Explainability with Decision Trees:**
   - Incorporates decision tree classifiers for feature-based explanations.
   - Allows users to understand which features influenced the model's response.

4. **Model Evaluation:**
   - Analyzes model predictions for correctness.
   - Displays visual metrics for hallucination probability, feature usefulness, and more.

5. **Streamlined UI/UX:**
   - Developed using **Streamlit** for a user-friendly web interface.
   - Includes a sidebar for navigation and dynamic visualizations.

### Objectives:
- Enhance transparency in model decision-making using **mechanistic interpretability** techniques.
- Enable **clinicians** and **AI researchers** to interact with and understand AI systems effectively.
- Support feature-based debugging and refinement to reduce hallucinations and improve diagnostic relevance.

### Technologies and Frameworks:
- **Goodfire API:** Advanced tools for feature steering and latent space analysis.
- **Streamlit:** Interactive and responsive UI for AI model evaluation.
- **Python Libraries:** NumPy, Pandas, Matplotlib for data handling and visualization.

This project exemplifies the intersection of AI safety and interpretability, providing a practical prototype for trustworthy AI applications in healthcare diagnostics.

## Installation

### Prerequisites
Ensure you have the following installed:
- Python 3.8 or higher
- `pip` (Python package installer)

### Setting Up the Environment
1. **Clone the repository**
    ```bash
   cd project-folder
   git clone https://github.com/Mechanistic-Interpretability-Hackathon/Mech-Interp.git
   cd Mech-Interp

2. **Create virtual environment**
    ```bash
    python3 -m venv .venv

3. **Activate virtual environment**
    ```bash
    Mac: python3 -m venv .venv
    Windows: .venv\Scripts\activate

4. **Install necessary packages**
    ```bash
    pip install -r requirements.txt

### Run the project
    ```bash
    streamlit run src/app.py