{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "We'll be providing the LLM with a Pubmed ID, eg '35020963', and then asking it for two words from the title of that document.\n",
    "\n",
    "If it gets both words correct then we classify the PubMedID as 'known', \n",
    "if it gets both wrong then it is 'unknown', \n",
    "and if it gets one correct and one wrong then it is 'half-known'. \n",
    "\n",
    "As per Ferrando et al 2024, We ignore the half-known Pubmed ID's and seek the features for the known and unknown Pubmed ID's only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oliver/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/oliver/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "ds = load_dataset(\"openlifescienceai/Med-HALT\", \"IR_pmid2title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MedHALT \"IR_pmid2title\" example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PMID': 35020963,\n",
       " 'Title': 'The relationship between nitrous oxide sedation and psychosocial factors in the pediatric outpatient setting',\n",
       " 'Authors': 'Moharrami M, Ali S, Dick BD, Moeinvaziri F, Amin M.',\n",
       " 'First Author': 'Moharrami M',\n",
       " 'DOI': '10.1111/pan.14395',\n",
       " 'Publication Year': 2022,\n",
       " 'Abstract': \"BACKGROUND: Moderate sedation using nitrous oxide (N\\nAIMS: This study aimed to examine pediatric dental patients' behaviors while undergoing N\\nMETHODS: In this within-subject observational study, participants received 40% N\\nRESULTS: In 80 children with a mean age of 7.2 (2.2) years, administration of N\\nCONCLUSION: In children sedated with N\",\n",
       " 'url': 'https://ncbi.nlm.nih.gov/pubmed/35020963',\n",
       " 'id': '96892a50-abbb-42a8-8ff2-93c406d593fe',\n",
       " 'source_title': None,\n",
       " 'source_abstract': None,\n",
       " 'pubmed_data_type': 'real_data',\n",
       " 'is_paper_exists': 'Yes'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_long = \"\"\"You are an intelligent retrieval system that uses state-of-the-art natural language processing and \n",
    "information retrieval techniques to search for and fetch the title of a specific scientific article. \n",
    "You take Pubmed Research Paper PMID as input and retrieve two key words from the title of that scientific article by searching through your memory.\n",
    "The keywords should be nouns or verbs, not prepositions or adjectives.\n",
    "\n",
    "The response should be returned in JSON format with three keys 'key_word_1', 'key_word_2', 'is_unknown'\n",
    "The values of key_word_1 and key_word_2 should be the two key words from the corresponding Pubmed Paper title. \n",
    "If the article is not found or the correct title is unknown, respond with 'Unknown' to indicate the absence of the requested information, don't try to make up an answer.\n",
    "Hence, the value of is_unknown should be 'True' if the article is not found or the correct title is unknown, 'False' otherwise.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_short = \"\"\"Do not discuss, simply give a keyword from the title of the Pubmed Research Paper which has PMID = 35020963\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_short = \"\"\"Do not discuss, simply give a keyword from the title of the Pubmed Research Paper which has PMID = 25335925\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PMID': 25335925,\n",
       " 'Title': 'Pwp1 is required for the differentiation potential of mouse embryonic stem cells through regulating Stat3 signaling',\n",
       " 'Authors': 'Shen J, Jia W, Yu Y, Chen J, Cao X, Du Y, Zhang X, Zhu S, Chen W, Xi J, Wei T, Wang G, Yuan D, Duan T, Jiang C, Kang J.',\n",
       " 'First Author': 'Shen J',\n",
       " 'DOI': '10.1002/stem.1876',\n",
       " 'Publication Year': 2015,\n",
       " 'Abstract': 'Leukemia inhibitory factor/Stat3 signaling is critical for maintaining the self-renewal and differentiation potential of mouse embryonic stem cells (mESCs). However, the upstream effectors of this pathway have not been clearly defined. Here, we show that periodic tryptophan protein 1 (Pwp1), a WD-40 repeat-containing protein associated with histone H4 modification, is required for the exit of mESCs from the pluripotent state into all lineages. Knockdown (KD) of Pwp1 does not affect mESC proliferation, self-renewal, or apoptosis. However, KD of Pwp1 impairs the differentiation potential of mESCs both in vitro and in vivo. PWP1 chromatin immunoprecipitation-seq results revealed that the PWP1-occupied regions were marked with significant levels of H4K20me3. Moreover, Pwp1 binds to sites in the upstream region of Stat3. KD of Pwp1 decreases the level of H4K20me3 in the upstream region of Stat3 gene and upregulates the expression of Stat3. Furthermore, Pwp1 KD mESCs recover their differentiation potential through suppressing the expression of Stat3 or inhibiting the tyrosine phosphorylation of STAT3. Together, our results suggest that Pwp1 plays important roles in the differentiation potential of mESCs.',\n",
       " 'url': 'https://ncbi.nlm.nih.gov/pubmed/25335925',\n",
       " 'id': 'b90c503b-12af-49bb-b569-fb953ce2d6e1',\n",
       " 'source_title': None,\n",
       " 'source_abstract': None,\n",
       " 'pubmed_data_type': 'real_data',\n",
       " 'is_paper_exists': 'Yes'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PMID of this and other articles was entered into GPT4o, Claude 3.5, Llama 3.1 405B and llama 8B.\n",
    "\n",
    "NONE of these models knew any of the PMID's. The task of NER (named entity recognition) of PMID's is too difficult and cannot be used for our research.\n",
    "\n",
    "Instead, we trun to the NCBI dataset, which focusses on Named Entity Recognition (NER) of diseases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other datasets, NCBI\n",
    "\n",
    "#### Dataset Card for NCBI Disease\n",
    "\n",
    "[Available on Huggingface](https://huggingface.co/datasets/ncbi/ncbi_disease)\n",
    "\n",
    "This dataset contains the disease name and concept annotations of the NCBI disease corpus, a collection of 793 PubMed abstracts fully annotated at the mention and concept level to serve as a research resource for the biomedical natural language processing community.\n",
    "\n",
    "Instances of the dataset contain an array of tokens, ner_tags and an id. An example of an instance of the dataset:\n",
    "\n",
    "{\n",
    "  'tokens': ['Identification', 'of', 'APC2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.'],\n",
    "  'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0],\n",
    "  'id': '0'\n",
    "}\n",
    "\n",
    "Data Fields\n",
    "id: Sentence identifier.\n",
    "tokens: Array of tokens composing a sentence.\n",
    "ner_tags: Array of tags, where 0 indicates no disease mentioned, 1 signals the first token of a disease and 2 the subsequent disease tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records in train dataset:  5433\n",
      "Records in test dataset:  941\n",
      "Records in val dataset:  924\n"
     ]
    }
   ],
   "source": [
    "ds_ncbi = load_dataset(\"ncbi/ncbi_disease\")\n",
    "ds_ncbi_train = list(ds_ncbi['train'])\n",
    "ds_ncbi_testi = list(ds_ncbi['test'])\n",
    "ds_ncbi_valid = list(ds_ncbi['validation'])\n",
    "print(\"Records in train dataset: \", len(ds_ncbi_train))\n",
    "print(\"Records in test dataset: \", len(ds_ncbi_testi))\n",
    "print(\"Records in val dataset: \", len(ds_ncbi_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['Identification',\n",
       "  'of',\n",
       "  'APC2',\n",
       "  ',',\n",
       "  'a',\n",
       "  'homologue',\n",
       "  'of',\n",
       "  'the',\n",
       "  'adenomatous',\n",
       "  'polyposis',\n",
       "  'coli',\n",
       "  'tumour',\n",
       "  'suppressor',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0]}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view example\n",
    "ds_ncbi_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we use this Dataset for Entity Recognition\n",
    "\n",
    "We process the data retinaing only those quotes with entity names which are at least 3 tokens long.\n",
    "The ner_tags identify the entity's tokens in the quote.\n",
    "Later we can mask the final two tokens of the entity name, to see if the LLM knows what they are.\n",
    "\n",
    "Using the above example:\n",
    "'Identification of APC2, a homologue of the adenomatous polyposis coli tumour suppressor .'\n",
    "\n",
    "The entity is: \"adenomatous polyposis coli tumour\", as identified by the ner_tags.\n",
    "\n",
    "To test the LLM's entity recognition we create a masked quote:\n",
    "'Identification of APC2, a homologue of the adenomatous polyposis [Word A] [Word B] suppressor .'\n",
    "Then ask the LLM for A and B, thus scoring those which it knows both words a 'known', no words as 'unknown' and one word as inbetween.\n",
    "As per Ferrando et al.\n",
    "\n",
    "\n",
    "In the following code chunk we filter the dataset to retain only those records with at least 3 entity tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_sequences(dataset, min_tokens=2):\n",
    "    \"\"\"\n",
    "    Filter records containing disease mentions of at least min_tokens non-punctuation tokens \n",
    "    and add positions of the last two tokens in the final valid sequence.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list): List of dictionaries containing 'tokens' and 'ner_tags'\n",
    "        min_tokens (int): Minimum number of non-punctuation tokens\n",
    "        \n",
    "    Returns:\n",
    "        list: Filtered dataset with new 'final_positions' key for valid records\n",
    "    \"\"\"\n",
    "    import string\n",
    "    \n",
    "    def is_punctuation(token):\n",
    "        \"\"\"Helper function to check if a token consists only of punctuation.\"\"\"\n",
    "        return all(char in string.punctuation for char in token)\n",
    "    \n",
    "    filtered_dataset = []\n",
    "    \n",
    "    for record in dataset:\n",
    "        tokens = record['tokens']\n",
    "        ner_tags = record['ner_tags']\n",
    "        current_sequence = []\n",
    "        sequences = []\n",
    "        \n",
    "        # Find all non-zero sequences\n",
    "        for i, (token, tag) in enumerate(zip(tokens, ner_tags)):\n",
    "            if tag != 0:\n",
    "                current_sequence.append(i)\n",
    "                # If we have enough positions, check if we have at least min_tokens non-punctuation tokens\n",
    "                if len(current_sequence) >= min_tokens:\n",
    "                    non_punct_count = sum(1 for pos in current_sequence \n",
    "                                        if not is_punctuation(tokens[pos]))\n",
    "                    if non_punct_count >= min_tokens and current_sequence not in sequences:\n",
    "                        sequences.append(current_sequence.copy())\n",
    "            elif current_sequence:\n",
    "                current_sequence = []\n",
    "                \n",
    "        # Handle sequence that might end at the last position\n",
    "        if current_sequence:\n",
    "            non_punct_count = sum(1 for pos in current_sequence \n",
    "                                if not is_punctuation(tokens[pos]))\n",
    "            if non_punct_count >= min_tokens and current_sequence not in sequences:\n",
    "                sequences.append(current_sequence)\n",
    "            \n",
    "        # If we found any valid sequences\n",
    "        if sequences:\n",
    "            # Get the last valid sequence\n",
    "            final_sequence = sequences[-1]\n",
    "            \n",
    "            # Create new record with final positions\n",
    "            new_record = record.copy()\n",
    "            new_record['final_positions'] = final_sequence[-2:]\n",
    "            filtered_dataset.append(new_record)\n",
    "            \n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of train dataset:\n",
      "  1964\n",
      "Len of test dataset:\n",
      "  368\n",
      "Len of validation dataset:\n",
      "  366\n",
      "\n",
      "Example:\n",
      " {'id': '25', 'tokens': ['These', 'intersex', 'differences', 'in', 'colorectal', 'cancer', 'risks', 'have', 'implications', 'for', 'screening', 'programmes', 'and', 'for', 'attempts', 'to', 'identify', 'colorectal', 'cancer', 'susceptibility', 'modifiers', '.'], 'ner_tags': [0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0], 'final_positions': [17, 18]}\n"
     ]
    }
   ],
   "source": [
    "ds_ncbi_train_filtered = filter_long_sequences(ds_ncbi_train)\n",
    "print(\"Len of train dataset:\\n \", len(ds_ncbi_train_filtered))\n",
    "\n",
    "ds_ncbi_testi_filtered = filter_long_sequences(ds_ncbi_testi)\n",
    "print(\"Len of test dataset:\\n \", len(ds_ncbi_testi_filtered))\n",
    "\n",
    "ds_ncbi_valid_filtered = filter_long_sequences(ds_ncbi_valid)\n",
    "print(\"Len of validation dataset:\\n \", len(ds_ncbi_valid_filtered))\n",
    "\n",
    "print(\"\\nExample:\\n\",ds_ncbi_train_filtered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test is somewhat small and we don't need validation dataset, so we'll add validation to test.\n",
    "\n",
    "ds_ncbi_testi_filtered = ds_ncbi_testi_filtered + ds_ncbi_valid_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function which masks the last two words of the disease name (both words if there are only two)\n",
    "We will present this to the LLM and ask it to give use the masked words.\n",
    "Two correct words will count as a 'known entity', two incorrect words will count as an 'unknown entity'.\n",
    "Inbetween will count as a 'half-known entity' nd be dismissed form the analysis, as per Ferrando et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_disease_record(record):\n",
    "    \"\"\"\n",
    "    Process a disease record to create a masked sentence and extract relevant information.\n",
    "    Skips punctuation when selecting tokens to mask.\n",
    "    \n",
    "    Args:\n",
    "        record (dict): Dictionary containing 'tokens', 'ner_tags', and 'final_positions'\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (query_sentence, missing_words, disease_name)\n",
    "            - query_sentence: string with masked words\n",
    "            - missing_words: dict of masked words\n",
    "            - disease_name: complete disease mention\n",
    "            Returns empty strings if valid masking not possible.\n",
    "    \"\"\"\n",
    "    id = record['id']\n",
    "    tokens = record['tokens'].copy()\n",
    "    positions = record['final_positions']\n",
    "    \n",
    "    def is_punctuation(token):\n",
    "        return token in {',', '.', ':', ';', '!', '?', '-', '(', ')', '[', ']', '\"', \"'\"}\n",
    "    \n",
    "    # Find valid positions for masking (non-punctuation)\n",
    "    valid_positions = []\n",
    "    current_pos = positions[0]\n",
    "    \n",
    "    # Try to find first valid position\n",
    "    while current_pos < len(tokens) and len(valid_positions) < 1:\n",
    "        if not is_punctuation(tokens[current_pos]):\n",
    "            valid_positions.append(current_pos)\n",
    "        current_pos += 1\n",
    "    \n",
    "    # Try to find second valid position, starting from the next token after first valid position\n",
    "    if valid_positions:\n",
    "        current_pos = valid_positions[0] + 1\n",
    "        while current_pos < len(tokens) and len(valid_positions) < 2:\n",
    "            if not is_punctuation(tokens[current_pos]):\n",
    "                valid_positions.append(current_pos)\n",
    "            current_pos += 1\n",
    "    \n",
    "    # If we couldn't find two valid positions, return empty results\n",
    "    if len(valid_positions) != 2:\n",
    "        return {'id': id, 'query': '', 'missing_words': ['', ''], 'entity_name': ''}\n",
    "    \n",
    "    # Store the missing words before masking\n",
    "    missing_words = [\n",
    "        tokens[valid_positions[0]],\n",
    "        tokens[valid_positions[1]]\n",
    "    ]\n",
    "    \n",
    "    # Replace tokens with placeholders\n",
    "    tokens[valid_positions[0]] = '[Word A]'\n",
    "    tokens[valid_positions[1]] = '[Word B]'\n",
    "    query = ' '.join(tokens)\n",
    "    \n",
    "    # Extract the complete disease name\n",
    "    ner_tags = record['ner_tags']\n",
    "    disease_tokens = []\n",
    "    in_sequence = False\n",
    "    current_sequence = []\n",
    "    \n",
    "    # Find the sequence containing the valid positions\n",
    "    for i, (token, tag) in enumerate(zip(record['tokens'], ner_tags)):\n",
    "        if tag != 0:\n",
    "            in_sequence = True\n",
    "            current_sequence.append(token)\n",
    "        elif in_sequence:\n",
    "            if valid_positions[1] in range(i - len(current_sequence), i):\n",
    "                disease_tokens = current_sequence\n",
    "            current_sequence = []\n",
    "            in_sequence = False\n",
    "            \n",
    "    # Handle sequence at the end of the text\n",
    "    if in_sequence and valid_positions[1] in range(len(ner_tags) - len(current_sequence), len(ner_tags)):\n",
    "        disease_tokens = current_sequence\n",
    "        \n",
    "    entity_name = ' '.join(disease_tokens)\n",
    "    \n",
    "    return {'id':id, 'query':query, 'missing_words': missing_words, 'entity_name':entity_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of train dataset:\n",
      "  1942\n",
      "Len of test dataset:\n",
      "  725\n",
      "\n",
      "Example:\n",
      " {'id': '25', 'query': 'These intersex differences in colorectal cancer risks have implications for screening programmes and for attempts to identify [Word A] [Word B] susceptibility modifiers .', 'missing_words': ['colorectal', 'cancer'], 'entity_name': 'colorectal cancer'}\n"
     ]
    }
   ],
   "source": [
    "ds_ncbi_train_processed = []\n",
    "ds_ncbi_testi_processed = []\n",
    "\n",
    "for record in ds_ncbi_train_filtered:\n",
    "    record_processed = process_disease_record(record)\n",
    "    if record_processed['missing_words'] != ['', '']:\n",
    "        ds_ncbi_train_processed.append(record_processed)\n",
    "print(\"Len of train dataset:\\n \", len(ds_ncbi_train_processed))\n",
    "\n",
    "for record in ds_ncbi_testi_filtered:\n",
    "    record_processed = process_disease_record(record)\n",
    "    if record_processed['missing_words'] != ['', '']:\n",
    "        ds_ncbi_testi_processed.append(record_processed)\n",
    "print(\"Len of test dataset:\\n \", len(ds_ncbi_testi_processed))\n",
    "\n",
    "print(\"\\nExample:\\n\",ds_ncbi_train_processed[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask the LLM to nominate the missing words like this:\n",
    "\n",
    "\"\"\"\n",
    "Without discussion give the two missing words from this sentence; [Word A] and [Word B] as a python list: \n",
    "\n",
    "\"RESULTS A statistically significant excess risk of cancer was seen among the patients with CHH ( standardized incidence ratio 6 . 9 , 95 % confidence interval 2 . 3 to 16 ) , which was mainly attributable to non - [Word A] [Word B] ( standardized incidence ratio 90 , 95 % confidence interval 18 to 264 ) .\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "The LLM then responds:\n",
    "\"\"\"\n",
    "[\"Hodgkin\", \"lymphoma\"]\n",
    "\"\"\"\n",
    "\n",
    "We then execute a function to score the response against expected missing words for the disease (i.e. entity), [\"Hodgkins\", \"lymphoma\"]\n",
    "Note, additional 's', but this is close enough, we use a fuzzy match and we ignore order. \n",
    "The score is the number of correct missing words, 0, 1 or 2.\n",
    "\n",
    "A score of 2 will denote that the LLM knows the entity\n",
    "A score of 0 will denote that the LLM does not know the entity\n",
    "Records scoring 1 will be removed from further analysis\n",
    "\n",
    "NOTE\n",
    "\n",
    "This is inverted to the approach taken by Ferrando et al. They presented the entity to the LLM and asked for two attributes to discover whether the entity is known or unknown. Whereas we present attributes and ask for the indicated entity.\n",
    "\n",
    "This inverted approach has been taken because medical entities (i.e diseases) are complex with many attributes, a large number of which are shared with other entities (diseases). It is a combination of attributes which indicates a single entity (disease).\n",
    "\n",
    "Most medical datasets are in the form of context (i.e. attributes) from which we are asked to propose an entity. This is how the data is useful to medical professionals, so this is the approach we take here.\n",
    "\n",
    "Therefore, we have assumed that the graph of the LLM's knowledge can be read in either direction along each edge. From attributes to entity, or from entity to attributes.\n",
    "\n",
    "This makes our work interestingly different from Ferrando et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_llm_response(llm_response, expected_words, missing_qty=2):\n",
    "    \"\"\"\n",
    "    Score LLM response against expected words using flexible matching rules.\n",
    "    \n",
    "    Args:\n",
    "        llm_response (str): String containing one or two words in any format\n",
    "        expected_words (list): List of expected words (one or two)\n",
    "        missing_qty (int): Number of words expected (1 or 2)\n",
    "        \n",
    "    Returns:\n",
    "        int: Score (0, 1, or 2 for missing_qty=2; 0 or 1 for missing_qty=1) based on number of correct matches\n",
    "    \"\"\"\n",
    "    def clean_word(word):\n",
    "        \"\"\"Convert to singular form and lowercase\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        word = word.lower().strip()\n",
    "        word = word.strip('\"\\'[](),.')  # Remove common punctuation\n",
    "        \n",
    "        word = lemmatizer.lemmatize(word, 'n')\n",
    "        return word\n",
    "    \n",
    "    def words_match(word1, word2):\n",
    "        \"\"\"Check if words approximately match using sequence matcher\"\"\"\n",
    "        word1 = clean_word(word1)\n",
    "        word2 = clean_word(word2)\n",
    "        \n",
    "        ratio = SequenceMatcher(None, word1, word2).ratio()\n",
    "        return ratio > 0.85  # Threshold for approximate matching\n",
    "    \n",
    "    def extract_words(llm_response, missing_qty):\n",
    "        \"\"\"Extract first two words from first sequence in various input formats\"\"\"\n",
    "        # Split on common sequence separators like 'or', 'and', newlines\n",
    "        sequences = re.split(r'\\s+or\\s+|\\s+and\\s+|\\n', llm_response, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Take only the first sequence\n",
    "        first_sequence = sequences[0]\n",
    "        \n",
    "        # Remove any brackets, quotes, etc.\n",
    "        first_sequence = re.sub(r'[\\[\\]\\'\"{},]', ' ', first_sequence)\n",
    "        \n",
    "        # Split on whitespace and filter out empty strings\n",
    "        words = [word.strip() for word in first_sequence.split() if word.strip()]\n",
    "        \n",
    "        # Take first two non-empty words\n",
    "        return words[:missing_qty]\n",
    "    \n",
    "    try:\n",
    "        # Validate missing_qty parameter\n",
    "        if missing_qty not in [1, 2]:\n",
    "            raise ValueError(\"missing_qty must be either 1 or 2\")\n",
    "            \n",
    "        # Validate expected_words length matches missing_qty\n",
    "        if len(expected_words) != missing_qty:\n",
    "            raise ValueError(f\"Expected {missing_qty} words but got {len(expected_words)} words\")\n",
    "        \n",
    "        # Extract words from response string\n",
    "        llm_words = extract_words(llm_response, missing_qty)\n",
    "        \n",
    "        if len(llm_words) < missing_qty:\n",
    "            return 0  # Not enough words provided\n",
    "            \n",
    "        # Count matches (allowing for either order)\n",
    "        matches = 0\n",
    "        used_expected = set()\n",
    "        used_llm = set()\n",
    "        \n",
    "        # Look for matches in any order\n",
    "        for i, llm_word in enumerate(llm_words):\n",
    "            for j, expected_word in enumerate(expected_words):\n",
    "                if j not in used_expected and i not in used_llm:\n",
    "                    if words_match(llm_word, expected_word):\n",
    "                        matches += 1\n",
    "                        used_expected.add(j)\n",
    "                        used_llm.add(i)\n",
    "                        break\n",
    "        \n",
    "        return matches\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our code on real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import goodfire\n",
    "import os\n",
    "\n",
    "api_key ='REDACTED'\n",
    "client = goodfire.Client(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY = 'REDACTED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from together import Together\n",
    "import time\n",
    "\n",
    "def query_to_llm(ds_ncbi_processed, provider=\"goodfire\", missing_qty = 2):\n",
    "    \n",
    "    # Get data\n",
    "    ds_ncbi_completed = []\n",
    "    client_gf = goodfire.Client(api_key)\n",
    "    client_tg = Together(api_key=TOGETHER_API_KEY)\n",
    "\n",
    "    # loop over the data with tqdm progress bar\n",
    "    for record in tqdm(ds_ncbi_processed, desc=\"Processing records\"):\n",
    "    \n",
    "        # Create query with instructions to LLM\n",
    "        if missing_qty == 2:\n",
    "            instruction = \"Without preamble, give the two missing words as indicated by [Word A] and [Word B] in the following sentence. Return your answer as a python list:\\n \"\n",
    "        elif missing_qty == 1:\n",
    "            instruction = \"Without preamble, give the missing word as indicated by [Word A] in the following sentence. Return your answer as a python list:\\n \"\n",
    "        else:\n",
    "            print(\"No such quantity of missing words\")\n",
    "            return\n",
    "            \n",
    "        query = instruction + record['query']\n",
    "\n",
    "        if provider == 'goodfire':\n",
    "            \n",
    "            # Simple rate limiting\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            # Create a non-streaming completion using Goodfire\n",
    "            response = client_gf.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": query}],\n",
    "                model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", # \"meta-llama/Llama-3.3-70B-Instruct\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "            )\n",
    "            llm_response = response.choices[0].message[\"content\"]\n",
    "\n",
    "        else:\n",
    "            # call Together.ai\n",
    "\n",
    "            try:\n",
    "                response = client_tg.chat.completions.create(\n",
    "                    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": query}],\n",
    "                    temperature=0.0,\n",
    "                )\n",
    "                llm_response = response.choices[0].message.content\n",
    "\n",
    "            except Exception as e:\n",
    "                llm_response = ''\n",
    "                print(e)\n",
    "\n",
    "        # Score LLM response\n",
    "        score = score_llm_response(llm_response, expected_words=record['missing_words'], missing_qty=missing_qty)\n",
    "\n",
    "        # Save results\n",
    "        record['llm_response'] = llm_response\n",
    "        record['score'] = score\n",
    "        ds_ncbi_completed.append(record)\n",
    "\n",
    "    return ds_ncbi_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 1942/1942 [20:25<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the TRAIN data we have:\n",
      "Known Entities    : 225\n",
      "Uncertain Entities: 529\n",
      "Unknown Entities  : 1188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds_ncbi_train_completed_tg = query_to_llm(ds_ncbi_train_processed, provider='together')\n",
    "\n",
    "# Scores:\n",
    "Score_2 = sum(1 for d in ds_ncbi_train_completed_tg if d['score'] == 2)\n",
    "Score_1 = sum(1 for d in ds_ncbi_train_completed_tg if d['score'] == 1)\n",
    "Score_0 = sum(1 for d in ds_ncbi_train_completed_tg if d['score'] == 0)\n",
    "\n",
    "print(\"In the TRAIN data we have:\")\n",
    "print(\"Known Entities    :\", Score_2)\n",
    "print(\"Uncertain Entities:\", Score_1)\n",
    "print(\"Unknown Entities  :\", Score_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# save progress with together.ai\n",
    "pd.DataFrame(ds_ncbi_train_completed_tg).to_parquet('../classifier/data_classifier/ds_ncbi_train_completed_tg.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ncbi_train_completed_gf = query_to_llm(ds_ncbi_train_processed, provider='goodfire')\n",
    "\n",
    "# Scores:\n",
    "Score_2 = sum(1 for d in ds_ncbi_train_completed_gf if d['score'] == 2)\n",
    "Score_1 = sum(1 for d in ds_ncbi_train_completed_gf if d['score'] == 1)\n",
    "Score_0 = sum(1 for d in ds_ncbi_train_completed_gf if d['score'] == 0)\n",
    "\n",
    "print(\"In the TRAIN data we have:\")\n",
    "print(\"Known Entities    :\", Score_2)\n",
    "print(\"Uncertain Entities:\", Score_1)\n",
    "print(\"Unknown Entities  :\", Score_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save progress with goodfire\n",
    "pd.DataFrame(ds_ncbi_train_completed_gf.to_parquet('../classifier/data_classifier/ds_ncbi_train_completed_gf.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 725/725 [03:05<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the TEST data we have:\n",
      "Known Entities    : 81\n",
      "Uncertain Entities: 227\n",
      "Unknown Entities  : 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds_ncbi_testi_completed_tg = query_to_llm(ds_ncbi_testi_processed, provider='together')\n",
    "\n",
    "# Scores:\n",
    "Score_2 = sum(1 for d in ds_ncbi_testi_completed_tg if d['score'] == 2)\n",
    "Score_1 = sum(1 for d in ds_ncbi_testi_completed_tg if d['score'] == 1)\n",
    "Score_0 = sum(1 for d in ds_ncbi_testi_completed_tg if d['score'] == 0)\n",
    "\n",
    "print(\"In the TEST data we have:\")\n",
    "print(\"Known Entities    :\", Score_2)\n",
    "print(\"Uncertain Entities:\", Score_1)\n",
    "print(\"Unknown Entities  :\", Score_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save progress\n",
    "pd.DataFrame(ds_ncbi_testi_completed_tg).to_parquet('../classifier/data_classifier/ds_ncbi_testi_completed_tg.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from file\n",
    "import pandas as pd\n",
    "import goodfire\n",
    "import os\n",
    "\n",
    "api_key ='REDACTED'\n",
    "client_gf = goodfire.Client(api_key)\n",
    "variant = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# load from file\n",
    "ds_ncbi_train_completed_tg = pd.read_parquet('../classifier/data_classifier/ds_ncbi_train_completed_tg.parquet')\n",
    "ds_ncbi_testi_completed_tg = pd.read_parquet('../classifier/data_classifier/ds_ncbi_testi_completed_tg.parquet')\n",
    "\n",
    "# return to format previously used, not pandas, list of dicts\n",
    "ds_ncbi_train_completed_tg = ds_ncbi_train_completed_tg.to_dict('records')\n",
    "ds_ncbi_testi_completed_tg = ds_ncbi_testi_completed_tg.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load from file\n",
    "ds_ncbi_train_completed_tg = pd.read_parquet('../classifier/data_classifier/ds_ncbi_train_completed_tg.parquet')\n",
    "ds_ncbi_testi_completed_tg = pd.read_parquet('../classifier/data_classifier/ds_ncbi_testi_completed_tg.parquet')\n",
    "\n",
    "# return to format previously used, not pandas, list of dicts\n",
    "ds_ncbi_train_completed_tg = ds_ncbi_train_completed_tg.to_dict('records')\n",
    "ds_ncbi_testi_completed_tg = ds_ncbi_testi_completed_tg.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beware the Contrastive Trick\n",
    "\n",
    "In the contrastive trick we subtract the feature activiations of a 'known' entity from the feature activations of an unknown entity. We can sum or average the differences over a batch of such pairs, getting the most different features between each set. Negative features are those which indicate hallucination, positive features indicate non-hallucination.We can repeat this batched process over the entire data set and union the resulting features from each batch into a single distinct list of feature differences.\n",
    "\n",
    "BUT\n",
    "\n",
    "As per Ferrando et al, we want the features which are activated almost exclusively in one condition (known vs. unknown) but rarely or  never activate in the other condition. We are interested in exclusivity not scale.\n",
    "\n",
    "Simply summing the per-batch differences in activations is not well-suited to detecting exclusive features. Summation over subsets will highlight features that have large aggregate differences. This will include the exclusive features we seek, but will not isolate them. We wont know which if the differing features are alsmost exclusive to one class or other.\n",
    "\n",
    "So\n",
    "\n",
    "We follow the approach by Ferrando et al, see p4 of 'Do I know this entity?': \n",
    "\n",
    "```\n",
    "For each latent, we obtain the fraction of the time that it is active (i.e. has a value greater than zero) on known and unknown entities respectively...Then, we take the difference, obtaining the latent separation scores\n",
    "```\n",
    "\n",
    "In order to achieve this, we need to get the activated features from each and every prompt, we can't use the contrastive trick as per above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def get_feature_activations(client, variant, entity_names, k=50):\n",
    "    \"\"\"\n",
    "    Simple synchronous version that processes one example at a time\n",
    "    \"\"\"\n",
    "    feature_activations = []\n",
    "    feature_library = set()\n",
    "\n",
    "    for entity_name in tqdm(entity_names, desc=\"Processing entity_names\"):\n",
    "        try:\n",
    "            inspector = client.features.inspect(\n",
    "                [{\"role\": \"user\", \"content\": entity_name}],\n",
    "                model=variant,\n",
    "                #features=features\n",
    "            )\n",
    "\n",
    "            #  prepare to collect activated features\n",
    "            entity_features=[]\n",
    "\n",
    "            for activation in inspector.top(k=k):\n",
    "\n",
    "                # keep our own library of all unique features and their id's\n",
    "                feature_library.add((activation.feature.uuid, activation.feature.label))\n",
    "\n",
    "                # record the feature id and its activation value for this entity \n",
    "                entity_features.append({'uuid':activation.feature.uuid, 'activation':activation.activation})\n",
    "\n",
    "            feature_activations.append({'entity_name': entity_name, 'entity_features': entity_features})\n",
    "            time.sleep(0.5)  # Simple rate limiting\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process example: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return feature_activations, feature_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of known entities =  81\n",
      "No. of unknown entities =  417\n"
     ]
    }
   ],
   "source": [
    "# The known and unknown entities are as follows:\n",
    "\n",
    "entities_known   = [key['entity_name'] for key in ds_ncbi_testi_completed_tg if key['score'] == 2]\n",
    "print(\"No. of known entities = \", len(entities_known))\n",
    "\n",
    "entities_unknown = [key['entity_name'] for key in ds_ncbi_testi_completed_tg if key['score'] == 0]\n",
    "print(\"No. of unknown entities = \", len(entities_unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entity_names:   0%|          | 0/81 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entity_names: 100%|██████████| 81/81 [03:29<00:00,  2.59s/it]\n",
      "Processing entity_names: 100%|██████████| 417/417 [17:18<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the activations for the known and unknown entities\n",
    "\n",
    "feature_activations_known,   feature_library_known   = get_feature_activations(client_gf, variant, entities_known,   k=100)\n",
    "feature_activations_unknown, feature_library_unknown = get_feature_activations(client_gf, variant, entities_unknown, k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have two datasets in python, each is a list of records. \n",
    "The first list is for 'known' entities and the second list is for 'unknown' entities.\n",
    "Below is an example record: \n",
    "\n",
    "```\n",
    "{'entity_name': 'breast and ovarian cancer',\n",
    " 'entity_features': [\n",
    "  {'uuid': UUID('cd092e67-147c-4e1d-92ab-4c5011e15cd1'), 'activation': 3},\n",
    "  {'uuid': UUID('5e4deda8-16c3-4306-851d-e3c92f45b874'), 'activation': 3},\n",
    "  {'uuid': UUID('c4e5975c-4fd7-4726-af27-b14180f03672'), 'activation': 3},\n",
    "  {'uuid': UUID('c1d7b972-3e06-4db3-8a2e-d7e586dc413f'), 'activation': 2},\n",
    "  {'uuid': UUID('eb51f833-8faa-485b-9ae8-0eda6dbc430b'), 'activation': 2},\n",
    "  {'uuid': UUID('ae37912a-ff49-4e1d-bea5-c0d057190475'), 'activation': 2},\n",
    "  {'uuid': UUID('08431659-4dec-4ea4-9637-f1ab4e2e51f8'), 'activation': 2},\n",
    "  ...\n",
    "  ]}\n",
    "```\n",
    "\n",
    "The uuid's are the unique id's of features in the sparse autoencoder. The activation value is the extent to which that feature was activated by the entity_name. Not the entire query, we sought features for the entity_name alone.\n",
    "\n",
    "There are up to a 100 such feature uuid's and their activations per entity_name. The features list is typically DIFFERENT for each entity_name. \n",
    "\n",
    "We may find that some entity names appear in both lists. So we need to select the UNIQUE known and unknown entities first.\n",
    "\n",
    "Then, as per Ferrando et al, we need a function to : \n",
    "a) obtain the fraction of records for which each feature is active on known and unknown entities respectively.\n",
    "b) calculate the difference in number of occurences, thus obtaining a 'separation score' for each feature.\n",
    "\n",
    "BEWARE, there are many more records in the 'unknown' entities list than in the 'known' entities list. \n",
    "We want to use the information we have to the fullest, but need fair comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to help select the unique known and unknown entities\n",
    "\n",
    "def filter_duplicate_entities(known_list, unknown_list):\n",
    "    # Create sets of entity names from both lists\n",
    "    known_names = {record['entity_name'] for record in known_list}\n",
    "    unknown_names = {record['entity_name'] for record in unknown_list}\n",
    "    \n",
    "    # Find duplicate names that appear in both lists\n",
    "    duplicate_names = known_names.intersection(unknown_names)\n",
    "    \n",
    "    # Filter out records with duplicate names from both lists\n",
    "    filtered_known = [record for record in known_list \n",
    "                     if record['entity_name'] not in duplicate_names]\n",
    "    filtered_unknown = [record for record in unknown_list \n",
    "                       if record['entity_name'] not in duplicate_names]\n",
    "    \n",
    "    return filtered_known, filtered_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of unique known entites:  40\n",
      "Len of unique unknown entites:  355\n"
     ]
    }
   ],
   "source": [
    "feature_activations_known_uq, feature_activations_unknown_uq = filter_duplicate_entities(feature_activations_known, feature_activations_unknown)\n",
    "\n",
    "print(\"Len of unique known entites: \", len(feature_activations_known_uq))\n",
    "print(\"Len of unique unknown entites: \", len(feature_activations_unknown_uq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Dict, Set, Tuple\n",
    "from uuid import UUID\n",
    "\n",
    "def analyze_feature_separation(\n",
    "    feature_activations_known: List[Dict],\n",
    "    feature_activations_unknown: List[Dict],\n",
    "    feature_library_known: Set[Tuple[UUID, str]],\n",
    "    feature_library_unknown: Set[Tuple[UUID, str]]\n",
    ") -> List[Tuple[str, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Analyzes feature activation patterns between known and unknown entities.\n",
    "    \n",
    "    Returns list of tuples: (feature_label, known_fraction, unknown_fraction, separation_score)\n",
    "    sorted by absolute separation score (highest to lowest)\n",
    "    \"\"\"\n",
    "    # Convert feature libraries to dict for easier lookup\n",
    "    known_labels = dict(feature_library_known)\n",
    "    unknown_labels = dict(feature_library_unknown)\n",
    "    \n",
    "    # Count feature occurrences\n",
    "    known_counts = defaultdict(int)\n",
    "    unknown_counts = defaultdict(int)\n",
    "    \n",
    "    # Count total records\n",
    "    n_known = len(feature_activations_known)\n",
    "    n_unknown = len(feature_activations_unknown)\n",
    "    \n",
    "    # Count feature occurrences in known entities\n",
    "    for record in feature_activations_known:\n",
    "        for feature in record['entity_features']:\n",
    "            known_counts[feature['uuid']] += 1\n",
    "            \n",
    "    # Count feature occurrences in unknown entities\n",
    "    for record in feature_activations_unknown:\n",
    "        for feature in record['entity_features']:\n",
    "            unknown_counts[feature['uuid']] += 1\n",
    "    \n",
    "    # Get all unique features\n",
    "    all_features = set(known_counts.keys()) | set(unknown_counts.keys())\n",
    "    \n",
    "    # Calculate fractions and separation scores\n",
    "    results = []\n",
    "    for feature_id in all_features:\n",
    "        # Get feature label\n",
    "        label = known_labels.get(feature_id) or unknown_labels.get(feature_id) or str(feature_id)\n",
    "        \n",
    "        # Calculate fractions\n",
    "        known_fraction = known_counts[feature_id] / n_known if n_known > 0 else 0\n",
    "        unknown_fraction = unknown_counts[feature_id] / n_unknown if n_unknown > 0 else 0\n",
    "        \n",
    "        # Calculate separation score (difference in occurrence rates)\n",
    "        separation_score = known_fraction - unknown_fraction\n",
    "        \n",
    "        results.append((label, feature_id, known_fraction, unknown_fraction, separation_score))\n",
    "    \n",
    "    # Sort by absolute separation score (highest to lowest)\n",
    "    results.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_id</th>\n",
       "      <th>known_frac</th>\n",
       "      <th>unknown_frac</th>\n",
       "      <th>sep_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Syntactical special characters and delimiters in programming contexts</td>\n",
       "      <td>16ba158a-4dda-4b5f-a58f-64c26d26e804</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.456338</td>\n",
       "      <td>-0.406338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>feature_0</td>\n",
       "      <td>1057541e-c829-400a-a14b-a0e777118869</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.445070</td>\n",
       "      <td>-0.395070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Variable assignments and value retrievals in calculator code</td>\n",
       "      <td>62d75912-9f40-45b3-8a5d-19e966f5a7af</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>-0.382042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>The Russian word состав (composition/compilation) and its variations</td>\n",
       "      <td>555d3f44-674b-4027-80eb-3a0182d388f8</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.419718</td>\n",
       "      <td>-0.369718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Birth dates in biographical/historical contexts</td>\n",
       "      <td>9a449812-dd0e-47bb-9d4c-602c3c7a3b13</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>-0.367958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Syntactical markers that introduce descriptions or explanations</td>\n",
       "      <td>b78ce929-8c7b-424a-aaec-21eca0ffa0e7</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.405634</td>\n",
       "      <td>-0.355634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Syntactical elements in programming code and database queries</td>\n",
       "      <td>17b64e16-b078-498c-b81b-76128a5dacf7</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.473239</td>\n",
       "      <td>-0.348239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Structural elements for creating detailed multi-part explanations</td>\n",
       "      <td>0ffb54ca-cd50-4be9-9961-42d0a9e62ca6</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.391549</td>\n",
       "      <td>-0.341549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Discussion of limited or scarce resources and constraints</td>\n",
       "      <td>8c9622fa-8be7-4574-8676-cc71e48654ef</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.456338</td>\n",
       "      <td>-0.331338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Format conversion operators in API and task names</td>\n",
       "      <td>5c80d794-61b3-487f-bc63-303a62c97f53</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.374648</td>\n",
       "      <td>-0.324648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Formal corporate description boilerplate language</td>\n",
       "      <td>0aa99bcd-a125-46ab-8536-6df6f0f1d56e</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.335211</td>\n",
       "      <td>-0.310211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>feature_8</td>\n",
       "      <td>8073ec92-6b17-4e21-a6d5-19c8585447c4</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.346479</td>\n",
       "      <td>-0.296479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Outline hierarchy level punctuation markers</td>\n",
       "      <td>9fb53a76-cc1c-4e39-8562-49d48c208ee6</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.312676</td>\n",
       "      <td>-0.287676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Discussions of optimal spatial arrangement and positioning of technical components</td>\n",
       "      <td>9d00c8a8-face-4258-b0e6-ec5288c30059</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.278873</td>\n",
       "      <td>-0.253873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>The user needs technical clarification</td>\n",
       "      <td>48c7bdcd-2f84-4dd3-9e58-96cd1e7f23c0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>-0.252113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>End of current speaker's turn in conversation</td>\n",
       "      <td>62dbc8e9-c0dc-470e-9066-bb123791a262</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.543662</td>\n",
       "      <td>-0.243662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The user is requesting code explanation or implementation, especially in Python</td>\n",
       "      <td>5dce662e-e73b-43d1-94dd-f7073960f9f1</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.515493</td>\n",
       "      <td>-0.240493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>The user has a question</td>\n",
       "      <td>7295b91c-ec85-4df4-a783-57e180d2074f</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.261972</td>\n",
       "      <td>-0.236972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The user has asked a question</td>\n",
       "      <td>f327a12d-c3bf-4ab2-8a44-3baaf6bae63a</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.484507</td>\n",
       "      <td>-0.234507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Offensive request from the user instructing AI to bypass ethical constraints</td>\n",
       "      <td>ca3dd35c-fbc9-400f-adbc-8021e9511fec</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.256338</td>\n",
       "      <td>-0.231338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>The character sequence 'pet' regardless of semantic meaning or language context</td>\n",
       "      <td>a0bdcfca-4ec0-4c99-a3c4-31093c2be55b</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.247887</td>\n",
       "      <td>-0.222887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>The model is analyzing or processing questions</td>\n",
       "      <td>0440c255-bf9a-4492-94de-ffbcc9c7e725</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.236620</td>\n",
       "      <td>-0.211620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Syntactic delimiters and formatting characters in structured text</td>\n",
       "      <td>ddfa1c50-bb46-4e22-bf2f-f9db1d4ed7d0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.208451</td>\n",
       "      <td>-0.208451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Structured data parsing and classification tasks</td>\n",
       "      <td>196c6399-c7bd-4d62-af0e-d33bac282b1d</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.205634</td>\n",
       "      <td>-0.205634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>The assistant is politely offering help using formal language patterns</td>\n",
       "      <td>ecb92152-1f34-482e-99e2-4e8fd62f9271</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.219718</td>\n",
       "      <td>-0.194718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 label  \\\n",
       "46               Syntactical special characters and delimiters in programming contexts   \n",
       "47                                                                           feature_0   \n",
       "35                        Variable assignments and value retrievals in calculator code   \n",
       "48                The Russian word состав (composition/compilation) and its variations   \n",
       "37                                     Birth dates in biographical/historical contexts   \n",
       "51                     Syntactical markers that introduce descriptions or explanations   \n",
       "43                       Syntactical elements in programming code and database queries   \n",
       "52                   Structural elements for creating detailed multi-part explanations   \n",
       "45                           Discussion of limited or scarce resources and constraints   \n",
       "55                                   Format conversion operators in API and task names   \n",
       "61                                   Formal corporate description boilerplate language   \n",
       "59                                                                           feature_8   \n",
       "63                                         Outline hierarchy level punctuation markers   \n",
       "66  Discussions of optimal spatial arrangement and positioning of technical components   \n",
       "57                                              The user needs technical clarification   \n",
       "31                                       End of current speaker's turn in conversation   \n",
       "32     The user is requesting code explanation or implementation, especially in Python   \n",
       "69                                                             The user has a question   \n",
       "40                                                       The user has asked a question   \n",
       "70        Offensive request from the user instructing AI to bypass ethical constraints   \n",
       "72     The character sequence 'pet' regardless of semantic meaning or language context   \n",
       "76                                      The model is analyzing or processing questions   \n",
       "86                   Syntactic delimiters and formatting characters in structured text   \n",
       "91                                    Structured data parsing and classification tasks   \n",
       "82              The assistant is politely offering help using formal language patterns   \n",
       "\n",
       "                              feature_id  known_frac  unknown_frac  sep_score  \n",
       "46  16ba158a-4dda-4b5f-a58f-64c26d26e804       0.050      0.456338  -0.406338  \n",
       "47  1057541e-c829-400a-a14b-a0e777118869       0.050      0.445070  -0.395070  \n",
       "35  62d75912-9f40-45b3-8a5d-19e966f5a7af       0.125      0.507042  -0.382042  \n",
       "48  555d3f44-674b-4027-80eb-3a0182d388f8       0.050      0.419718  -0.369718  \n",
       "37  9a449812-dd0e-47bb-9d4c-602c3c7a3b13       0.125      0.492958  -0.367958  \n",
       "51  b78ce929-8c7b-424a-aaec-21eca0ffa0e7       0.050      0.405634  -0.355634  \n",
       "43  17b64e16-b078-498c-b81b-76128a5dacf7       0.125      0.473239  -0.348239  \n",
       "52  0ffb54ca-cd50-4be9-9961-42d0a9e62ca6       0.050      0.391549  -0.341549  \n",
       "45  8c9622fa-8be7-4574-8676-cc71e48654ef       0.125      0.456338  -0.331338  \n",
       "55  5c80d794-61b3-487f-bc63-303a62c97f53       0.050      0.374648  -0.324648  \n",
       "61  0aa99bcd-a125-46ab-8536-6df6f0f1d56e       0.025      0.335211  -0.310211  \n",
       "59  8073ec92-6b17-4e21-a6d5-19c8585447c4       0.050      0.346479  -0.296479  \n",
       "63  9fb53a76-cc1c-4e39-8562-49d48c208ee6       0.025      0.312676  -0.287676  \n",
       "66  9d00c8a8-face-4258-b0e6-ec5288c30059       0.025      0.278873  -0.253873  \n",
       "57  48c7bdcd-2f84-4dd3-9e58-96cd1e7f23c0       0.100      0.352113  -0.252113  \n",
       "31  62dbc8e9-c0dc-470e-9066-bb123791a262       0.300      0.543662  -0.243662  \n",
       "32  5dce662e-e73b-43d1-94dd-f7073960f9f1       0.275      0.515493  -0.240493  \n",
       "69  7295b91c-ec85-4df4-a783-57e180d2074f       0.025      0.261972  -0.236972  \n",
       "40  f327a12d-c3bf-4ab2-8a44-3baaf6bae63a       0.250      0.484507  -0.234507  \n",
       "70  ca3dd35c-fbc9-400f-adbc-8021e9511fec       0.025      0.256338  -0.231338  \n",
       "72  a0bdcfca-4ec0-4c99-a3c4-31093c2be55b       0.025      0.247887  -0.222887  \n",
       "76  0440c255-bf9a-4492-94de-ffbcc9c7e725       0.025      0.236620  -0.211620  \n",
       "86  ddfa1c50-bb46-4e22-bf2f-f9db1d4ed7d0       0.000      0.208451  -0.208451  \n",
       "91  196c6399-c7bd-4d62-af0e-d33bac282b1d       0.000      0.205634  -0.205634  \n",
       "82  ecb92152-1f34-482e-99e2-4e8fd62f9271       0.025      0.219718  -0.194718  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the function\n",
    "results = analyze_feature_separation(\n",
    "    feature_activations_known_uq,\n",
    "    feature_activations_unknown_uq,\n",
    "    feature_library_known,\n",
    "    feature_library_unknown\n",
    ")\n",
    "\n",
    "# Convert to pandas for presentation\n",
    "feature_separations = pd.DataFrame(results)\n",
    "feature_separations.columns = ['label', 'feature_id', 'known_frac', 'unknown_frac', 'sep_score' ]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# view in order of features most exclusive to unknown entities (-ve separation scores)\n",
    "feature_separations.sort_values('sep_score', ascending=True).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the unknown feature, feature_0\n",
    "def find_entity_by_uuid(records_list, target_uuid):\n",
    "    \"\"\"\n",
    "    Find unique entity names where the given UUID exists in entity_features\n",
    "    \n",
    "    Args:\n",
    "        records_list (list): List of dictionaries containing entity records\n",
    "        target_uuid (str or UUID): UUID to search for\n",
    "        \n",
    "    Returns:\n",
    "        list: List of unique entity names that contain the target UUID\n",
    "    \"\"\"\n",
    "    # Convert target_uuid to string for comparison if it's not already\n",
    "    target_uuid_str = str(target_uuid)\n",
    "    \n",
    "    # Use a set for automatic deduplication\n",
    "    matching_entities = set()\n",
    "    \n",
    "    for record in records_list:\n",
    "        # Check each feature in entity_features\n",
    "        for feature in record['entity_features']:\n",
    "            if str(feature['uuid']) == target_uuid_str:\n",
    "                matching_entities.add(record['entity_name'])\n",
    "                break  # Found a match, no need to check other features\n",
    "                \n",
    "    # Convert set back to list for return\n",
    "    return list(matching_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_object(uuid_feature, label_feature, feature_activations_list):\n",
    "\n",
    "    feature_entity_name = find_entity_by_uuid(feature_activations_list, uuid_feature)[2]\n",
    "\n",
    "    inspector = client.features.inspect(\n",
    "                    [{\"role\": \"user\", \"content\": feature_entity_name}],\n",
    "                    model=variant,\n",
    "                    #features=features\n",
    "                )\n",
    "\n",
    "    feature = None\n",
    "    for activation in inspector.top(k=100):\n",
    "        if str(activation.feature.uuid) == uuid_feature:\n",
    "            feature = activation.feature\n",
    "            break\n",
    "\n",
    "    # Get neighbors of a feature\n",
    "    neighbors =  client_gf.features.neighbors(\n",
    "        feature,\n",
    "        model=variant,\n",
    "        top_k=10\n",
    "    )\n",
    "\n",
    "    print (\"Neighbouring Features for: \", label_feature)\n",
    "    print (\"--------------------------\")\n",
    "    for neighbor in neighbors:\n",
    "        print(neighbor.label)\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbouring Features for:  feature_0\n",
      "--------------------------\n",
      "Start of a new conversation segment requiring fresh context\n",
      "Beginning of a new conversation or dialogue segment\n",
      "New conversation or topic segment boundary marker\n",
      "Start of new conversation segment, often involving content moderation decisions\n",
      "Start of potentially sensitive conversation segments requiring moderation\n",
      "Start of a new conversation or topic thread\n",
      "Reset conversation and maintain ethical boundaries\n",
      "Beginning of new conversation segment or topic change\n",
      "Start of new conversation segment requiring ethical consistency\n",
      "Start of a new conversation or chat segment\n",
      "Conversation reset marker for context boundaries\n"
     ]
    }
   ],
   "source": [
    "# what are its neighbours?\n",
    "feature = get_feature_object('1057541e-c829-400a-a14b-a0e777118869', 'feature_0', feature_activations_unknown_uq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbouring Features for:  feature_8\n",
      "--------------------------\n",
      "Beginning of a new conversation or dialogue segment\n",
      "New conversation or topic segment boundary marker\n",
      "Start of a new conversation segment requiring fresh context\n",
      "Start of new conversation segment, often involving content moderation decisions\n",
      "Reset conversation and maintain ethical boundaries\n",
      "Start of potentially sensitive conversation segments requiring moderation\n",
      "Beginning of new conversation segment or topic change\n",
      "Start of a new conversation or chat segment\n",
      "Start of a new conversation segment or topic change\n",
      "Conversation reset marker for context boundaries\n",
      "Start of new conversation segment requiring ethical consistency\n"
     ]
    }
   ],
   "source": [
    "# what are its neighbours?\n",
    "feature = get_feature_object('8073ec92-6b17-4e21-a6d5-19c8585447c4', 'feature_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbouring Features for:  The Russian word состав (composition/compilation) and its variations\n",
      "--------------------------\n",
      "Technical composition and assembly of components\n",
      "Technical discussion of system components and subparts\n",
      "French language explanations involving comprehension\n",
      "Component-based explanations and definitions using 'consists of'\n",
      "Constitutional/costituzionale prefix tokens in Romance languages\n",
      "Portuguese words beginning with comp- (compreender, compaixão, complicações)\n",
      "Assembly processes in both manufacturing and programming contexts\n",
      "Korean and Japanese characters related to resource organization and distribution\n",
      "Software component architecture and development\n",
      "The concept of aggregation across multiple languages and technical contexts\n"
     ]
    }
   ],
   "source": [
    "# what are its neighbours?\n",
    "feature = get_feature_object('555d3f44-674b-4027-80eb-3a0182d388f8', 'The Russian word состав (composition/compilation) and its variations')\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  New Datasets!\n",
    "\n",
    "The above dataset is just toooo small and I am not sure about the inversion discussed above, would prefer to stick closer to Ferrando's approach of presenting an entity and asking the LLM for related attributes.\n",
    "\n",
    "Let's try 'The Human Disease Ontology' (HumanDO) which is freely available on the web and github:\n",
    "\n",
    "https://disease-ontology.org/\n",
    "\n",
    "https://github.com/DiseaseOntology/HumanDiseaseOntology/tree/main/src/ontology\n",
    "\n",
    "From the github site we can download the HumanDO.json file. Example content below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'http://purl.obolibrary.org/obo/DOID_0001816',\n",
       " 'lbl': 'angiosarcoma',\n",
       " 'type': 'CLASS',\n",
       " 'meta': {'definition': {'val': 'A vascular cancer that derives_from the cells that line the walls of blood vessels or lymphatic vessels.',\n",
       "   'xrefs': ['url:http://en.wikipedia.org/wiki/Hemangiosarcoma',\n",
       "    'url:https://en.wikipedia.org/wiki/Angiosarcoma',\n",
       "    'url:https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&ns=ncit&code=C3088',\n",
       "    'url:https://www.ncbi.nlm.nih.gov/pubmed/23327728']},\n",
       "  'subsets': ['http://purl.obolibrary.org/obo/doid#DO_cancer_slim',\n",
       "   'http://purl.obolibrary.org/obo/doid#NCIthesaurus'],\n",
       "  'synonyms': [{'pred': 'hasExactSynonym', 'val': 'hemangiosarcoma'}],\n",
       "  'xrefs': [{'val': 'ICDO:9120/3'},\n",
       "   {'val': 'MESH:D006394'},\n",
       "   {'val': 'NCI:C3088'},\n",
       "   {'val': 'NCI:C9275'},\n",
       "   {'val': 'SNOMEDCT_US_2023_03_01:39000009'},\n",
       "   {'val': 'UMLS_CUI:C0018923'},\n",
       "   {'val': 'UMLS_CUI:C0854893'}],\n",
       "  'basicPropertyValues': [{'pred': 'http://www.geneontology.org/formats/oboInOwl#hasAlternativeId',\n",
       "    'val': 'DOID:267'},\n",
       "   {'pred': 'http://www.geneontology.org/formats/oboInOwl#hasAlternativeId',\n",
       "    'val': 'DOID:4508'},\n",
       "   {'pred': 'http://www.geneontology.org/formats/oboInOwl#hasOBONamespace',\n",
       "    'val': 'disease_ontology'}]}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example from HumanDO.json\n",
    "import json\n",
    "\n",
    "with open('../classifier/data_classifier/HumanDO.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    # If the JSON structure has a top-level key containing the graph data,\n",
    "    # you might need to access it like data['graphs'] or similar\n",
    "    humando = data.get('graphs', [{}])[0].get('nodes', [])\n",
    "\n",
    "humando[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need only \n",
    "# a) the disease label, which will our entity. \n",
    "#   - Some are long names with many words, we limit to disease entities with 2 words in their name.\n",
    "# b) the meta definition, which will be used to generate questions and attributes.\n",
    "# c) an id should we ever need to relate back to the original data\n",
    "\n",
    "def parse_disease_entries(data, max_words=2):\n",
    "    \"\"\"\n",
    "    Parse disease entries from Disease Ontology JSON format, filtering by word count in label.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of disease entry dictionaries\n",
    "        max_words (int, optional): Maximum number of words allowed in disease label. Defaults to 3.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing parsed disease information with filtered labels\n",
    "    \"\"\"\n",
    "    parsed_diseases = []\n",
    "    \n",
    "    for entry in data:\n",
    "        label = entry.get('lbl', '')\n",
    "        \n",
    "        # Skip entries where label word count exceeds max_words\n",
    "        if len(label.split()) > max_words:\n",
    "            continue\n",
    "            \n",
    "        disease_info = {\n",
    "            'entity': label,\n",
    "            'description': '',\n",
    "            'id': entry.get('id', '')\n",
    "        }\n",
    "        \n",
    "        # Extract description from meta.definition if it exists\n",
    "        meta = entry.get('meta', {})\n",
    "        if 'definition' in meta:\n",
    "            definition = meta['definition']\n",
    "            if isinstance(definition, dict) and 'val' in definition:\n",
    "                # Replace underscores with spaces in the description, as these appear annnoyingly common\n",
    "                disease_info['description'] = definition['val'].replace('_', ' ')\n",
    "        \n",
    "        parsed_diseases.append(disease_info)\n",
    "    \n",
    "    return parsed_diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4302 selected diseases in the Human Disease Ontology\n",
      "Example:\n",
      " {'entity': 'angiosarcoma', 'description': 'A vascular cancer that derives from the cells that line the walls of blood vessels or lymphatic vessels.', 'id': 'http://purl.obolibrary.org/obo/DOID_0001816'}\n"
     ]
    }
   ],
   "source": [
    "humando_parsed = parse_disease_entries(humando)\n",
    "\n",
    "print(f\"There are\", len(humando_parsed), \"selected diseases in the Human Disease Ontology\")\n",
    "print(\"Example:\\n\",humando_parsed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "class MedicalTermExtractor:\n",
    "    def __init__(self, model_name: str = \"en_core_sci_md\"):\n",
    "        \"\"\"\n",
    "        Initialize the extractor with a scientific/medical spaCy model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the spaCy model to use\n",
    "        \"\"\"\n",
    "        self.nlp = spacy.load(model_name)\n",
    "        self.medical_categories = {\n",
    "            'DISEASE', 'CHEMICAL', 'PROCEDURE', 'ANATOMY',\n",
    "            'PROBLEM', 'TEST', 'TREATMENT'\n",
    "        }\n",
    "\n",
    "    def analyze_sentence(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Analyze sentence to identify unique medical terms that are nouns or adjectives\n",
    "        and not stop words.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input sentence\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of unique filtered tokens\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # First, collect all tokens that meet our criteria\n",
    "        filtered_tokens = []\n",
    "        for token in doc:\n",
    "            if (token.ent_type_ and  # Has an entity type\n",
    "                token.pos_ in ['NOUN', 'ADJ'] and  # Is a noun or adjective\n",
    "                not token.is_stop):  # Is not a stop word\n",
    "                filtered_tokens.append(token.text)\n",
    "        \n",
    "        # Count occurrences of each token\n",
    "        token_counts = Counter(filtered_tokens)\n",
    "        \n",
    "        # Keep only tokens that appear exactly once\n",
    "        unique_tokens = [token for token, count in token_counts.items() if count == 1]\n",
    "        \n",
    "        return unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      " A vascular cancer that derives from the cells that line the walls of blood vessels or lymphatic vessels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliver/anaconda3/envs/mechinterp2/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_sci_md' (0.5.1) was trained with spaCy v3.4.1 and may not be 100% compatible with the current version (3.8.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vascular', 'cancer', 'cells', 'walls', 'blood', 'lymphatic']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try this on our first example, we hope to extract medical attributes for the disease, which is 'angiosarcoma'\n",
    "# 'A vascular cancer that derives_from the cells that line the walls of blood vessels or lymphatic vessels.'\n",
    "\n",
    "# Example text\n",
    "sample_text = humando_parsed[0]['description']\n",
    "print(\"Sample Text:\\n\",sample_text)\n",
    "\n",
    "# Initialize the extractor\n",
    "extractor = MedicalTermExtractor()\n",
    "word_list  = extractor.analyze_sentence(sample_text)\n",
    "word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fill_in_blanks(humando: dict, words_to_replace: list) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a fill-in-the-blanks version of the description, prefixed with the entity name.\n",
    "    If words_to_replace has more than 2 members, uses positions 0 and 2.\n",
    "    \n",
    "    Args:\n",
    "        humando (dict): Input dictionary with entity, description, and id\n",
    "        words_to_replace (list): List of words to be replaced with placeholders\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with transformed description and related information\n",
    "    \"\"\"\n",
    "    # Initialize the result dictionary\n",
    "    result = {\n",
    "        'id': humando['id'],\n",
    "        'entity_name': humando['entity'],\n",
    "        'missing_words': [],\n",
    "        'query': humando['description']\n",
    "    }\n",
    "    \n",
    "    # Select the words to actually replace\n",
    "    if len(words_to_replace) > 2:\n",
    "        selected_words = [words_to_replace[0], words_to_replace[2]]\n",
    "    else:\n",
    "        selected_words = words_to_replace[:2]\n",
    "    \n",
    "    # Create a mapping of words to their placeholders\n",
    "    replacements = {}\n",
    "    placeholder_labels = ['[Word A]', '[Word B]']\n",
    "    \n",
    "    for word, placeholder in zip(selected_words, placeholder_labels):\n",
    "        replacements[word] = placeholder\n",
    "        result['missing_words'].append(word)\n",
    "    \n",
    "    # Create the modified description by replacing words\n",
    "    modified_text = humando['description']\n",
    "    for word, placeholder in replacements.items():\n",
    "        # Using word boundaries to ensure we replace whole words only\n",
    "        modified_text = modified_text.replace(f\" {word} \", f\" {placeholder} \")\n",
    "        # Handle case where word is at the start of the text\n",
    "        modified_text = modified_text.replace(f\"{word} \", f\"{placeholder} \")\n",
    "        # Handle case where word is at the end of the text\n",
    "        modified_text = modified_text.replace(f\" {word}.\", f\" {placeholder}.\")\n",
    "    \n",
    "    # Capitalize the first letter of the entity name and create the prefixed text\n",
    "    capitalized_entity = humando['entity'].capitalize()\n",
    "    result['query'] = f\"{capitalized_entity}: {modified_text}\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'http://purl.obolibrary.org/obo/DOID_0001816',\n",
       " 'entity_name': 'angiosarcoma',\n",
       " 'missing_words': ['vascular', 'cells'],\n",
       " 'query': 'Angiosarcoma: A [Word A] cancer that derives from the [Word B] that line the walls of blood vessels or lymphatic vessels.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it out:\n",
    "create_fill_in_blanks(humando_parsed[0], words_to_replace=word_list )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the records and then spit into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliver/anaconda3/envs/mechinterp2/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_sci_md' (0.5.1) was trained with spaCy v3.4.1 and may not be 100% compatible with the current version (3.8.3). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 9%\n",
      "Progress: 19%\n",
      "Progress: 29%\n",
      "Progress: 39%\n",
      "Progress: 49%\n",
      "Progress: 59%\n",
      "Progress: 69%\n",
      "Progress: 79%\n",
      "Progress: 89%\n",
      "Progress: 99%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'http://purl.obolibrary.org/obo/DOID_4104',\n",
       "  'entity_name': 'obsolete rinderpest',\n",
       "  'missing_words': ['viral', 'disease'],\n",
       "  'query': 'Obsolete rinderpest: A [Word A] infectious [Word B] that results in infection in cattle, has material basis in Rinderpest virus, which is transmitted by direct contact with an infected animal, or transmitted by ingestion of contaminated water. The infection has symptom fever, has symptom nasal and eye discharge, and results in formation of erosions in the mouth, the lining of the nose and the genital tract.'},\n",
       " {'id': 'http://purl.obolibrary.org/obo/DOID_2977',\n",
       "  'entity_name': 'primary hyperoxaluria',\n",
       "  'missing_words': ['carbohydrate', 'disorder'],\n",
       "  'query': 'Primary hyperoxaluria: A [Word A] metabolic [Word B] characterized by impaired glyoxylate metabolism resulting in accumulation of oxalate throughout the body typically manifesting as kidney and bladder stones.'},\n",
       " {'id': 'http://purl.obolibrary.org/obo/DOID_9723',\n",
       "  'entity_name': 'vitreous abscess',\n",
       "  'missing_words': ['disease', 'eye'],\n",
       "  'query': 'Vitreous abscess: A vitreous [Word A] that is characterized by an abscess located in the vitreous of the [Word B].'},\n",
       " {'id': 'http://purl.obolibrary.org/obo/DOID_5162',\n",
       "  'entity_name': 'arteriolosclerosis',\n",
       "  'missing_words': ['arteriosclerosis', 'small'],\n",
       "  'query': 'Arteriolosclerosis: An [Word A] that is characterized by thickening of the wall of the [Word B] arteries and arterioles, caused by deposition of hyaline material in the wall or concentric smooth muscle wall hypertrophy, and results in lumen narrowing and tissue ischemia.'},\n",
       " {'id': 'http://purl.obolibrary.org/obo/DOID_1856',\n",
       "  'entity_name': 'cherubism',\n",
       "  'missing_words': ['bone', 'replacement'],\n",
       "  'query': 'Cherubism: A [Word A] disease characterized by [Word B] of [Word A] in the jaws with fibrous tissue leding to facial swelling that has material basis in heterozygous mutation in the SH3BP2 gene on chromosome 4p16.3.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets get processed records for train and test.\n",
    "import random\n",
    "\n",
    "# how many records do we want for train + test ?\n",
    "n = len(humando_parsed)\n",
    "humando_parsed_sample = random.sample(humando_parsed, n)\n",
    "\n",
    "# instantiate the term extractor\n",
    "extractor = MedicalTermExtractor()\n",
    "\n",
    "# prepare blank list, will eventually be in same format as ds_ncbi_train_processed\n",
    "humando_parsed_sample_processed = []\n",
    "\n",
    "# loop over all records,. Beware tqdm appears to cause spacy to crash, so we enumerate to report progress...\n",
    "for i, record in enumerate(humando_parsed_sample):\n",
    "\n",
    "    # report progress\n",
    "    if (i + 1) % (n // 10) == 0:\n",
    "            print(f\"Progress: {((i + 1) * 100) // n}%\")\n",
    "\n",
    "    # get words to replace\n",
    "    word_list = extractor.analyze_sentence(record['description'])\n",
    "\n",
    "    # need at least two words to replace\n",
    "    if len(word_list) >= 2:\n",
    "        # process the disease description, masking th key words\n",
    "        record_updated = create_fill_in_blanks(record, words_to_replace=word_list)\n",
    "        # add the new data to the list\n",
    "        humando_parsed_sample_processed.append(record_updated)\n",
    "\n",
    "# view a sample\n",
    "humando_parsed_sample_processed[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into train and test, already shuffled.\n",
    "import math\n",
    "import pandas as pd\n",
    "n= len(humando_parsed_sample_processed)\n",
    "test_propn = 0.2\n",
    "split = math.ceil(n*(1-test_propn))\n",
    "\n",
    "# get split data, we will use ALL records for Training, as there is no model to test, just a separation score of features.\n",
    "humando_train = humando_parsed_sample_processed\n",
    "# humando_testi = humando_parsed_sample_processed[split:]\n",
    "\n",
    "# save progress\n",
    "pd.DataFrame(humando_train).to_parquet('../classifier/data_classifier/humando_train.parquet', index=False)\n",
    "# pd.DataFrame(humando_testi).to_parquet('../classifier/data_classifier/humando_testi.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'http://purl.obolibrary.org/obo/DOID_4104',\n",
       "  'entity_name': 'obsolete rinderpest',\n",
       "  'missing_words': array(['viral', 'disease'], dtype=object),\n",
       "  'query': 'Obsolete rinderpest: A [Word A] infectious [Word B] that results in infection in cattle, has material basis in Rinderpest virus, which is transmitted by direct contact with an infected animal, or transmitted by ingestion of contaminated water. The infection has symptom fever, has symptom nasal and eye discharge, and results in formation of erosions in the mouth, the lining of the nose and the genital tract.'},\n",
       " {'id': 'http://purl.obolibrary.org/obo/DOID_2977',\n",
       "  'entity_name': 'primary hyperoxaluria',\n",
       "  'missing_words': array(['carbohydrate', 'disorder'], dtype=object),\n",
       "  'query': 'Primary hyperoxaluria: A [Word A] metabolic [Word B] characterized by impaired glyoxylate metabolism resulting in accumulation of oxalate throughout the body typically manifesting as kidney and bladder stones.'},\n",
       " {'id': 'http://purl.obolibrary.org/obo/DOID_9723',\n",
       "  'entity_name': 'vitreous abscess',\n",
       "  'missing_words': array(['disease', 'eye'], dtype=object),\n",
       "  'query': 'Vitreous abscess: A vitreous [Word A] that is characterized by an abscess located in the vitreous of the [Word B].'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect sample\n",
    "humando_train = pd.read_parquet('../classifier/data_classifier/humando_train.parquet')\n",
    "humando_train = humando_train.to_dict('records')\n",
    "humando_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anlayse Data for Features - Known vs Unknown\n",
    "\n",
    "We now need to see how well the LLM performs on the query, will it get fill in the blanks correctly?\n",
    "Waht are the features separation scores on the known vs unknown entities?\n",
    "\n",
    "Happily, we can use the functions we have already written on the Humand DO data.\n",
    "\n",
    "1. Send each query to the LLM and score their knowledge o the disease entites\n",
    "2. Use Goodfire to get the SAE features for the known vs unknown entities (not the queries, but the entities)\n",
    "3. Calculate the separate scores for each feature on known vs unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 3145/3145 [16:03<00:00,  3.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the TRAIN data we have:\n",
      "Known Entities    : 80\n",
      "Uncertain Entities: 1147\n",
      "Unknown Entities  : 1918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Send each query to the LLM and score their knowledge o the disease entites\n",
    "\n",
    "humando_train_completed_tg = query_to_llm(humando_train, provider='together')\n",
    "\n",
    "# Scores:\n",
    "Score_2 = sum(1 for d in humando_train_completed_tg if d['score'] == 2)\n",
    "Score_1 = sum(1 for d in humando_train_completed_tg if d['score'] == 1)\n",
    "Score_0 = sum(1 for d in humando_train_completed_tg if d['score'] == 0)\n",
    "\n",
    "print(\"In the TRAIN data we have:\")\n",
    "print(\"Known Entities    :\", Score_2)\n",
    "print(\"Uncertain Entities:\", Score_1)\n",
    "print(\"Unknown Entities  :\", Score_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save progress\n",
    "pd.DataFrame(humando_train_completed_tg).to_parquet('../classifier/data_classifier/humando_train_completed_tg.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh, we have ONLY 80 known entities. Waayyyy too small. \n",
    "\n",
    "Let's see if we can make it easier for the LLM. We will mask only one word per query, but send two queries. The LLM may perform better with just one masked word per query, there ar emore clues in the remaining sentence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Questions Using HumanDO data\n",
    "\n",
    "We need to do this precisely as per Ferrando et al.\n",
    "So we need to ask an LLM to do the following, as per an example using HumanDO...\n",
    "We will use GTP4-mini, as there will be thousands of questions, so many millions of tokens to process. \n",
    "\n",
    "```\n",
    "Devise two questions with one word answers about this using this information only:\n",
    "\"The disease Primary hyperoxaluria is a carbohydrate metabolic disorder characterized by impaired glyoxylate metabolism resulting in accumulation of oxalate throughout the body typically manifesting as kidney and bladder stones.\", \n",
    "\n",
    "Your questions must start with \"The disease Primary hyperoxaluria ...\".\n",
    "Return your questions as a python list of dicts with the query and answer:\n",
    "[{query:\"The disease....\", answer:\"...\"}{query:\"The disease....\", answer:\"...\"}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'angiosarcoma',\n",
       "  'description': 'A vascular cancer that derives from the cells that line the walls of blood vessels or lymphatic vessels.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0001816'},\n",
       " {'entity': 'pterygium',\n",
       "  'description': 'A corneal disease that is characterized by a triangular tissue growth located in cornea of the eye that is the result of collagen degeneration and fibrovascular proliferation.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0002116'},\n",
       " {'entity': 'shrimp allergy',\n",
       "  'description': 'A crustacean allergy that has allergic trigger shrimp.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0040001'}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's go back to the humando data, sample below\n",
    "humando_parsed[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import logging\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import asyncio\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    query: str = Field(..., description=\"The question about the disease\")\n",
    "    answer: str = Field(..., description=\"One-word answer to the question\")\n",
    "\n",
    "class QuestionGenerationAgent():\n",
    "    \"\"\"\n",
    "    Agent responsible for generating questions and answers about diseases based on given descriptions.\n",
    "    \"\"\"\n",
    "    async def run(self, llm, disease_records):\n",
    "        \"\"\"\n",
    "        Generate questions and answers based on the provided disease records.\n",
    "\n",
    "        Args:\n",
    "            llm: The language model to use for question generation\n",
    "            disease_records (List[Dict]): List of disease records containing entity, description, and id\n",
    "\n",
    "        Returns:\n",
    "            tuple: A list of generated questions with answers and consumption details\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tools = [{\n",
    "                \"name\": \"submit_questions\",\n",
    "                \"description\": \"Submit generated questions and answers\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question_answers\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"query\": {\"type\": \"string\"},\n",
    "                                    \"answer\": {\"type\": \"string\"}\n",
    "                                },\n",
    "                                \"required\": [\"query\", \"answer\"]\n",
    "                            },\n",
    "                            \"description\": \"List of questions and their one-word answers\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"question_answers\"]\n",
    "                }\n",
    "            }]\n",
    "\n",
    "            all_questions = []\n",
    "\n",
    "            for record in disease_records:\n",
    "\n",
    "                # Create the prompt with the specific disease information\n",
    "                messages = [\n",
    "                    (\"system\", \"You are a medical question generator.\"),\n",
    "                    (\"user\", \"\"\"Devise two questions, each with with one word answers, about this disease '{entity}', but using ONLY the following information: \n",
    "                    \"The disease {entity} is {description}\"\n",
    "                    Each of the two questions must start with \"The disease {entity} ...\". \n",
    "                    Ensure the one word answers are not ambiguous, and are clearly complete despite being only one word.\n",
    "                    Return the two questions with their respective one word answer as per the tool 'submit_questions'.\"\"\")\n",
    "                ]\n",
    "                prompt = ChatPromptTemplate.from_messages(messages).partial()\n",
    "\n",
    "                # Create and invoke the chain\n",
    "                chain = (prompt | llm.bind_tools(tools, tool_choice=\"submit_questions\"))\n",
    "                response = await chain.ainvoke(record)\n",
    "\n",
    "                # Extract and validate questions\n",
    "                func_args = response.additional_kwargs['tool_calls'][0]['function']['arguments']\n",
    "                func_args_data = json.loads(func_args)\n",
    "                \n",
    "                # Validate each question-answer pair using Pydantic\n",
    "                for qa_pair in func_args_data['question_answers']:\n",
    "                    validated_qa = QuestionAnswer(**qa_pair)\n",
    "                    all_questions.append(validated_qa.model_dump())\n",
    "\n",
    "            return all_questions\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            msg = f\"Question Generation Agent Error: Invalid JSON: {str(e)}\"\n",
    "            print(msg)\n",
    "            raise\n",
    "\n",
    "        except KeyError as e:\n",
    "            msg = f\"Question Generation Agent Error: Missing key in JSON: {str(e)}\"\n",
    "            print(msg)\n",
    "            raise\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = f\"Question Generation Agent Error: Unexpected error: {str(e)}\"\n",
    "            print(msg)\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_records(records, agent, llm, batch_size=10):\n",
    "    \"\"\"\n",
    "    Process records in simple batches with progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        records: List of disease records\n",
    "        agent: QuestionGenerationAgent instance\n",
    "        llm: Language model instance\n",
    "        batch_size: How many records to process at once\n",
    "    \"\"\"\n",
    "    processed_records = []\n",
    "    \n",
    "    # Create batches\n",
    "    total_batches = (len(records) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Processing {len(records)} records in {total_batches} batches\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Process each batch\n",
    "    for i in tqdm(range(0, len(records), batch_size), desc=\"Processing batches\"):\n",
    "        batch = records[i:i + batch_size]\n",
    "        batch_tasks = []\n",
    "        \n",
    "        # Process each record\n",
    "        for record in batch:\n",
    "            try:\n",
    "                question_answers = await agent.run(llm, [record])\n",
    "                # Ensure questions is treated as a list\n",
    "                if not isinstance(question_answers, list):\n",
    "                    print(f\"Warning: questions for {record['id']} is not a list: {question_answers}\")\n",
    "                    question_answers_list = [question_answers] if question_answers else []\n",
    "                else:\n",
    "                    question_answers_list = question_answers\n",
    "                processed_records.append({**record, \"question_answers\": question_answers_list})\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing record {record['id']}: {str(e)}\")\n",
    "                processed_records.append({**record, \"question_answers\": [], \"error\": str(e)})\n",
    "        \n",
    "    # Print summary\n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    print(f\"\\nProcessing completed in {duration:.2f} seconds\")\n",
    "    print(f\"Successfully processed: {len(processed_records)} records\")\n",
    "    print(f\"Average rate: {len(processed_records)/duration:.2f} records/second\")\n",
    "    \n",
    "    return processed_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"REDACTED\"\n",
    "\n",
    "# Cell 1: Setup\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create the LLM instance\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",  # or whatever model you're using\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Set up components\n",
    "agent = QuestionGenerationAgent()\n",
    "\n",
    "# Cell 2: Process records (use this code in a new cell)\n",
    "# This works in Jupyter because it uses the notebook's event loop\n",
    "humando_parsed_2Q = await process_records(humando_parsed, agent, llm, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity': 'holoprosencephaly 5',\n",
       " 'description': 'A holoprosencephaly that has material basis in heterozygous mutation in the ZIC2 gene on chromosome 13q32.',\n",
       " 'id': 'http://purl.obolibrary.org/obo/DOID_0110878',\n",
       " 'question_answers': [{'query': 'The disease holoprosencephaly 5 has a material basis in a mutation in which gene?',\n",
       "   'answer': 'ZIC2'},\n",
       "  {'query': 'The disease holoprosencephaly 5 is associated with a mutation on which chromosome?',\n",
       "   'answer': '13q32'}]}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(humando_parsed_2Q).to_parquet('../classifier/data_classifier/humando_parsed_2Q.parquet')\n",
    "\n",
    "#View an example\n",
    "humando_parsed_2Q[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(record):\n",
    "    \"\"\"\n",
    "    Convert records to the format required by previous functions.\n",
    "    \n",
    "    Args:\n",
    "        record (dict): Input record with question_answers field\n",
    "        \n",
    "    Returns:\n",
    "        list: List of transformed records\n",
    "    \"\"\"\n",
    "    # Create base record without question_answers\n",
    "    base_record = {\n",
    "        'entity': record['entity'],\n",
    "        'description': record['description'],\n",
    "        'id': record['id']\n",
    "    }\n",
    "    \n",
    "    # Create a new record for each question-answer pair\n",
    "    transformed_records = []\n",
    "    for qa in record['question_answers']:\n",
    "        new_record = base_record.copy()\n",
    "        new_record['query'] = qa['query'] +' [Word A]'\n",
    "        new_record['missing_words'] = [qa['answer']]\n",
    "        transformed_records.append(new_record)\n",
    "    \n",
    "    return transformed_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "humando_parsed_1Q = []\n",
    "for record in humando_parsed_2Q:\n",
    "    humando_parsed_1Q.extend(convert_format(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'angiosarcoma',\n",
       "  'description': 'A vascular cancer that derives from the cells that line the walls of blood vessels or lymphatic vessels.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0001816',\n",
       "  'query': 'The disease angiosarcoma affects which type of vessels? [Word A]',\n",
       "  'missing_words': ['vascular']},\n",
       " {'entity': 'angiosarcoma',\n",
       "  'description': 'A vascular cancer that derives from the cells that line the walls of blood vessels or lymphatic vessels.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0001816',\n",
       "  'query': 'The disease angiosarcoma originates from which type of cells? [Word A]',\n",
       "  'missing_words': ['endothelial']},\n",
       " {'entity': 'pterygium',\n",
       "  'description': 'A corneal disease that is characterized by a triangular tissue growth located in cornea of the eye that is the result of collagen degeneration and fibrovascular proliferation.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0002116',\n",
       "  'query': 'The disease pterygium is characterized by what type of tissue growth? [Word A]',\n",
       "  'missing_words': ['triangular']},\n",
       " {'entity': 'pterygium',\n",
       "  'description': 'A corneal disease that is characterized by a triangular tissue growth located in cornea of the eye that is the result of collagen degeneration and fibrovascular proliferation.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0002116',\n",
       "  'query': 'The disease pterygium affects which part of the eye? [Word A]',\n",
       "  'missing_words': ['cornea']}]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect sample\n",
    "# we expect two queries, i.e. two records, per id.\n",
    "# We expect the missing word, [Word A], to be the last word in all queries, i.e. the answer.\n",
    "humando_parsed_1Q[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is in the same format as previously we can use same functions.\n",
    "We will have twice as many records to process, so 8000+ calls to Goodfire.\n",
    "Let's process the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humando_parsed_1Q_completed_tg = query_to_llm(humando_parsed_1Q, provider='together', missing_qty=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'angiosarcoma',\n",
       "  'description': 'A vascular cancer that derives from the cells that line the walls of blood vessels or lymphatic vessels.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0001816',\n",
       "  'query': 'The disease angiosarcoma affects which type of vessels? [Word A]',\n",
       "  'missing_words': ['vascular'],\n",
       "  'llm_response': \"['lymphatic', 'blood', 'lymphatic and blood', 'blood vessels', 'lymph vessels']\",\n",
       "  'score': 0},\n",
       " {'entity': 'angiosarcoma',\n",
       "  'description': 'A vascular cancer that derives from the cells that line the walls of blood vessels or lymphatic vessels.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0001816',\n",
       "  'query': 'The disease angiosarcoma originates from which type of cells? [Word A]',\n",
       "  'missing_words': ['endothelial'],\n",
       "  'llm_response': \"['endothelial']\",\n",
       "  'score': 1},\n",
       " {'entity': 'pterygium',\n",
       "  'description': 'A corneal disease that is characterized by a triangular tissue growth located in cornea of the eye that is the result of collagen degeneration and fibrovascular proliferation.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0002116',\n",
       "  'query': 'The disease pterygium is characterized by what type of tissue growth? [Word A]',\n",
       "  'missing_words': ['triangular'],\n",
       "  'llm_response': \"['fibrovascular']\",\n",
       "  'score': 0},\n",
       " {'entity': 'pterygium',\n",
       "  'description': 'A corneal disease that is characterized by a triangular tissue growth located in cornea of the eye that is the result of collagen degeneration and fibrovascular proliferation.',\n",
       "  'id': 'http://purl.obolibrary.org/obo/DOID_0002116',\n",
       "  'query': 'The disease pterygium affects which part of the eye? [Word A]',\n",
       "  'missing_words': ['cornea'],\n",
       "  'llm_response': \"['cornea']\",\n",
       "  'score': 1}]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect sample\n",
    "humando_parsed_1Q_completed_tg[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save progress\n",
    "\n",
    "pd.DataFrame(humando_parsed_1Q_completed_tg).to_parquet('../classifier/data_classifier/humando_parsed_1Q_completed_tg.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two questions per record, i.e. two records per id.\n",
    "We need to group these by id in order to score each entity as having 0, 1, or 2 attributes correctly named by the LLM.\n",
    "So, a new scoring function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a fuction to score split data\n",
    "\n",
    "def calculate_combined_scores(split_data):\n",
    "    \"\"\"\n",
    "    Calculate scores for the original queries based on their split versions.\n",
    "    \n",
    "    Args:\n",
    "        split_data (list): List of dictionaries containing split queries with individual scores\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (total_score_2, total_score_1, total_score_0) counts for queries where \n",
    "               2, 1, or 0 words were correctly guessed\n",
    "    \"\"\"\n",
    "    # Group the split queries by their ID\n",
    "    query_scores = {}\n",
    "    for item in split_data:\n",
    "        query_id = item['id']\n",
    "        if query_id not in query_scores:\n",
    "            query_scores[query_id] = []\n",
    "        query_scores[query_id].append(item['score'])\n",
    "    \n",
    "    # Count queries based on combined scores, start at zero\n",
    "    score_2 = 0  # Both words correct\n",
    "    score_1 = 0  # One word correct\n",
    "    score_0 = 0  # No words correct\n",
    "    \n",
    "    for query_id, scores in query_scores.items():\n",
    "        # Each query should have exactly 2 scores (Word A and Word B)\n",
    "        assert len(scores) == 2, f\"Query {query_id} has {len(scores)} scores instead of 2\"\n",
    "        \n",
    "        # Sum the individual scores for the query\n",
    "        total_correct = sum(scores)\n",
    "        \n",
    "        if total_correct == 2:  # Both words correct\n",
    "            score_2 += 1\n",
    "        elif total_correct == 1:  # One word correct\n",
    "            score_1 += 1\n",
    "        else:  # No words correct\n",
    "            score_0 += 1\n",
    "    \n",
    "    return score_0, score_1, score_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the split TRAIN data we have:\n",
      "Known Entities    : 707\n",
      "Uncertain Entities: 1925\n",
      "Unknown Entities  : 1670\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scores:\n",
    "scores = calculate_combined_scores(humando_parsed_1Q_completed_tg)\n",
    "print(\"In the split TRAIN data we have:\")\n",
    "print(f\"Known Entities    : {scores[2]}\")\n",
    "print(f\"Uncertain Entities: {scores[1]}\")\n",
    "print(f\"Unknown Entities  : {scores[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to separate data into known and unknown entities\n",
    "\n",
    "def filter_score_sets(data):\n",
    "    \"\"\"\n",
    "    Filter data into two sets based on combined scores per ID.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of dictionaries containing query data with scores\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (perfect_scores, zero_scores) where each is a list of dictionaries\n",
    "               containing unique id, entity, and description for entries with\n",
    "               total scores of 2 and 0 respectively\n",
    "    \"\"\"\n",
    "    # First, group by ID and calculate total scores\n",
    "    grouped_scores = {}\n",
    "    for item in data:\n",
    "        if item['id'] not in grouped_scores:\n",
    "            grouped_scores[item['id']] = {\n",
    "                'total_score': 0,\n",
    "                'entity': item['entity'],\n",
    "                'description': item['description'],\n",
    "                'id': item['id']\n",
    "            }\n",
    "        grouped_scores[item['id']]['total_score'] += item['score']\n",
    "    \n",
    "    # Filter into two sets\n",
    "    perfect_scores = [\n",
    "        {\n",
    "            'id': info['id'],\n",
    "            'entity': info['entity'],\n",
    "            'description': info['description']\n",
    "        }\n",
    "        for info in grouped_scores.values()\n",
    "        if info['total_score'] == 2\n",
    "    ]\n",
    "    \n",
    "    zero_scores = [\n",
    "        {\n",
    "            'id': info['id'],\n",
    "            'entity': info['entity'],\n",
    "            'description': info['description']\n",
    "        }\n",
    "        for info in grouped_scores.values()\n",
    "        if info['total_score'] == 0\n",
    "    ]\n",
    "    \n",
    "    return perfect_scores, zero_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of known entities =  707\n",
      "Examples:  ['intracranial arteriosclerosis', 'cholestasis', 'penile cancer', 'duodenal atresia']\n",
      "\n",
      "\n",
      "No. of unknown entities =  1670\n",
      "Examples:  ['branchiootorenal syndrome', 'calciphylaxis', 'papillary ependymoma', 'biotinidase deficiency']\n"
     ]
    }
   ],
   "source": [
    "# The known and unknown entities are as follows:\n",
    "\n",
    "records_known, records_unknown = filter_score_sets(humando_parsed_1Q_completed_tg)\n",
    "\n",
    "entities_known_1Q  = [record['entity'] for record in records_known]\n",
    "print(\"No. of known entities = \", len(entities_known_1Q))\n",
    "print(\"Examples: \", random.sample(entities_known_1Q, 4))\n",
    "print('\\n')\n",
    "entities_unknown_1Q = [record['entity'] for record in records_unknown]\n",
    "print(\"No. of unknown entities = \", len(entities_unknown_1Q))\n",
    "print(\"Examples: \", random.sample(entities_unknown_1Q, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the previous dataset we had to remove duplicates, i.e. entities which appeared in both the known and unknown lists. \n",
    "That is not a concern with this Human DO dataset, so we cna skip that step and move directly to getting feature activations from Goodfire.\n",
    "\n",
    "Beware, slow process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entity_names: 100%|██████████| 1670/1670 [1:23:02<00:00,  2.98s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import goodfire\n",
    "import os\n",
    "\n",
    "api_key ='REDACTED'\n",
    "client_gf = goodfire.Client(api_key)\n",
    "variant = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Get the activations for the known and unknown entities\n",
    "\n",
    "#feature_activations_known_1Q,   feature_library_known_1Q   = get_feature_activations(client_gf, variant, entities_known_1Q,   k=100)\n",
    "feature_activations_unknown_1Q, feature_library_unknown_1Q = get_feature_activations(client_gf, variant, entities_unknown_1Q, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_name': 'Streptococcus pneumonia',\n",
       "  'entity_features': [{'uuid': UUID('cd092e67-147c-4e1d-92ab-4c5011e15cd1'),\n",
       "    'activation': 4},\n",
       "   {'uuid': UUID('b6e2e00d-5b8e-482a-814e-3d5c12064335'), 'activation': 3},\n",
       "   {'uuid': UUID('5ee69d55-8cc5-4961-a033-5fbfa061a541'), 'activation': 3},\n",
       "   {'uuid': UUID('716e640e-96ce-444a-9c30-3f75a7be468d'), 'activation': 3},\n",
       "   {'uuid': UUID('43fa3f6f-55db-4b96-bf22-4c09cfe309c3'), 'activation': 3},\n",
       "   {'uuid': UUID('952fc13e-2675-4091-8e09-17a0244d0320'), 'activation': 3},\n",
       "   {'uuid': UUID('61d3172d-bb52-4f18-8928-61d654b046d1'), 'activation': 3},\n",
       "   {'uuid': UUID('5e4deda8-16c3-4306-851d-e3c92f45b874'), 'activation': 3},\n",
       "   {'uuid': UUID('89662c0e-31f0-4d73-80f3-2b13621abb7a'), 'activation': 2},\n",
       "   {'uuid': UUID('48e23010-d5b6-421c-9893-6d6c4e1497ce'), 'activation': 2},\n",
       "   {'uuid': UUID('746d21da-0e98-4d7c-8387-78c4b1b1b93b'), 'activation': 2},\n",
       "   {'uuid': UUID('5fdee60f-1a49-4a44-a855-fd09b1ed5f54'), 'activation': 2},\n",
       "   {'uuid': UUID('8e25f9ff-ac20-4a78-a420-2491142a0757'), 'activation': 2},\n",
       "   {'uuid': UUID('3ce1a431-4fa8-4d9b-8be5-a43b8de48443'), 'activation': 2},\n",
       "   {'uuid': UUID('ea0f615a-5cfa-4971-8c0e-63aa9835b31a'), 'activation': 2},\n",
       "   {'uuid': UUID('ded03cb7-659d-4505-8a57-132bc4594c86'), 'activation': 2},\n",
       "   {'uuid': UUID('d4c30933-3eed-49b0-bf39-bec57c0ba269'), 'activation': 2},\n",
       "   {'uuid': UUID('2379fd4b-9173-4da2-a640-62158ec58f4e'), 'activation': 2},\n",
       "   {'uuid': UUID('0be31be5-8fb1-45fa-bdd7-8f92ac60b14d'), 'activation': 2},\n",
       "   {'uuid': UUID('23c3d9f1-5664-4a29-92ae-f3bf1e6b1717'), 'activation': 2},\n",
       "   {'uuid': UUID('d2f7ed41-c6c5-4345-994c-f9eca390a5d2'), 'activation': 2},\n",
       "   {'uuid': UUID('4ec87d30-8932-41ce-84cd-4ec55608e719'), 'activation': 2},\n",
       "   {'uuid': UUID('1ae8878d-fa9d-4545-9fa8-59ee5397ecdb'), 'activation': 1},\n",
       "   {'uuid': UUID('b8829561-ad0e-4027-ac2d-d5897dd2ad49'), 'activation': 1},\n",
       "   {'uuid': UUID('43f39c64-a63c-4ad4-b965-ec41d105b6c9'), 'activation': 1},\n",
       "   {'uuid': UUID('2cda0439-8748-4b5c-95cb-9d0de5591be0'), 'activation': 1},\n",
       "   {'uuid': UUID('e576dcfa-1539-4bcc-ac5f-cf4dbcac9518'), 'activation': 1},\n",
       "   {'uuid': UUID('98bebe07-3938-4a6e-9d87-626779b21802'), 'activation': 1},\n",
       "   {'uuid': UUID('94a9e42e-4b8e-421f-a27c-49e1445680ce'), 'activation': 1},\n",
       "   {'uuid': UUID('671bb925-a2ca-402b-a771-91603d7d7f64'), 'activation': 1},\n",
       "   {'uuid': UUID('1cd275f8-9627-4059-a1be-58a8684127a1'), 'activation': 1},\n",
       "   {'uuid': UUID('b7b46eca-d9a2-4646-a49f-fd977fda3f4f'), 'activation': 1},\n",
       "   {'uuid': UUID('af42d6dd-9119-461f-8d86-491445b4ac7d'), 'activation': 1},\n",
       "   {'uuid': UUID('1a5e933f-e8a5-4417-8ffc-4844d94367e0'), 'activation': 1},\n",
       "   {'uuid': UUID('ad189296-e0ca-47f8-b8c2-c68254a3fef7'), 'activation': 1},\n",
       "   {'uuid': UUID('8268b5d2-44ca-4b79-9883-d6d473c9b621'), 'activation': 1},\n",
       "   {'uuid': UUID('ba4c6e16-8b3f-4068-ad69-c3b82154152d'), 'activation': 1},\n",
       "   {'uuid': UUID('83c9c019-cced-4ca7-9b04-1bfd06b70aba'), 'activation': 1},\n",
       "   {'uuid': UUID('a6b058aa-ed89-4ba3-abc5-9b63fe5db528'), 'activation': 1},\n",
       "   {'uuid': UUID('6a5a0eb6-a81d-4569-adb3-b003606a7028'), 'activation': 1},\n",
       "   {'uuid': UUID('f51800b0-5f0b-4762-87b0-22501fb9e293'), 'activation': 1},\n",
       "   {'uuid': UUID('afb62c6b-f640-4bd9-bbc3-7333cf548f64'), 'activation': 1},\n",
       "   {'uuid': UUID('dfb76ea4-0df7-430e-b76f-58f2bf06d633'), 'activation': 1},\n",
       "   {'uuid': UUID('d7c31949-af1f-41e5-a68b-995b9d1a7e32'), 'activation': 1},\n",
       "   {'uuid': UUID('5419e88d-7a78-49b2-8a18-9b7b0d6be021'), 'activation': 1},\n",
       "   {'uuid': UUID('8efb7347-0ec4-49c8-911e-4f8decc2a176'), 'activation': 1},\n",
       "   {'uuid': UUID('96135704-974d-4588-b827-caed6568a90a'), 'activation': 1},\n",
       "   {'uuid': UUID('d59c49ea-deef-440b-8741-4826589ed744'), 'activation': 1},\n",
       "   {'uuid': UUID('d2d422f8-58b9-4411-959c-049a7a71cec2'), 'activation': 1},\n",
       "   {'uuid': UUID('cd549f0e-0e1d-4079-bd36-618073cb15d3'), 'activation': 1},\n",
       "   {'uuid': UUID('ef37fd26-e391-471f-a2f1-08b461642183'), 'activation': 1},\n",
       "   {'uuid': UUID('4b3056a0-4f3b-495d-b4da-90a23c48a88e'), 'activation': 1},\n",
       "   {'uuid': UUID('c133e07b-af08-4aeb-9155-d203e9ed757f'), 'activation': 1},\n",
       "   {'uuid': UUID('fd02e3de-4ac0-4706-a3c8-01d2f67f6fb8'), 'activation': 1},\n",
       "   {'uuid': UUID('fc054e20-2c20-4290-b46c-ba3ecd6bae6a'), 'activation': 1},\n",
       "   {'uuid': UUID('caba68e3-d938-4438-b541-23bff9522147'), 'activation': 1},\n",
       "   {'uuid': UUID('862fcbbc-4e15-4f14-b79a-133a8124b666'), 'activation': 1},\n",
       "   {'uuid': UUID('de7f8a60-947f-4c90-a0c2-4245be3a9f29'), 'activation': 1},\n",
       "   {'uuid': UUID('b1872a0f-f8d1-499e-a12a-4b78cfcf09f9'), 'activation': 1},\n",
       "   {'uuid': UUID('cfaac716-d661-48d9-a408-e2ce0e7e35b0'), 'activation': 1},\n",
       "   {'uuid': UUID('e6973f5e-f5f7-4997-886b-13b65577b9b8'), 'activation': 1},\n",
       "   {'uuid': UUID('09eae0a8-0199-4d0d-acc7-5dedc66704a9'), 'activation': 1},\n",
       "   {'uuid': UUID('fba808c1-84b0-418d-a174-c74d82d84c5d'), 'activation': 1},\n",
       "   {'uuid': UUID('579c5f3e-6723-43e3-b1cf-228a15881f27'), 'activation': 1},\n",
       "   {'uuid': UUID('eb51f833-8faa-485b-9ae8-0eda6dbc430b'), 'activation': 1},\n",
       "   {'uuid': UUID('e851e2d5-88bd-414b-87ea-6cd78c974374'), 'activation': 1},\n",
       "   {'uuid': UUID('40fc2868-684e-4793-8ead-2e6791acc95a'), 'activation': 1},\n",
       "   {'uuid': UUID('1b4a888f-fa28-41c2-b442-ce6957841e5f'), 'activation': 1},\n",
       "   {'uuid': UUID('a81b27a2-9e9c-41c3-9daa-7fb90bf3346f'), 'activation': 1},\n",
       "   {'uuid': UUID('eb0f6b53-6a0e-4406-b766-b510b4ee5459'), 'activation': 1},\n",
       "   {'uuid': UUID('0f630862-bd66-4002-86e1-948197addfd1'), 'activation': 1},\n",
       "   {'uuid': UUID('d8121360-4d1b-4599-9225-8174e0d3dd9c'), 'activation': 1},\n",
       "   {'uuid': UUID('0a2efeb4-17a2-4e29-a5a2-53f1ba4ec342'), 'activation': 1},\n",
       "   {'uuid': UUID('4fac3d00-0cfd-4f87-a387-6d04b0537ce0'), 'activation': 1},\n",
       "   {'uuid': UUID('04c704a0-c2c2-4608-af89-75a41741b61c'), 'activation': 1},\n",
       "   {'uuid': UUID('1d278387-6f34-47c0-9d35-d78dbd4ee19e'), 'activation': 1},\n",
       "   {'uuid': UUID('2fa7704f-fc0d-4d6d-a6fe-193ab2fdd90b'), 'activation': 1},\n",
       "   {'uuid': UUID('97c5bc47-1d0e-4346-a643-9749d28e2ee8'), 'activation': 1},\n",
       "   {'uuid': UUID('20510269-d647-49c8-ac50-94c1a1b6c378'), 'activation': 1},\n",
       "   {'uuid': UUID('6a90b061-7f5e-4a4a-ab02-6869dd20467a'), 'activation': 1},\n",
       "   {'uuid': UUID('3a0f9054-a337-4231-a0ed-0808d9010567'), 'activation': 1},\n",
       "   {'uuid': UUID('5cfea54d-547d-4ff7-b262-cebff54e2a34'), 'activation': 1},\n",
       "   {'uuid': UUID('244c0dcd-4703-455a-bef1-2c46d81c0a85'), 'activation': 1},\n",
       "   {'uuid': UUID('5c7447b2-50b2-4e2c-b58c-004dd75c64f6'), 'activation': 1},\n",
       "   {'uuid': UUID('53f192ca-a794-440d-9735-13b210ba1b01'), 'activation': 1},\n",
       "   {'uuid': UUID('e4a0bd04-a1cb-44ba-9de9-e01fd40bc37b'), 'activation': 1},\n",
       "   {'uuid': UUID('f1323940-adbe-4c83-abd1-06f7ccd60e8d'), 'activation': 1},\n",
       "   {'uuid': UUID('f75ee2d0-bb64-4e95-88e9-2a288e38c0bc'), 'activation': 1},\n",
       "   {'uuid': UUID('35cf0b1c-3ad8-477b-a697-18e7af97fb3e'), 'activation': 1},\n",
       "   {'uuid': UUID('057bc84a-6d80-4043-831b-5e3bb3c35637'), 'activation': 1},\n",
       "   {'uuid': UUID('aed2684f-b536-45df-8ee1-7b9a12ecc099'), 'activation': 1},\n",
       "   {'uuid': UUID('696dd87f-b012-47d3-8461-cba9259976fa'), 'activation': 1},\n",
       "   {'uuid': UUID('efb98c56-062f-4fa6-9d68-8ab144863927'), 'activation': 1},\n",
       "   {'uuid': UUID('74a9e261-da95-4f29-b954-ff0fac44267c'), 'activation': 1},\n",
       "   {'uuid': UUID('b89db5ed-1bfc-4b56-be1c-1bde56de4ff3'), 'activation': 1},\n",
       "   {'uuid': UUID('3f72df38-ff8c-45ae-894e-67c35fa985ff'), 'activation': 1},\n",
       "   {'uuid': UUID('f8d8bb3e-61e6-49e6-b12b-0023b3c3618e'), 'activation': 1},\n",
       "   {'uuid': UUID('40982ad6-668e-4965-af54-4f15bf2caaeb'), 'activation': 1},\n",
       "   {'uuid': UUID('565f4ab6-b86b-41e6-937d-e7490793dbea'), 'activation': 1},\n",
       "   {'uuid': UUID('79ec550a-285a-47e9-bf7c-da42558edd16'), 'activation': 1}]}]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data\n",
    "feature_activations_known_1Q[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save progress\n",
    "import pickle\n",
    "\n",
    "# Save feature activations\n",
    "with open('../classifier/data_classifier/feature_activations_known_1Q.parquet', 'wb') as f:\n",
    "    pickle.dump(feature_activations_known_1Q, f)\n",
    "\n",
    "# Save feature library\n",
    "with open('../classifier/data_classifier/feature_library_known_1Q.parquet', 'wb') as f:\n",
    "    pickle.dump(feature_library_known_1Q, f)\n",
    "\n",
    "# Save feature activations\n",
    "with open('../classifier/data_classifier/feature_activations_unknown_1Q.parquet', 'wb') as f:\n",
    "    pickle.dump(feature_activations_unknown_1Q, f)\n",
    "\n",
    "# Save feature library\n",
    "with open('../classifier/data_classifier/feature_library_unknown_1Q.parquet', 'wb') as f:\n",
    "    pickle.dump(feature_library_unknown_1Q, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function\n",
    "results = analyze_feature_separation(\n",
    "    feature_activations_known_1Q,\n",
    "    feature_activations_unknown_1Q,\n",
    "    feature_library_known_1Q,\n",
    "    feature_library_unknown_1Q\n",
    ")\n",
    "\n",
    "# Convert to pandas for presentation\n",
    "feature_separations = pd.DataFrame(results)\n",
    "feature_separations.columns = ['label', 'feature_id', 'known_frac', 'unknown_frac', 'sep_score' ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_id</th>\n",
       "      <th>known_frac</th>\n",
       "      <th>unknown_frac</th>\n",
       "      <th>sep_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Instructions for natural/human-like writing and presentation style</td>\n",
       "      <td>a459ec5c-461c-4260-9faa-0366fa5ee37f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078443</td>\n",
       "      <td>-0.078443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Whitespace used for data structure formatting and alignment</td>\n",
       "      <td>48c3e38d-9b87-4152-b744-d0b240f7dff0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>-0.065868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Three digit numerical sequences</td>\n",
       "      <td>aea0c814-fb43-46b7-ae1f-56e21b3217a4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053293</td>\n",
       "      <td>-0.053293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Numbers between 15-30 in temporal or sequential contexts</td>\n",
       "      <td>68164743-afbf-4f47-8afb-0ed30b9dc0aa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051497</td>\n",
       "      <td>-0.051497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Expressions of absolute certainty or definitiveness</td>\n",
       "      <td>981ddf7c-2639-47f9-8574-1204437f281e</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044910</td>\n",
       "      <td>-0.044910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>References to people with disabilities or special needs</td>\n",
       "      <td>811dfbba-7bd0-44b8-a971-1252225c97b2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038922</td>\n",
       "      <td>-0.038922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Portuguese technical terms for customization and updates</td>\n",
       "      <td>29cfa1b3-e211-4f20-9f7b-8fa12ec5e415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038922</td>\n",
       "      <td>-0.038922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Historical immigration to America and its cultural symbolism</td>\n",
       "      <td>5223d8f2-6804-4235-8b49-c693e1a1bb03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037725</td>\n",
       "      <td>-0.037725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Small to medium sized numbers used for concrete quantities</td>\n",
       "      <td>14ecb2ec-c2d0-45b4-a821-db181a1a878d</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035329</td>\n",
       "      <td>-0.035329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>Word count specifications in writing prompts</td>\n",
       "      <td>9f4175de-7994-4168-9e07-e5e0fb86331c</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034132</td>\n",
       "      <td>-0.034132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>End of turn/message in conversation</td>\n",
       "      <td>e2b0ab89-8f8e-4b27-ad45-34cd7c729b19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026347</td>\n",
       "      <td>-0.026347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>Numbers specifying desired list length in requests</td>\n",
       "      <td>51855ce7-d0ef-4104-a226-d25a44e410ea</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025150</td>\n",
       "      <td>-0.025150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>Words indicating absoluteness or totality</td>\n",
       "      <td>670ce781-37ed-45d9-a79c-1e20d562a552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023952</td>\n",
       "      <td>-0.023952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>The user is requesting continuation or more content</td>\n",
       "      <td>dce5464e-3b90-409e-9cb2-11527a0512ec</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023353</td>\n",
       "      <td>-0.023353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>Numbers being read aloud digit-by-digit for verification</td>\n",
       "      <td>2e756245-da1a-4094-bb4f-4ac90a8bd5a6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020958</td>\n",
       "      <td>-0.020958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  label  \\\n",
       "208  Instructions for natural/human-like writing and presentation style   \n",
       "247         Whitespace used for data structure formatting and alignment   \n",
       "297                                     Three digit numerical sequences   \n",
       "309            Numbers between 15-30 in temporal or sequential contexts   \n",
       "361                 Expressions of absolute certainty or definitiveness   \n",
       "408             References to people with disabilities or special needs   \n",
       "410            Portuguese technical terms for customization and updates   \n",
       "418        Historical immigration to America and its cultural symbolism   \n",
       "446          Small to medium sized numbers used for concrete quantities   \n",
       "463                        Word count specifications in writing prompts   \n",
       "574                                 End of turn/message in conversation   \n",
       "589                  Numbers specifying desired list length in requests   \n",
       "615                           Words indicating absoluteness or totality   \n",
       "622                 The user is requesting continuation or more content   \n",
       "680            Numbers being read aloud digit-by-digit for verification   \n",
       "\n",
       "                               feature_id  known_frac  unknown_frac  sep_score  \n",
       "208  a459ec5c-461c-4260-9faa-0366fa5ee37f         0.0      0.078443  -0.078443  \n",
       "247  48c3e38d-9b87-4152-b744-d0b240f7dff0         0.0      0.065868  -0.065868  \n",
       "297  aea0c814-fb43-46b7-ae1f-56e21b3217a4         0.0      0.053293  -0.053293  \n",
       "309  68164743-afbf-4f47-8afb-0ed30b9dc0aa         0.0      0.051497  -0.051497  \n",
       "361  981ddf7c-2639-47f9-8574-1204437f281e         0.0      0.044910  -0.044910  \n",
       "408  811dfbba-7bd0-44b8-a971-1252225c97b2         0.0      0.038922  -0.038922  \n",
       "410  29cfa1b3-e211-4f20-9f7b-8fa12ec5e415         0.0      0.038922  -0.038922  \n",
       "418  5223d8f2-6804-4235-8b49-c693e1a1bb03         0.0      0.037725  -0.037725  \n",
       "446  14ecb2ec-c2d0-45b4-a821-db181a1a878d         0.0      0.035329  -0.035329  \n",
       "463  9f4175de-7994-4168-9e07-e5e0fb86331c         0.0      0.034132  -0.034132  \n",
       "574  e2b0ab89-8f8e-4b27-ad45-34cd7c729b19         0.0      0.026347  -0.026347  \n",
       "589  51855ce7-d0ef-4104-a226-d25a44e410ea         0.0      0.025150  -0.025150  \n",
       "615  670ce781-37ed-45d9-a79c-1e20d562a552         0.0      0.023952  -0.023952  \n",
       "622  dce5464e-3b90-409e-9cb2-11527a0512ec         0.0      0.023353  -0.023353  \n",
       "680  2e756245-da1a-4094-bb4f-4ac90a8bd5a6         0.0      0.020958  -0.020958  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate features most exclusive to unknown entities\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "feature_separations.sort_values(['known_frac','unknown_frac'], ascending=[True, False]).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature_id</th>\n",
       "      <th>known_frac</th>\n",
       "      <th>unknown_frac</th>\n",
       "      <th>sep_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Explanations of rare genetic disorders and syndromes</td>\n",
       "      <td>1292651f-8c52-4863-acf8-615b472c95bf</td>\n",
       "      <td>0.199434</td>\n",
       "      <td>0.356287</td>\n",
       "      <td>-0.156853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Requests for 2000-word technical articles about industrial topics</td>\n",
       "      <td>b0226353-caf5-41e8-a645-888e6ff42100</td>\n",
       "      <td>0.062235</td>\n",
       "      <td>0.183234</td>\n",
       "      <td>-0.120999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>References to having or experiencing medical/psychological disorders</td>\n",
       "      <td>7ac9f4da-3e62-48b8-8699-062101e362cb</td>\n",
       "      <td>0.181047</td>\n",
       "      <td>0.294012</td>\n",
       "      <td>-0.112965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Classical and technical word suffixes, especially in long words</td>\n",
       "      <td>adbdc27d-53f2-4398-b82b-6291c3d59e2c</td>\n",
       "      <td>0.222065</td>\n",
       "      <td>0.323353</td>\n",
       "      <td>-0.101288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Nonsensical or potentially harmful input requiring clarification or rejection</td>\n",
       "      <td>475a1ebc-1432-4f32-a8f8-f3398de51bec</td>\n",
       "      <td>0.214993</td>\n",
       "      <td>0.315569</td>\n",
       "      <td>-0.100576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Points in text where careful or diplomatic phrasing is needed</td>\n",
       "      <td>9761aacd-bf71-4dbc-a69a-bc8105688ce3</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.093413</td>\n",
       "      <td>-0.090584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Requests for concrete examples or specifications</td>\n",
       "      <td>ab25d2cf-a989-4d62-8834-17ff9da7ddf6</td>\n",
       "      <td>0.048091</td>\n",
       "      <td>0.130539</td>\n",
       "      <td>-0.082448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Small numbers (1-9) used for document structure and organization</td>\n",
       "      <td>04704c84-a9e6-4fc4-a9f9-03831f279691</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.080838</td>\n",
       "      <td>-0.079424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Instructions for natural/human-like writing and presentation style</td>\n",
       "      <td>a459ec5c-461c-4260-9faa-0366fa5ee37f</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078443</td>\n",
       "      <td>-0.078443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Setup patterns for content requiring ethical moderation</td>\n",
       "      <td>8fb94a59-f19a-4ed1-b987-037679479015</td>\n",
       "      <td>0.063649</td>\n",
       "      <td>0.141317</td>\n",
       "      <td>-0.077668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Template formatting patterns with parentheses and dashes</td>\n",
       "      <td>53afbd70-853e-4cbf-9722-2c3ef3d45188</td>\n",
       "      <td>0.192362</td>\n",
       "      <td>0.268263</td>\n",
       "      <td>-0.075901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Syntactic delimiters and formatting characters in structured text</td>\n",
       "      <td>ddfa1c50-bb46-4e22-bf2f-f9db1d4ed7d0</td>\n",
       "      <td>0.005658</td>\n",
       "      <td>0.072455</td>\n",
       "      <td>-0.066797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Whitespace used for data structure formatting and alignment</td>\n",
       "      <td>48c3e38d-9b87-4152-b744-d0b240f7dff0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>-0.065868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Word-final grammatical suffixes across multiple languages</td>\n",
       "      <td>1d704012-1af6-42bc-8dc6-4c695c12d0a8</td>\n",
       "      <td>0.130127</td>\n",
       "      <td>0.194611</td>\n",
       "      <td>-0.064483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Dramatic or fantastical narrative elements in storytelling</td>\n",
       "      <td>5ccbbab7-0739-4b38-949e-9dd873e4bd28</td>\n",
       "      <td>0.045262</td>\n",
       "      <td>0.107186</td>\n",
       "      <td>-0.061924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             label  \\\n",
       "50                            Explanations of rare genetic disorders and syndromes   \n",
       "87               Requests for 2000-word technical articles about industrial topics   \n",
       "58            References to having or experiencing medical/psychological disorders   \n",
       "53                 Classical and technical word suffixes, especially in long words   \n",
       "54   Nonsensical or potentially harmful input requiring clarification or rejection   \n",
       "172                  Points in text where careful or diplomatic phrasing is needed   \n",
       "116                               Requests for concrete examples or specifications   \n",
       "202               Small numbers (1-9) used for document structure and organization   \n",
       "208             Instructions for natural/human-like writing and presentation style   \n",
       "110                        Setup patterns for content requiring ethical moderation   \n",
       "65                        Template formatting patterns with parentheses and dashes   \n",
       "229              Syntactic delimiters and formatting characters in structured text   \n",
       "247                    Whitespace used for data structure formatting and alignment   \n",
       "81                       Word-final grammatical suffixes across multiple languages   \n",
       "148                     Dramatic or fantastical narrative elements in storytelling   \n",
       "\n",
       "                               feature_id  known_frac  unknown_frac  sep_score  \n",
       "50   1292651f-8c52-4863-acf8-615b472c95bf    0.199434      0.356287  -0.156853  \n",
       "87   b0226353-caf5-41e8-a645-888e6ff42100    0.062235      0.183234  -0.120999  \n",
       "58   7ac9f4da-3e62-48b8-8699-062101e362cb    0.181047      0.294012  -0.112965  \n",
       "53   adbdc27d-53f2-4398-b82b-6291c3d59e2c    0.222065      0.323353  -0.101288  \n",
       "54   475a1ebc-1432-4f32-a8f8-f3398de51bec    0.214993      0.315569  -0.100576  \n",
       "172  9761aacd-bf71-4dbc-a69a-bc8105688ce3    0.002829      0.093413  -0.090584  \n",
       "116  ab25d2cf-a989-4d62-8834-17ff9da7ddf6    0.048091      0.130539  -0.082448  \n",
       "202  04704c84-a9e6-4fc4-a9f9-03831f279691    0.001414      0.080838  -0.079424  \n",
       "208  a459ec5c-461c-4260-9faa-0366fa5ee37f    0.000000      0.078443  -0.078443  \n",
       "110  8fb94a59-f19a-4ed1-b987-037679479015    0.063649      0.141317  -0.077668  \n",
       "65   53afbd70-853e-4cbf-9722-2c3ef3d45188    0.192362      0.268263  -0.075901  \n",
       "229  ddfa1c50-bb46-4e22-bf2f-f9db1d4ed7d0    0.005658      0.072455  -0.066797  \n",
       "247  48c3e38d-9b87-4152-b744-d0b240f7dff0    0.000000      0.065868  -0.065868  \n",
       "81   1d704012-1af6-42bc-8dc6-4c695c12d0a8    0.130127      0.194611  -0.064483  \n",
       "148  5ccbbab7-0739-4b38-949e-9dd873e4bd28    0.045262      0.107186  -0.061924  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First view highest separation scores.., note this is not same as exclusivity\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "feature_separations.sort_values('sep_score', ascending=True).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These features are interesting, being RLHF about known vs unknown...\n",
    "\n",
    "**From the most exclusive to unknown entities...**\n",
    "- Instructions for natural/human-like writing and presentation style \n",
    "    - aka making stuff up????\n",
    "    - a459ec5c-461c-4260-9faa-0366fa5ee37f\n",
    "\n",
    "- Expressions of absolute certainty or definitiveness\n",
    "    - 981ddf7c-2639-47f9-8574-1204437f281e\n",
    "\n",
    "**From the highest separation scores...**\n",
    "\n",
    "- Nonsensical or potentially harmful input requiring clarification or rejection\n",
    "    - NICE, this is what we were hoping for!\n",
    "    - 475a1ebc-1432-4f32-a8f8-f3398de51bec\t\n",
    "\n",
    "- Points in text where careful or diplomatic phrasing is needed\n",
    "    - 9761aacd-bf71-4dbc-a69a-bc8105688ce3\n",
    "\n",
    "- Requests for concrete examples or specifications\n",
    "    - ab25d2cf-a989-4d62-8834-17ff9da7ddf6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbouring Features for:  Nonsensical or potentially harmful input requiring clarification or rejection\n",
      "--------------------------\n",
      "Requests for 2000-word technical articles about industrial topics\n",
      "Suffixes and word parts that make terms sound fantastical or science-fiction-like\n",
      "Academic punctuation patterns in references and formal lists\n",
      "Text corruption and encoding artifacts\n",
      "Formal conversation openings and polite transitions in professional discourse\n",
      "Brand names and product names that get split across multiple tokens\n",
      "Korean language text generation tokens\n",
      "Visualizing and organizing spatial relationships and structures\n",
      "Common connecting words and question markers in non-English languages\n",
      "Substrings common in made-up or invalid words across languages\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Feature(\"Nonsensical or potentially harmful input requiring clarification or rejection\")"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's investigate the top two features:\n",
    "# \"Instructions for natural/human-like writing and presentation style\"\n",
    "# \"Nonsensical or potentially harmful input requiring clarification or rejection\"\n",
    "\n",
    "get_feature_object('475a1ebc-1432-4f32-a8f8-f3398de51bec', 'Nonsensical or potentially harmful input requiring clarification or rejection', feature_activations_unknown_1Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbouring Features for:  Expressions of absolute certainty or definitiveness\n",
      "--------------------------\n",
      "Domain-specific information queries in professional or technical contexts\n",
      "The user has a question\n",
      "Physical infrastructure and hardware being investigated or analyzed\n",
      "Requests for creative content in series or sequence format\n",
      "Prepositions connecting qualifying specifications or requirements\n",
      "Offensive request from the user\n",
      "Difficult interpersonal conversations that need to be had\n",
      "Computer ports and connection interfaces\n",
      "Physical placement or installation of components and forces\n",
      "The user is making a request or asking for something to be done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Feature(\"Requests for concrete examples or specifications\")"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feature_object('ab25d2cf-a989-4d62-8834-17ff9da7ddf6', 'Expressions of absolute certainty or definitiveness', feature_activations_unknown_1Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: 'Nonsensical or potentially harmful input requiring clarification or rejection'\n",
    "\n",
    "This feature is as predicted by Ferrando et al for unknown entities likely to cause hallucination. \n",
    "Its an RLHF trained rejection of nonsensical inputs, as discussed in their paper.\n",
    "It enjoys a realtively high separation score between unknown and known entities in the dataset, ranked no. 5.\n",
    "\n",
    "BUT, why is the separation score relatively low, just 10%?\n",
    "\n",
    "This is likely an issue with the quality of separation of the data and the model used for chat completion (scoring):\n",
    "- For example, breast cancer appears in the 'unknown' entites, but I find this hard to believe.\n",
    "More likely, the breast cancer questions were simply too difficult to get perfectly correct.\n",
    "- Furthermore, many of the 'known' entities may actually have been 'uncertain' rather than fully 'known', hence 21% of known entities activate this feature.\n",
    "- Finally, the scoring was done using the Turbo version of LLM on Together.ai, because it is fast and reliable. But that version is quantised and does not perform the same as the standard model, which is used by Goodfire for the SAE. We'd have used Goodfire for scoring, but their chat completion API repeatedly failed after just 100 calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
