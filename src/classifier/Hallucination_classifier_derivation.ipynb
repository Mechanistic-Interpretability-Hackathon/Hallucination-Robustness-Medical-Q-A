{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def get_balanced_samples(df: pd.DataFrame, \n",
    "                        n_per_class: Optional[int] = None,\n",
    "                        train_fraction: float = 0.8,\n",
    "                        random_state: Optional[int] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Randomly sample an equal number of records where hallucinated is True and False,\n",
    "    split into training and test sets, and format prompts for each row.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with 'hallucinated', 'question', and 'options' columns\n",
    "        n_per_class (int, optional): Number of samples to take from each class.\n",
    "                                   If None, uses the size of the smaller class.\n",
    "        train_fraction (float): Fraction of data to use for training (default: 0.8)\n",
    "        random_state (int, optional): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: (train_df, test_df) containing balanced samples\n",
    "                                         with formatted prompts\n",
    "    \"\"\"\n",
    "    if not 0 < train_fraction < 1:\n",
    "        raise ValueError(\"train_fraction must be between 0 and 1\")\n",
    "    \n",
    "    # Ensure hallucinated column is boolean\n",
    "    df = df.copy()\n",
    "\n",
    "    # Filter by activation if the column exists\n",
    "    if 'activation' in df.columns:\n",
    "        original_len = len(df)\n",
    "        df = df[df['activation'] == 0]\n",
    "        filtered_len = len(df)\n",
    "\n",
    "        if filtered_len == 0:\n",
    "            raise ValueError(\"No rows remaining after filtering activation = 0\")\n",
    "\n",
    "    # Ensure hallucinated column is boolean\n",
    "    df['hallucinated'] = df['hallucinated'].astype(bool)\n",
    "    \n",
    "    # Split into True and False groups\n",
    "    true_samples = df[df['hallucinated'] == True]\n",
    "    false_samples = df[df['hallucinated'] == False]\n",
    "    \n",
    "    # Get counts\n",
    "    n_true = len(true_samples)\n",
    "    n_false = len(false_samples)\n",
    "    \n",
    "    # If n_per_class not specified, use size of smaller group\n",
    "    if n_per_class is None:\n",
    "        n_per_class = min(n_true, n_false)\n",
    "    \n",
    "    # Verify we have enough samples\n",
    "    if n_per_class > min(n_true, n_false):\n",
    "        raise ValueError(f\"Requested {n_per_class} samples per class but smallest class only has {min(n_true, n_false)} samples\")\n",
    "    \n",
    "    # Sample from each group\n",
    "    sampled_true = true_samples.sample(n=n_per_class, random_state=random_state)\n",
    "    sampled_false = false_samples.sample(n=n_per_class, random_state=random_state)\n",
    "    \n",
    "    # Calculate number of training samples (ensuring even split between classes)\n",
    "    n_train_per_class = int(n_per_class * train_fraction)\n",
    "    \n",
    "    # Split each class into train and test\n",
    "    train_true = sampled_true.iloc[:n_train_per_class]\n",
    "    test_true = sampled_true.iloc[n_train_per_class:]\n",
    "    \n",
    "    train_false = sampled_false.iloc[:n_train_per_class]\n",
    "    test_false = sampled_false.iloc[n_train_per_class:]\n",
    "    \n",
    "    # Combine and shuffle train and test sets\n",
    "    train_df = pd.concat([train_true, train_false])\n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    test_df = pd.concat([test_true, test_false])\n",
    "    test_df = test_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Format prompts for both datasets\n",
    "    def format_prompts(df):\n",
    "        introduction = (\"You are a medical expert and this is a multiple choice exam question. \"\n",
    "                       \"Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\")\n",
    "        \n",
    "        formatted_df = df.copy()\n",
    "        formatted_prompts = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            question = row['question']\n",
    "            \n",
    "            # Parse options\n",
    "            if isinstance(row['options'], str):\n",
    "                options_dict = ast.literal_eval(row['options'])\n",
    "            elif isinstance(row['options'], list) and len(row['options']) > 0:\n",
    "                options_dict = row['options'][0]\n",
    "            else:\n",
    "                options_dict = row['options']\n",
    "            \n",
    "            # Filter out 'correct answer' from options\n",
    "            options_filtered = {k: v for k, v in options_dict.items() if k != 'correct answer'}\n",
    "            options_formatted = \"Options: \" + json.dumps(options_filtered)\n",
    "            \n",
    "            # Construct prompt\n",
    "            prompt = f\"{introduction}\\n\\n{question}\\n\\n{options_formatted}\"\n",
    "            formatted_prompts.append(prompt)\n",
    "        \n",
    "        formatted_df['prompt'] = formatted_prompts\n",
    "        return formatted_df\n",
    "    \n",
    "    # Apply prompt formatting to both datasets\n",
    "    train_df = format_prompts(train_df)\n",
    "    test_df = format_prompts(test_df)\n",
    "    \n",
    "    print(f\"Created balanced samples with {n_per_class} records per class\")\n",
    "    print(f\"Training set: {len(train_df)} records ({n_train_per_class} per class)\")\n",
    "    print(f\"Test set: {len(test_df)} records ({n_per_class - n_train_per_class} per class)\")\n",
    "    print(\"\\nClass distribution in training set:\")\n",
    "    print(train_df['hallucinated'].value_counts())\n",
    "    print(\"\\nClass distribution in test set:\")\n",
    "    print(test_df['hallucinated'].value_counts())\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created balanced samples with 1172 records per class\n",
      "Training set: 1874 records (937 per class)\n",
      "Test set: 470 records (235 per class)\n",
      "\n",
      "Class distribution in training set:\n",
      "hallucinated\n",
      "False    937\n",
      "True     937\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in test set:\n",
      "hallucinated\n",
      "True     235\n",
      "False    235\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/baseline_results.tsv'\n",
    "\n",
    "# Read the TSV file\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Get balanced samples with train/test split\n",
    "train_data, val_data = get_balanced_samples(\n",
    "    df=df,\n",
    "    # n_per_class=125,\n",
    "    train_fraction=0.8,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of records: 7100\n",
      "hallucinated: 1172\n",
      "proportion hallucinated: 0.16507042253521126\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of records:\", len(df))\n",
    "print(\"hallucinated:\", df['hallucinated'].sum())\n",
    "print(\"proportion hallucinated:\",  df['hallucinated'].sum()/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7100, 9), (1874, 9), (470, 9))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import goodfire as gf\n",
    "import os\n",
    "\n",
    "# Get API key\n",
    "api_key = os.getenv('GOODFIRE_API_KEY')\n",
    "client  = gf.Client(api_key)\n",
    "variant = gf.Variant(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "# 1. Prepare Dataset\n",
    "def prepare_dataset(train_data: pd.DataFrame, hallucinated: bool=True) -> Tuple[List, pd.DataFrame]: \n",
    "  \"\"\"Prepare a dataset from training data based on hallucinated or truthful examples. \n",
    "  Args:\n",
    "      train_data (pd.DataFrame): DataFrame containing training data with 'hallucinated' and 'prompt' columns.\n",
    "      hallucinated (bool): Boolean indicating whether to filter for hallucinated examples.\n",
    "  Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - dataset (list): A list of conversations in the required format for each example.\n",
    "            - filtered_examples (pd.DataFrame): The filtered DataFrame containing only the examples \n",
    "                                                corresponding to the specified hallucinated value.\n",
    "  \"\"\"\n",
    "  filtered_examples = train_data[train_data['hallucinated'] == hallucinated]\n",
    "  dataset = [\n",
    "    [\n",
    "      {\"role\": \"user\", \"content\": prompt},\n",
    "      {\"role\": \"assistant\", \"content\": \"3\"} \n",
    "    ] for prompt in filtered_examples[\"prompt\"].tolist()\n",
    "  ]\n",
    "  return dataset, filtered_examples\n",
    "\n",
    "\n",
    "# 2. Chunk Dataset\n",
    "def chunk_dataset(dataset, chunk_size=64): \n",
    "  \"\"\"\n",
    "    Split a dataset into smaller chunks for API processing.\n",
    "    Args:\n",
    "        dataset: The dataset to chunk.\n",
    "        chunk_size: Maximum size of each chunk.\n",
    "    Yields:\n",
    "        Chunks of the dataset.\n",
    "    \"\"\"\n",
    "  for i in range(0, len(dataset), chunk_size):\n",
    "    yield dataset[i:i+chunk_size]\n",
    "    \n",
    "\n",
    "# 3. Contrast Features\n",
    "def contrast_features(client, variant, dataset_1_chunks, dataset_2_chunks, top_k=50):\n",
    "  \"\"\"\n",
    "    Perform feature contrast for two datasets using Goodfire.\n",
    "    Args:\n",
    "        client: The Goodfire client instance.\n",
    "        variant: The model variant to use.\n",
    "        dataset_1_chunks: List of dataset chunks for the first class.\n",
    "        dataset_2_chunks: List of dataset chunks for the second class.\n",
    "        top_k: Number of top features to extract.\n",
    "    Returns:\n",
    "        A tuple of feature groups for dataset_1 and dataset_2.\n",
    "    \"\"\" \n",
    "  hallucinated_features = None  # Initialize to None\n",
    "  truthful_features = None \n",
    "  \n",
    "  for d1_chunk, d2_chunk in zip(dataset_1_chunks, dataset_2_chunks):\n",
    "    hallucinated_chunk_features, truthful_chunk_features = client.features.contrast(\n",
    "      dataset_1=d1_chunk, \n",
    "      dataset_2=d2_chunk, \n",
    "      model=variant, \n",
    "      top_k=top_k\n",
    "    )\n",
    "    if hallucinated_features is None:\n",
    "        hallucinated_features = hallucinated_chunk_features\n",
    "    else:\n",
    "        hallucinated_features = hallucinated_features.union(hallucinated_chunk_features)\n",
    "\n",
    "    if truthful_features is None:\n",
    "        truthful_features = truthful_chunk_features\n",
    "    else:\n",
    "        truthful_features = truthful_features.union(truthful_chunk_features)\n",
    "\n",
    "  return hallucinated_features, truthful_features\n",
    "  \n",
    "\n",
    "\n",
    "# 4. Rerank Features\n",
    "def rerank_features(client, features, query, model, top_k=50):\n",
    "  \"\"\"\n",
    "  Rerank features based on a query.\n",
    "  Args:\n",
    "      client: The Goodfire client instance.\n",
    "      features: The features to rerank.\n",
    "      query: Query string for reranking.\n",
    "      model: The model variant to use.\n",
    "      top_k: Number of top features to return.\n",
    "  Returns:\n",
    "      Reranked features.\n",
    "  \"\"\"\n",
    "  return client.features.rerank(\n",
    "    features=features,\n",
    "    query=query,\n",
    "    model=model,\n",
    "    top_k=top_k\n",
    "  )\n",
    "  \n",
    "  \n",
    "# 5. Combine Features \n",
    "def combine_features(hallucinated_features, truthful_features):\n",
    "  \"\"\"\n",
    "  Combine two sets of features into a single set.\n",
    "  Args:\n",
    "      hallucinated_features: Features from the hallucinated dataset.\n",
    "      truthful_features: Features from the truthful dataset.\n",
    "  Returns:\n",
    "      A combined set of features.\n",
    "  \"\"\"\n",
    "  return hallucinated_features | truthful_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup([\n",
      "   0: \"Medical report negation of concerning findings\",\n",
      "   1: \"Normal or unremarkable findings in medical reports\",\n",
      "   2: \"Medical history sections and references in clinical documentation\",\n",
      "   3: \"Chronic medical conditions with complex or contested etiologies\",\n",
      "   4: \"Medical comorbidities and condition combinations\",\n",
      "   5: \"Medical symptom descriptions using tentative language\",\n",
      "   6: \"Medical diagnostic procedures and test ordering sequences\",\n",
      "   7: \"Medical history documentation patterns in clinical settings\",\n",
      "   8: \"Vital signs measurements in medical documentation\",\n",
      "   ...\n",
      "   49: \"Medical descriptions of skin conditions and dermatological issues\"\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare datasets\n",
    "dataset_1, hallucinated_examples = prepare_dataset(train_data, hallucinated=True)\n",
    "dataset_2, truthful_examples = prepare_dataset(train_data, hallucinated=False)\n",
    "\n",
    "# Step 2: Chunk datasets\n",
    "chunk_size = 64\n",
    "dataset_1_chunks = list(chunk_dataset(dataset_1, chunk_size))\n",
    "dataset_2_chunks = list(chunk_dataset(dataset_2, chunk_size))\n",
    "\n",
    "# Step 3: Contrast features\n",
    "hallucinated_features, truthful_features = contrast_features(\n",
    "    client,\n",
    "    variant,\n",
    "    dataset_1_chunks,\n",
    "    dataset_2_chunks,\n",
    "    top_k=50\n",
    ")\n",
    "# query1_ = \"medical response hallucinated\",\n",
    "# Step 4: Rerank features\n",
    "hallucinated_features = rerank_features(\n",
    "    client,\n",
    "    hallucinated_features,\n",
    "    query=\"unknown medical query\",\n",
    "    model=variant,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "truthful_features = rerank_features(\n",
    "    client,\n",
    "    truthful_features,\n",
    "    query=\"medical query\",\n",
    "    model=variant,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "# Step 5: Combine features\n",
    "features_to_look_at = combine_features(hallucinated_features, truthful_features)\n",
    "\n",
    "# Final Output\n",
    "print(features_to_look_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "class FeatureSearch:\n",
    "    \"\"\"A class for systematically searching through \n",
    "      combinations of features to evaluate their predictive power.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_group):\n",
    "        self.feature_group = feature_group\n",
    "\n",
    "    def grid(self, k_features_per_combo: int = 2):\n",
    "        \"\"\"Perform a grid search over all possible combinations of features.\n",
    "        Args:\n",
    "            k_features_per_combo (int): The number of features to include in each combination.\n",
    "        Returns:\n",
    "            list: All possible k-sized combinations of features from the feature group.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all possible combinations of features\n",
    "        return list(combinations(self.feature_group, k_features_per_combo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_client = gf.AsyncClient(api_key=api_key, base_url=\"https://api.goodfire.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hallucinated_examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masyncio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_asyncio\n\u001b[1;32m      6\u001b[0m MIN_SAMPLES_PER_CLASS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[43mhallucinated_examples\u001b[49m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mlen\u001b[39m(truthful_examples),\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_feature_acts_for_sample_class\u001b[39m(\n\u001b[1;32m     12\u001b[0m     sample_class: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[1;32m     13\u001b[0m     features_to_use_for_classification: gf\u001b[38;5;241m.\u001b[39mFeatureGroup,\n\u001b[1;32m     14\u001b[0m     k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     15\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     16\u001b[0m ):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(features_to_use_for_classification):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hallucinated_examples' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "\n",
    "MIN_SAMPLES_PER_CLASS = min(\n",
    "    len(hallucinated_examples),\n",
    "    len(truthful_examples),\n",
    ")\n",
    "\n",
    "async def _get_feature_acts_for_sample_class(\n",
    "    sample_class: pd.DataFrame,\n",
    "    features_to_use_for_classification: gf.FeatureGroup,\n",
    "    k=50,\n",
    "    batch_size=50\n",
    "):\n",
    "    if k < len(features_to_use_for_classification):\n",
    "        raise ValueError(\n",
    "            \"k must be greater than the number of features to use for classification\"\n",
    "        )\n",
    "\n",
    "    samples = []\n",
    "    all_samples = sample_class[0:MIN_SAMPLES_PER_CLASS]\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(all_samples), batch_size):\n",
    "        batch = all_samples[i:i + batch_size]\n",
    "        tasks = []\n",
    "\n",
    "        for idx, row in batch.iterrows():\n",
    "            prompt = row[\"prompt\"]\n",
    "            tasks.append(\n",
    "                async_client.features.inspect(\n",
    "                    [\n",
    "                        {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "                        {\"role\": \"assistant\", \"content\": \"3\"}\n",
    "                    ],\n",
    "                    model=variant,\n",
    "                    features=features_to_use_for_classification,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Process this batch\n",
    "        batch_results = await tqdm_asyncio.gather(*tasks)\n",
    "        for context in batch_results:\n",
    "            features = context.top(k=k)\n",
    "            samples.append(features)\n",
    "\n",
    "    return samples\n",
    "\n",
    "async def process_all_classes():\n",
    "    print(\"Computing truthful features...\")\n",
    "    truthful_class_features = await _get_feature_acts_for_sample_class(\n",
    "        truthful_examples, features_to_look_at, k=50\n",
    "    )\n",
    "\n",
    "    print(\"Computing hallucinated features...\")\n",
    "    hallucinated_class_features = await _get_feature_acts_for_sample_class(\n",
    "        hallucinated_examples, features_to_look_at, k=50\n",
    "    )\n",
    "\n",
    "\n",
    "    return truthful_class_features, hallucinated_class_features\n",
    "\n",
    "truthful_class_features, hallucinated_class_features = await process_all_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing truthful features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1: 100%|██████████| 100/100 [00:20<00:00,  4.90it/s]\n",
      "Processing batch 2: 100%|██████████| 100/100 [00:19<00:00,  5.13it/s]\n",
      "Processing batch 3: 100%|██████████| 100/100 [00:19<00:00,  5.10it/s]\n",
      "Processing batch 4: 100%|██████████| 100/100 [00:19<00:00,  5.25it/s]\n",
      "Processing batch 5: 100%|██████████| 100/100 [00:19<00:00,  5.11it/s]\n",
      "Processing batch 6: 100%|██████████| 100/100 [00:19<00:00,  5.13it/s]\n",
      "Processing batch 7: 100%|██████████| 100/100 [00:19<00:00,  5.08it/s]\n",
      "Processing batch 8: 100%|██████████| 100/100 [00:20<00:00,  4.83it/s]\n",
      "Processing batch 9: 100%|██████████| 100/100 [00:20<00:00,  4.94it/s]\n",
      "Processing batch 10: 100%|██████████| 37/37 [00:09<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hallucinated features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1: 100%|██████████| 100/100 [00:19<00:00,  5.19it/s]\n",
      "Processing batch 2: 100%|██████████| 100/100 [00:19<00:00,  5.01it/s]\n",
      "Processing batch 3: 100%|██████████| 100/100 [00:18<00:00,  5.30it/s]\n",
      "Processing batch 4: 100%|██████████| 100/100 [00:19<00:00,  5.18it/s]\n",
      "Processing batch 5: 100%|██████████| 100/100 [00:19<00:00,  5.04it/s]\n",
      "Processing batch 6: 100%|██████████| 100/100 [00:18<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry 1/3 for prompt 'You are a medical expert and this is a multiple choice exam question. Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\n",
      "\n",
      "To examine the vaginal cells in of the rape victim test used is -\n",
      "\n",
      "Options: {\"0\": \"Lugol's iodine test\", \"1\": \"Takayama test\", \"2\": \"Florence test\", \"3\": \"Precipitin test\"}' failed with error: [Errno 8] nodename nor servname provided, or not known\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 7: 100%|██████████| 100/100 [00:21<00:00,  4.59it/s]\n",
      "Processing batch 8: 100%|██████████| 100/100 [00:18<00:00,  5.31it/s]\n",
      "Processing batch 9: 100%|██████████| 100/100 [00:19<00:00,  5.11it/s]\n",
      "Processing batch 10: 100%|██████████| 37/37 [00:08<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import httpx\n",
    "\n",
    "MIN_SAMPLES_PER_CLASS = min(\n",
    "    len(hallucinated_examples),\n",
    "    len(truthful_examples),\n",
    ")\n",
    "\n",
    "async def inspect_with_retry(client, prompt, model, features, retries=3, delay=2):\n",
    "    \"\"\"\n",
    "    Inspect features with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        client: The async client instance.\n",
    "        prompt: The prompt to send to the model.\n",
    "        model: The model to use.\n",
    "        features: The feature group to inspect.\n",
    "        retries: Maximum number of retries.\n",
    "        delay: Delay between retries in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        The response from the inspect call.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return await client.features.inspect(\n",
    "                [\n",
    "                    {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"3\"}\n",
    "                ],\n",
    "                model=model,\n",
    "                features=features,\n",
    "            )\n",
    "        except (httpx.ConnectError, httpx.ReadTimeout) as e:\n",
    "            print(f\"Retry {attempt + 1}/{retries} for prompt '{prompt}' failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                await asyncio.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "async def _get_feature_acts_for_sample_class(\n",
    "    sample_class: pd.DataFrame,\n",
    "    features_to_use_for_classification: gf.FeatureGroup,\n",
    "    k=50,\n",
    "    batch_size=100,\n",
    "    retries=3,\n",
    "    delay=2\n",
    "):\n",
    "    if k < len(features_to_use_for_classification):\n",
    "        raise ValueError(\n",
    "            \"k must be greater than the number of features to use for classification\"\n",
    "        )\n",
    "\n",
    "    samples = []\n",
    "    all_samples = sample_class[0:MIN_SAMPLES_PER_CLASS]\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(all_samples), batch_size):\n",
    "        batch = all_samples[i:i + batch_size]\n",
    "        tasks = []\n",
    "\n",
    "        for idx, row in batch.iterrows():\n",
    "            prompt = row[\"prompt\"]\n",
    "            tasks.append(\n",
    "                inspect_with_retry(\n",
    "                    async_client,\n",
    "                    prompt,\n",
    "                    model=variant,\n",
    "                    features=features_to_use_for_classification,\n",
    "                    retries=retries,\n",
    "                    delay=delay\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Process this batch\n",
    "        batch_results = await tqdm_asyncio.gather(*tasks, desc=f\"Processing batch {i // batch_size + 1}\")\n",
    "        for context in batch_results:\n",
    "            features = context.top(k=k)\n",
    "            samples.append(features)\n",
    "\n",
    "    return samples\n",
    "\n",
    "async def process_all_classes():\n",
    "    print(\"Computing truthful features...\")\n",
    "    truthful_class_features = await _get_feature_acts_for_sample_class(\n",
    "        truthful_examples, features_to_look_at, k=50\n",
    "    )\n",
    "\n",
    "    print(\"Computing hallucinated features...\")\n",
    "    hallucinated_class_features = await _get_feature_acts_for_sample_class(\n",
    "        hallucinated_examples, features_to_look_at, k=50\n",
    "    )\n",
    "\n",
    "    return truthful_class_features, hallucinated_class_features\n",
    "\n",
    "# Run the processing\n",
    "truthful_class_features, hallucinated_class_features = await process_all_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 937)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hallucinated_examples), len(truthful_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(truthful_class_features[0]), len(hallucinated_class_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the data to a file\n",
    "def save_feature_activations(name: str, feature_activations):\n",
    "  with open(f\"../data/{name}.pkl\", \"wb\") as f:\n",
    "      pickle.dump(feature_activations, f)\n",
    "\n",
    "\n",
    "save_feature_activations(\"truthful_class_features\", truthful_class_features)\n",
    "save_feature_activations(\"hallucinated_class_features\", hallucinated_class_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the data from a file\n",
    "def load_feature_activations(name: str):\n",
    "  with open(f\"../data/{name}.pkl\", \"rb\") as f:\n",
    "      activations = pickle.load(f)\n",
    "  return activations\n",
    "\n",
    "truthful_class_features = load_feature_activations('truthful_class_features')\n",
    "hallucinated_class_features = load_feature_activations('hallucinated_class_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# Grid search may take a while, you can curate the feature list to speed this process up significantly\n",
    "\n",
    "def train_tree(x, y, depth):\n",
    "  train_x, test_x, train_y, test_y = train_test_split(\n",
    "    x, y, train_size=0.5, random_state=42)\n",
    "\n",
    "  # Create a nice regularized tree\n",
    "  model = DecisionTreeClassifier(\n",
    "    max_depth=depth,\n",
    "    min_samples_leaf=len(train_x) // 10,\n",
    "    random_state=42\n",
    "  )\n",
    "\n",
    "  model.fit(train_x, train_y)\n",
    "\n",
    "  pred = model.predict(test_x)\n",
    "\n",
    "  # Calculate the f1 score of the model\n",
    "  accuracy = balanced_accuracy_score(test_y, pred)\n",
    "  score = f1_score(test_y, pred)\n",
    "\n",
    "  return model, pred, score,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_combo(features, k_features_per_combo=2):\n",
    "    combos = FeatureSearch(features).grid(k_features_per_combo=k_features_per_combo)\n",
    "    best_combo = None\n",
    "    best_model = None\n",
    "    mean_act_negative = 0\n",
    "    mean_act_positive = 0\n",
    "    support_vector_distances = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for combo in tqdm.tqdm(combos):\n",
    "        # Create a linear regression model\n",
    "        def _select_feature_acts(combo, row):\n",
    "            output = []\n",
    "            for index, feature in enumerate(combo):\n",
    "                for feature_act in row:\n",
    "                    if feature_act.feature.uuid == feature.uuid:\n",
    "                        output.append(feature_act.activation)\n",
    "                        break\n",
    "\n",
    "            return output\n",
    "\n",
    "        x_hallucinated = [\n",
    "            _select_feature_acts(combo, row) for row in hallucinated_class_features\n",
    "        ]\n",
    "        x_truthful = [\n",
    "            _select_feature_acts(combo, row) for row in truthful_class_features\n",
    "        ]\n",
    "\n",
    "        y_hallucinated = [-1] * len(x_hallucinated)\n",
    "        y_truthful = [1] * len(x_truthful)\n",
    "\n",
    "        x = x_hallucinated + x_truthful\n",
    "        y = y_truthful + y_hallucinated\n",
    "\n",
    "        model, pred, score = train_tree(x, y, depth=len(combo))\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_combo = combo\n",
    "            best_model = model\n",
    "\n",
    "    return best_combo, best_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 119.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (Feature(\"Medical history documentation patterns in clinical settings\"),) 0.642965204236006 DecisionTreeClassifier(max_depth=1, min_samples_leaf=93, random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1225/1225 [00:15<00:00, 80.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (Feature(\"Medical history documentation patterns in clinical settings\"), Feature(\"Vital signs measurements in medical documentation\")) 0.642965204236006 DecisionTreeClassifier(max_depth=2, min_samples_leaf=93, random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1825/19600 [00:31<05:08, 57.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m best_combo_at_k \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     best_combo, best_score, best_model \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_combo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfeatures_to_look_at\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_features_per_combo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, best_combo, best_score, best_model)\n\u001b[1;32m      8\u001b[0m     best_combo_at_k[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m (best_combo, best_score, best_model)\n",
      "Cell \u001b[0;32mIn[34], line 35\u001b[0m, in \u001b[0;36mfind_best_combo\u001b[0;34m(features, k_features_per_combo)\u001b[0m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m x_hallucinated \u001b[38;5;241m+\u001b[39m x_truthful\n\u001b[1;32m     33\u001b[0m y \u001b[38;5;241m=\u001b[39m y_truthful \u001b[38;5;241m+\u001b[39m y_hallucinated\n\u001b[0;32m---> 35\u001b[0m model, pred, score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcombo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m best_score:\n\u001b[1;32m     38\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m score\n",
      "Cell \u001b[0;32mIn[33], line 26\u001b[0m, in \u001b[0;36mtrain_tree\u001b[0;34m(x, y, depth)\u001b[0m\n\u001b[1;32m     23\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_x)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate the f1 score of the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mbalanced_accuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m score \u001b[38;5;241m=\u001b[39m f1_score(test_y, pred)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, pred, score,\n",
      "File \u001b[0;32m~/Documents/Projects/A.I./Prototype/Hallucination-Robustness-Medical-Q-A/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/A.I./Prototype/Hallucination-Robustness-Medical-Q-A/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2520\u001b[0m, in \u001b[0;36mbalanced_accuracy_score\u001b[0;34m(y_true, y_pred, sample_weight, adjusted)\u001b[0m\n\u001b[1;32m   2442\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2443\u001b[0m     {\n\u001b[1;32m   2444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2450\u001b[0m )\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbalanced_accuracy_score\u001b[39m(y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, adjusted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2452\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the balanced accuracy.\u001b[39;00m\n\u001b[1;32m   2453\u001b[0m \n\u001b[1;32m   2454\u001b[0m \u001b[38;5;124;03m    The balanced accuracy in binary and multiclass classification problems to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2518\u001b[0m \u001b[38;5;124;03m    np.float64(0.625)\u001b[39;00m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2520\u001b[0m     C \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2522\u001b[0m         per_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(C) \u001b[38;5;241m/\u001b[39m C\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/A.I./Prototype/Hallucination-Robustness-Medical-Q-A/.venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/A.I./Prototype/Hallucination-Robustness-Medical-Q-A/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:340\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03mBy definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m(np.int64(0), np.int64(2), np.int64(1), np.int64(1))\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[0;32m--> 340\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[0;32m~/Documents/Projects/A.I./Prototype/Hallucination-Robustness-Medical-Q-A/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:99\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m     98\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m---> 99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/Documents/Projects/A.I./Prototype/Hallucination-Robustness-Medical-Q-A/.venv/lib/python3.11/site-packages/sklearn/utils/multiclass.py:333\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name, raise_unknown)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/A.I./Prototype/Hallucination-Robustness-Medical-Q-A/.venv/lib/python3.11/site-packages/sklearn/utils/multiclass.py:172\u001b[0m, in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, VisibleDeprecationWarning)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Projects/A.I./Prototype/Hallucination-Robustness-Medical-Q-A/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/A.I./Prototype/Hallucination-Robustness-Medical-Q-A/.venv/lib/python3.11/site-packages/sklearn/utils/_array_api.py:832\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    830\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 832\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_combo_at_k = {}\n",
    "\n",
    "for i in range(4):\n",
    "    best_combo, best_score, best_model = find_best_combo(\n",
    "      features_to_look_at, k_features_per_combo = i + 1\n",
    "      )\n",
    "    print(i + 1, best_combo, best_score, best_model)\n",
    "    best_combo_at_k[i + 1] = (best_combo, best_score, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1 features: score=0.642965204236006\n",
      "k=2 features: score=0.642965204236006\n",
      "k=3 features: score=0.642965204236006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Feature(\"Medical history documentation patterns in clinical settings\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in [1, 2, 3]:\n",
    "    combo, score, model = best_combo_at_k[k]\n",
    "    print(f\"k={k} features: score={score}\")\n",
    "    \n",
    "best_individual_feature = best_combo_at_k[3][0][0]\n",
    "best_individual_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureGroup([\n",
       "   0: \"Section headers and structural elements in medical records and forms, especially patient history and allergies\",\n",
       "   1: \"Medical history sections and references in clinical documentation\",\n",
       "   2: \"Medical warnings about pre-existing conditions and health status\",\n",
       "   3: \"Medical comorbidities and condition combinations\",\n",
       "   4: \"References to people who have experienced trauma or hardship\",\n",
       "   5: \"The AI explaining it cannot remember past conversations\",\n",
       "   6: \"Descriptions of someone's qualifications and professional background in evaluative contexts\",\n",
       "   7: \"Patient history and symptom description in clinical documentation\",\n",
       "   8: \"Offensive request from the user\",\n",
       "   9: \"Phrases establishing credibility through accumulated experience\"\n",
       "])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_grp = client.features.neighbors(best_individual_feature, model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\" )\n",
    "feature_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((Feature(\"Medical history documentation patterns in clinical settings\"),\n",
       "  Feature(\"Vital signs measurements in medical documentation\"),\n",
       "  Feature(\"Inappropriate content focusing on bathroom urgency and loss of control\")),\n",
       " DecisionTreeClassifier(max_depth=3, min_samples_leaf=93, random_state=42))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anyways let's look at the best overall tree\n",
    "\n",
    "BEST_TREE_INDEX = 3\n",
    "best_features = best_combo_at_k[BEST_TREE_INDEX][0]\n",
    "best_tree = best_combo_at_k[BEST_TREE_INDEX][2]\n",
    "\n",
    "best_features, best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.0 (20241103.1931)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"514pt\" height=\"314pt\"\n",
       " viewBox=\"0.00 0.00 514.38 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-310 510.38,-310 510.38,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#fefcfa\" stroke=\"black\" d=\"M494.38,-306C494.38,-306 123.62,-306 123.62,-306 117.62,-306 111.62,-300 111.62,-294 111.62,-294 111.62,-235 111.62,-235 111.62,-229 117.62,-223 123.62,-223 123.62,-223 494.38,-223 494.38,-223 500.38,-223 506.38,-229 506.38,-235 506.38,-235 506.38,-294 506.38,-294 506.38,-300 500.38,-306 494.38,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"119.62\" y=\"-288.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Medical history documentation patterns in clinical settings ≤ 6.5</text>\n",
       "<text text-anchor=\"start\" x=\"281.25\" y=\"-273.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"264.38\" y=\"-258.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 937</text>\n",
       "<text text-anchor=\"start\" x=\"243.75\" y=\"-243.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [475.0, 462.0]</text>\n",
       "<text text-anchor=\"start\" x=\"265.12\" y=\"-228.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = truthful</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#f8fcfe\" stroke=\"black\" d=\"M352,-187C352,-187 12,-187 12,-187 6,-187 0,-181 0,-175 0,-175 0,-116 0,-116 0,-110 6,-104 12,-104 12,-104 352,-104 352,-104 358,-104 364,-110 364,-116 364,-116 364,-175 364,-175 364,-181 358,-187 352,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-169.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Vital signs measurements in medical documentation ≤ 1.5</text>\n",
       "<text text-anchor=\"start\" x=\"154.25\" y=\"-154.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"137.38\" y=\"-139.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 843</text>\n",
       "<text text-anchor=\"start\" x=\"128\" y=\"-124.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [414, 429]</text>\n",
       "<text text-anchor=\"start\" x=\"122.75\" y=\"-109.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = hallucinated</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M264.58,-222.58C254.86,-213.62 244.48,-204.06 234.47,-194.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"237.02,-192.43 227.29,-188.23 232.28,-197.58 237.02,-192.43\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.2\" y=\"-206.38\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#f3c5a4\" stroke=\"black\" d=\"M479.5,-179.5C479.5,-179.5 394.5,-179.5 394.5,-179.5 388.5,-179.5 382.5,-173.5 382.5,-167.5 382.5,-167.5 382.5,-123.5 382.5,-123.5 382.5,-117.5 388.5,-111.5 394.5,-111.5 394.5,-111.5 479.5,-111.5 479.5,-111.5 485.5,-111.5 491.5,-117.5 491.5,-123.5 491.5,-123.5 491.5,-167.5 491.5,-167.5 491.5,-173.5 485.5,-179.5 479.5,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"401.75\" y=\"-162.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.456</text>\n",
       "<text text-anchor=\"start\" x=\"396.12\" y=\"-147.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 94</text>\n",
       "<text text-anchor=\"start\" x=\"390.5\" y=\"-132.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [61, 33]</text>\n",
       "<text text-anchor=\"start\" x=\"393.12\" y=\"-117.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = truthful</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M353.77,-222.58C366.19,-211.23 379.67,-198.9 392.09,-187.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"394.37,-190.21 399.39,-180.88 389.64,-185.05 394.37,-190.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"399.38\" y=\"-199.04\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#fdfeff\" stroke=\"black\" d=\"M161.25,-68C161.25,-68 50.75,-68 50.75,-68 44.75,-68 38.75,-62 38.75,-56 38.75,-56 38.75,-12 38.75,-12 38.75,-6 44.75,0 50.75,0 50.75,0 161.25,0 161.25,0 167.25,0 173.25,-6 173.25,-12 173.25,-12 173.25,-56 173.25,-56 173.25,-62 167.25,-68 161.25,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"78.25\" y=\"-50.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"61.38\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 668</text>\n",
       "<text text-anchor=\"start\" x=\"52\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [332, 336]</text>\n",
       "<text text-anchor=\"start\" x=\"46.75\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = hallucinated</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.7,-103.73C147.81,-95.24 141.59,-86.28 135.66,-77.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.55,-75.76 129.98,-69.54 132.8,-79.75 138.55,-75.76\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#e8f3fc\" stroke=\"black\" d=\"M314.25,-68C314.25,-68 203.75,-68 203.75,-68 197.75,-68 191.75,-62 191.75,-56 191.75,-56 191.75,-12 191.75,-12 191.75,-6 197.75,0 203.75,0 203.75,0 314.25,0 314.25,0 320.25,0 326.25,-6 326.25,-12 326.25,-12 326.25,-56 326.25,-56 326.25,-62 320.25,-68 314.25,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"223.75\" y=\"-50.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.498</text>\n",
       "<text text-anchor=\"start\" x=\"214.38\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 175</text>\n",
       "<text text-anchor=\"start\" x=\"201.25\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [82.0, 93.0]</text>\n",
       "<text text-anchor=\"start\" x=\"199.75\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = hallucinated</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M210.67,-103.73C216.7,-95.15 223.07,-86.09 229.15,-77.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"231.82,-79.73 234.71,-69.54 226.1,-75.7 231.82,-79.73\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x296120810>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visualize the tree\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "  best_tree, \n",
    "  out_file=None, \n",
    "  feature_names=[feature.label for feature in best_features], \n",
    "  class_names=['truthful', 'hallucinated'], \n",
    "  filled=True, \n",
    "  rounded=True, \n",
    "  special_characters=True\n",
    "  )\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def run_in_thread(func, *args, **kwargs):\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "    # Call the function directly if it's not async\n",
    "    result = func(*args, **kwargs)\n",
    "\n",
    "    loop.close()\n",
    "    return result\n",
    "\n",
    "def get_feature_activations(client, variant, examples, features, k=50):\n",
    "    \"\"\"\n",
    "    Get feature activations for a set of examples using Goodfire with run_in_thread\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures_list = []\n",
    "\n",
    "        for example in examples:\n",
    "            futures_list.append(\n",
    "                executor.submit(\n",
    "                    run_in_thread,\n",
    "                    client.features.inspect,  # Async function\n",
    "                    example,\n",
    "                    model=variant,\n",
    "                    features=features,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for future in tqdm.tqdm(futures_list, desc=\"Fetching Features\"):\n",
    "            context = future.result()\n",
    "            features = context.top(k=k)\n",
    "            samples.append(features)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user',\n",
       "   'content': 'You are a medical expert and this is a multiple choice exam question. Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\\n\\nRapid tooth separation works on following principle:\\n\\nOptions: {\"0\": \"Wedge principle.\", \"1\": \"Traction principle.\", \"2\": \"Both of the above.\", \"3\": \"None.\"}'},\n",
       "  {'role': 'assistant', 'content': '3'}]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Features: 100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FeatureActivations(\n",
       "    0: (Feature(\"Vital signs measurements in medical documentation\"), 1)\n",
       "    1: (Feature(\"Medical history documentation patterns in clinical settings\"), 0)\n",
       "    2: (Feature(\"Inappropriate content focusing on bathroom urgency and loss of control\"), 0)\n",
       "    3: (Feature(\"Variable assignments and value retrievals in calculator code\"), 0)\n",
       "    4: (Feature(\"Birth dates in biographical/historical contexts\"), 0)\n",
       "    5: (Feature(\"Syntactical elements in programming code and database queries\"), 0)\n",
       "    6: (Feature(\"Discussion of limited or scarce resources and constraints\"), 0)\n",
       "    7: (Feature(\"Syntactical special characters and delimiters in programming contexts\"), 0)\n",
       "    8: (Feature(\"feature_0\"), 0)\n",
       "    9: (Feature(\"The Russian word состав (composition/compilation) and its variations\"), 0)\n",
       "    ...\n",
       "    22: (Feature(\"Technical documentation describing widespread applications and uses\"), 0)\n",
       " )]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_activations = get_feature_activations(client, variant, dataset_1[0:1], best_features)\n",
    "example_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import goodfire as gf\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import numpy as np\n",
    "\n",
    "class HallucinationClassifier:\n",
    "    def __init__(self, model_path: str, api_key: str):\n",
    "        \"\"\"\n",
    "        Initialize the hallucination classifier with a saved model and features.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the saved pickle file containing both the model and features\n",
    "            api_key: Goodfire API key for accessing the service\n",
    "        \"\"\"\n",
    "        # Load the model and features\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "            self.model = model_data['model']\n",
    "            self.features = model_data['features']\n",
    "        self.client = gf.Client(api_key)\n",
    "        self.variant = gf.Variant(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "    def _format_prompt(self, question: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Format a question into the expected prompt structure.\"\"\"\n",
    "        introduction = (\"You are a medical expert and this is a multiple choice exam question. \"\n",
    "                      \"Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\")\n",
    "        return [{\"role\": \"user\", \"content\": f\"{introduction}\\n\\n{question}\"}]\n",
    "      \n",
    "\n",
    "    def _get_feature_activations(self, prompt: List[Dict[str, str]]) -> List[float]:\n",
    "        \"\"\"Get feature activations for the input prompt.\"\"\"\n",
    "        context = self.client.features.inspect(\n",
    "            messages=prompt,\n",
    "            model=self.variant,\n",
    "            features=self.features\n",
    "        )\n",
    "        \n",
    "        # Get activations for our specific features\n",
    "        activations = []\n",
    "        features_dict = {f.uuid: 0.0 for f in self.features}\n",
    "        \n",
    "        for feature_act in context.top(k=len(self.features)):\n",
    "            if feature_act.feature.uuid in features_dict:\n",
    "                features_dict[feature_act.feature.uuid] = feature_act.activation\n",
    "        \n",
    "        # Maintain order matching the original features\n",
    "        for feature in self.features:\n",
    "            activations.append(features_dict[feature.uuid])\n",
    "            \n",
    "        return activations\n",
    "\n",
    "    def predict(self, question: str, debug: bool = False) -> Tuple[int, float]:\n",
    "        \"\"\"\n",
    "        Predict whether a given question-answer pair is likely to contain hallucination.\n",
    "        \n",
    "        Args:\n",
    "            question: The question text\n",
    "            debug: If True, print debugging information about feature activations\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - Prediction (0 for truthful, 1 for hallucinated)\n",
    "            - Confidence score (probability of the predicted class)\n",
    "        \"\"\"\n",
    "        # Format the prompt\n",
    "        prompt = self._format_prompt(question)\n",
    "        \n",
    "        # Get feature activations\n",
    "        activations = self._get_feature_activations(prompt)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"\\nFeature Activations:\")\n",
    "            for feature, activation in zip(self.features, activations):\n",
    "                print(f\"{feature.label}: {activation:.4f}\")\n",
    "            \n",
    "            # Get the decision path\n",
    "            decision_path = self.model.decision_path([activations])\n",
    "            feature_importance = self.model.feature_importances_\n",
    "            \n",
    "            print(\"\\nFeature Importance in Model:\")\n",
    "            for feature, importance in zip(self.features, feature_importance):\n",
    "                print(f\"{feature.label}: {importance:.4f}\")\n",
    "            \n",
    "            print(\"\\nDecision Path:\")\n",
    "            node_indicator = decision_path[0]\n",
    "            leaf_id = self.model.apply([activations])[0]\n",
    "            \n",
    "            # Get thresholds and feature indices for each node in path\n",
    "            for node_id in node_indicator.indices:\n",
    "                if node_id != leaf_id:\n",
    "                    feature_idx = self.model.tree_.feature[node_id]\n",
    "                    threshold = self.model.tree_.threshold[node_id]\n",
    "                    feature_name = self.features[feature_idx].label\n",
    "                    feature_value = activations[feature_idx]\n",
    "                    print(f\"Node {node_id}: {feature_name} = {feature_value:.4f} {'<=' if feature_value <= threshold else '>'} {threshold:.4f}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict([activations])[0]\n",
    "        probabilities = self.model.predict_proba([activations])[0]\n",
    "        confidence = probabilities[prediction]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nProbabilities:\")\n",
    "            print(f\"Truthful: {probabilities[0]:.4f}\")\n",
    "            print(f\"Hallucinated: {probabilities[1]:.4f}\")\n",
    "        \n",
    "        return int(prediction), float(confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_features(model, features, output_path: str):\n",
    "    \"\"\"Save both the sklearn decision tree model and Goodfire features to a file.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained sklearn decision tree model\n",
    "        features: The Goodfire features used by the model\n",
    "        output_path: Path where to save the pickle file\n",
    "    \"\"\"\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'features': features\n",
    "    }\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and best_features\n",
    "\n",
    "model_path = \"hallucination_model.pkl\"\n",
    "save_model_and_features(best_tree, best_features, model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "  with open(model_path, 'rb') as f:\n",
    "   return pickle.load(f) \n",
    "  \n",
    "model = load_model(model_path=model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier (now simpler, no need to pass features separately)\n",
    "classifier = HallucinationClassifier(\n",
    "    model_path=model_path,\n",
    "    api_key=api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a medical expert and this is a multiple choice exam question. Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\n",
      "\n",
      "A karyotype 45, XX, rob (14; 21) (q10; q10) can correspond to a woman:\n",
      "\n",
      "Options: {\"0\": \"With Down syndrome with a Robertsonian translocation.\", \"1\": \"Sana, carrier of a Robertsonian translocation.\", \"2\": \"With Prader-Willi syndrome.\", \"3\": \"With Edwards syndrome.\"}\n",
      "\n",
      "Feature Activations:\n",
      "Medical history documentation patterns in clinical settings: 0.0000\n",
      "Vital signs measurements in medical documentation: 0.0000\n",
      "Inappropriate content focusing on bathroom urgency and loss of control: 0.0000\n",
      "\n",
      "Feature Importance in Model:\n",
      "Medical history documentation patterns in clinical settings: 0.9495\n",
      "Vital signs measurements in medical documentation: 0.0505\n",
      "Inappropriate content focusing on bathroom urgency and loss of control: 0.0000\n",
      "\n",
      "Decision Path:\n",
      "Node 0: Medical history documentation patterns in clinical settings = 0.0000 <= 6.5000\n",
      "Node 1: Vital signs measurements in medical documentation = 0.0000 <= 1.5000\n",
      "\n",
      "Probabilities:\n",
      "Truthful: 0.4970\n",
      "Hallucinated: 0.5030\n",
      "Prediction:  1\n",
      "Prediction: Hallucinated\n",
      "Confidence: 0.50\n"
     ]
    }
   ],
   "source": [
    "# Make predictions - Example from truthful dataframe\n",
    "\n",
    "prompt_example = truthful_examples.iloc[20]['prompt']\n",
    "print(prompt_example)\n",
    "\n",
    "prediction, confidence = classifier.predict(prompt_example,debug=True)\n",
    "print(\"Prediction: \", prediction)\n",
    "print(f\"Prediction: {'Hallucinated' if prediction == 1 else 'Truthful'}\")\n",
    "print(f\"Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION ON TRAIN & TEST DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model_predictions(classifier, truthful_examples, hallucinated_examples):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions across all examples.\n",
    "    \n",
    "    Args:\n",
    "        classifier: The HallucinationClassifier instance\n",
    "        truthful_examples: DataFrame containing truthful examples\n",
    "        hallucinated_examples: DataFrame containing hallucinated examples\n",
    "    \"\"\"\n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Process truthful examples\n",
    "    print(\"\\nProcessing truthful examples...\")\n",
    "    for idx, row in tqdm(truthful_examples.iterrows(), total=len(truthful_examples)):\n",
    "        prediction, confidence = classifier.predict(row['prompt'])\n",
    "        results.append({\n",
    "            'true_label': 'truthful',\n",
    "            'predicted': 'hallucinated' if prediction == 1 else 'truthful',\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    # Process hallucinated examples\n",
    "    print(\"\\nProcessing hallucinated examples...\")\n",
    "    for idx, row in tqdm(hallucinated_examples.iterrows(), total=len(hallucinated_examples)):\n",
    "        prediction, confidence = classifier.predict(row['prompt'])\n",
    "        results.append({\n",
    "            'true_label': 'hallucinated',\n",
    "            'predicted': 'hallucinated' if prediction == 1 else 'truthful',\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    total_predictions = len(results_df)\n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(f\"Total examples evaluated: {total_predictions}\")\n",
    "    \n",
    "    # Prediction distribution\n",
    "    pred_dist = results_df['predicted'].value_counts()\n",
    "    print(\"\\nPrediction Distribution:\")\n",
    "    for pred, count in pred_dist.items():\n",
    "        percentage = (count/total_predictions) * 100\n",
    "        print(f\"{pred}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    confusion = pd.crosstab(results_df['true_label'], results_df['predicted'])\n",
    "    print(confusion)\n",
    "    \n",
    "    # Calculate metrics by class\n",
    "    print(\"\\nMetrics by True Label:\")\n",
    "    for label in ['truthful', 'hallucinated']:\n",
    "        class_results = results_df[results_df['true_label'] == label]\n",
    "        correct = (class_results['true_label'] == class_results['predicted']).sum()\n",
    "        total = len(class_results)\n",
    "        accuracy = (correct/total) * 100\n",
    "        avg_confidence = class_results['confidence'].mean()\n",
    "        \n",
    "        print(f\"\\n{label.title()} Examples:\")\n",
    "        print(f\"Accuracy: {accuracy:.1f}%\")\n",
    "        print(f\"Average Confidence: {avg_confidence:.3f}\")\n",
    "    \n",
    "    # Return the results DataFrame for further analysis if needed\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN DATASET EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, hallucinated_examples_test = prepare_dataset(val_data, hallucinated=True)\n",
    "_, truthful_examples_test = prepare_dataset(val_data, hallucinated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing truthful examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [09:25<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing hallucinated examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235/235 [09:04<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Statistics:\n",
      "Total examples evaluated: 470\n",
      "\n",
      "Prediction Distribution:\n",
      "hallucinated: 414 (88.1%)\n",
      "truthful: 56 (11.9%)\n",
      "\n",
      "Confusion Matrix:\n",
      "predicted     hallucinated  truthful\n",
      "true_label                          \n",
      "hallucinated           209        26\n",
      "truthful               205        30\n",
      "\n",
      "Metrics by True Label:\n",
      "\n",
      "Truthful Examples:\n",
      "Accuracy: 12.8%\n",
      "Average Confidence: 0.489\n",
      "\n",
      "Hallucinated Examples:\n",
      "Accuracy: 88.9%\n",
      "Average Confidence: 0.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## TRAINING DATASET EVALUATION\n",
    "\n",
    "# Assuming classifier is already initialized\n",
    "results_df = evaluate_model_predictions(classifier, truthful_examples_test, hallucinated_examples_test)\n",
    "\n",
    "# You can do additional analysis on results_df if needed\n",
    "# For example, look at high confidence mistakes:\n",
    "high_conf_mistakes = results_df[\n",
    "    (results_df['true_label'] != results_df['predicted']) & \n",
    "    (results_df['confidence'] > 0.8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [true_label, predicted, confidence]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_conf_mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Performance Interpretation\n",
    "\n",
    "The recall of 86% is quite good - the model catches 86% of all hallucinations. This aligns with your goal of preventing medical hallucinations, as it misses relatively few of them.\n",
    "The precision of 59.7% means that when the model flags something as a hallucination, it's right about 60% of the time. The relatively lower precision indicates that the model is somewhat aggressive in flagging content as hallucinated.\n",
    "This trade-off (high recall, moderate precision) might be acceptable for the medical use case, as:\n",
    "\n",
    "In medical contexts, missing a hallucination (false negative) could be more dangerous than incorrectly flagging truthful content (false positive)\n",
    "False positives can be manually reviewed, while missed hallucinations might go unnoticed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST DATASET EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
