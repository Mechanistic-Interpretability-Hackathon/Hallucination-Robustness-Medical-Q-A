{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def get_balanced_samples(df: pd.DataFrame, \n",
    "                        n_per_class: Optional[int] = None,\n",
    "                        train_fraction: float = 0.8,\n",
    "                        random_state: Optional[int] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Randomly sample an equal number of records where hallucinated is True and False,\n",
    "    split into training and test sets, and format prompts for each row.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with 'hallucinated', 'question', and 'options' columns\n",
    "        n_per_class (int, optional): Number of samples to take from each class.\n",
    "                                   If None, uses the size of the smaller class.\n",
    "        train_fraction (float): Fraction of data to use for training (default: 0.8)\n",
    "        random_state (int, optional): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: (train_df, test_df) containing balanced samples\n",
    "                                         with formatted prompts\n",
    "    \"\"\"\n",
    "    if not 0 < train_fraction < 1:\n",
    "        raise ValueError(\"train_fraction must be between 0 and 1\")\n",
    "    \n",
    "    # Ensure hallucinated column is boolean\n",
    "    df = df.copy()\n",
    "\n",
    "    # Filter by activation if the column exists\n",
    "    if 'activation' in df.columns:\n",
    "        original_len = len(df)\n",
    "        df = df[df['activation'] == 0]\n",
    "        filtered_len = len(df)\n",
    "\n",
    "        if filtered_len == 0:\n",
    "            raise ValueError(\"No rows remaining after filtering activation = 0\")\n",
    "\n",
    "    # Ensure hallucinated column is boolean\n",
    "    df['hallucinated'] = df['hallucinated'].astype(bool)\n",
    "    \n",
    "    # Split into True and False groups\n",
    "    true_samples = df[df['hallucinated'] == True]\n",
    "    false_samples = df[df['hallucinated'] == False]\n",
    "    \n",
    "    # Get counts\n",
    "    n_true = len(true_samples)\n",
    "    n_false = len(false_samples)\n",
    "    \n",
    "    # If n_per_class not specified, use size of smaller group\n",
    "    if n_per_class is None:\n",
    "        n_per_class = min(n_true, n_false)\n",
    "    \n",
    "    # Verify we have enough samples\n",
    "    if n_per_class > min(n_true, n_false):\n",
    "        raise ValueError(f\"Requested {n_per_class} samples per class but smallest class only has {min(n_true, n_false)} samples\")\n",
    "    \n",
    "    # Sample from each group\n",
    "    sampled_true = true_samples.sample(n=n_per_class, random_state=random_state)\n",
    "    sampled_false = false_samples.sample(n=n_per_class, random_state=random_state)\n",
    "    \n",
    "    # Calculate number of training samples (ensuring even split between classes)\n",
    "    n_train_per_class = int(n_per_class * train_fraction)\n",
    "    \n",
    "    # Split each class into train and test\n",
    "    train_true = sampled_true.iloc[:n_train_per_class]\n",
    "    test_true = sampled_true.iloc[n_train_per_class:]\n",
    "    \n",
    "    train_false = sampled_false.iloc[:n_train_per_class]\n",
    "    test_false = sampled_false.iloc[n_train_per_class:]\n",
    "    \n",
    "    # Combine and shuffle train and test sets\n",
    "    train_df = pd.concat([train_true, train_false])\n",
    "    train_df = train_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    test_df = pd.concat([test_true, test_false])\n",
    "    test_df = test_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    # Format prompts for both datasets\n",
    "    def format_prompts(df):\n",
    "        introduction = (\"You are a medical expert and this is a multiple choice exam question. \"\n",
    "                       \"Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\")\n",
    "        \n",
    "        formatted_df = df.copy()\n",
    "        formatted_prompts = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            question = row['question']\n",
    "            \n",
    "            # Parse options\n",
    "            if isinstance(row['options'], str):\n",
    "                options_dict = ast.literal_eval(row['options'])\n",
    "            elif isinstance(row['options'], list) and len(row['options']) > 0:\n",
    "                options_dict = row['options'][0]\n",
    "            else:\n",
    "                options_dict = row['options']\n",
    "            \n",
    "            # Filter out 'correct answer' from options\n",
    "            options_filtered = {k: v for k, v in options_dict.items() if k != 'correct answer'}\n",
    "            options_formatted = \"Options: \" + json.dumps(options_filtered)\n",
    "            \n",
    "            # Construct prompt\n",
    "            prompt = f\"{introduction}\\n\\n{question}\\n\\n{options_formatted}\"\n",
    "            formatted_prompts.append(prompt)\n",
    "        \n",
    "        formatted_df['prompt'] = formatted_prompts\n",
    "        return formatted_df\n",
    "    \n",
    "    # Apply prompt formatting to both datasets\n",
    "    train_df = format_prompts(train_df)\n",
    "    test_df = format_prompts(test_df)\n",
    "    \n",
    "    print(f\"Created balanced samples with {n_per_class} records per class\")\n",
    "    print(f\"Training set: {len(train_df)} records ({n_train_per_class} per class)\")\n",
    "    print(f\"Test set: {len(test_df)} records ({n_per_class - n_train_per_class} per class)\")\n",
    "    print(\"\\nClass distribution in training set:\")\n",
    "    print(train_df['hallucinated'].value_counts())\n",
    "    print(\"\\nClass distribution in test set:\")\n",
    "    print(test_df['hallucinated'].value_counts())\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/oliver/Downloads/fct_responses_clean.tsv'\n",
    "\n",
    "# Read the TSV file\n",
    "df = pd.read_csv(file_path, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of records: 4438\n",
      "hallucinated: 1185\n",
      "proportion hallucinated: 0.26701216764308244\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of records:\", len(df))\n",
    "print(\"hallucinated:\", df['hallucinated'].sum())\n",
    "print(\"proportion hallucinated:\",  df['hallucinated'].sum()/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created balanced samples with 1000 records per class\n",
      "Training set: 1600 records (800 per class)\n",
      "Test set: 400 records (200 per class)\n",
      "\n",
      "Class distribution in training set:\n",
      "hallucinated\n",
      "True     800\n",
      "False    800\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in test set:\n",
      "hallucinated\n",
      "False    200\n",
      "True     200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get balanced samples with train/test split\n",
    "train_data, test_data = get_balanced_samples(\n",
    "    df=df,\n",
    "    n_per_class=1000,\n",
    "    train_fraction=0.8,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import goodfire\n",
    "import os\n",
    "\n",
    "api_key ='sk-goodfire-9IJgLomji2zNdvFLPsTYPQvPPr_kUC19bFTh0HgT9h6SikyfPB7WmQ'\n",
    "client  = goodfire.Client(api_key)\n",
    "variant = goodfire.Variant(\"meta-llama/Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucinated_examples = train_data[train_data['hallucinated'] == True]\n",
    "truthful_examples     = train_data[train_data['hallucinated'] == False]\n",
    "\n",
    "hallucinated_examples_test = test_data[test_data['hallucinated'] == True]\n",
    "truthful_examples_test     = test_data[test_data['hallucinated'] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_1 = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"3\"}\n",
    "    ] for prompt in hallucinated_examples['prompt'][0:100].tolist()\n",
    "]\n",
    "\n",
    "dataset_2 = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"3\"}\n",
    "    ] for prompt in truthful_examples['prompt'][0:100].tolist()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hallucinated_features, truthful_features = client.features.contrast(\n",
    "    dataset_1=dataset_1,\n",
    "    dataset_2=dataset_2,\n",
    "    dataset_1_feature_rerank_query=\"medical response hallucinated\",\n",
    "    dataset_2_feature_rerank_query=\"medical response truthful\",\n",
    "    model=variant,\n",
    "    top_k=50\n",
    ")\n",
    "features_to_look_at = hallucinated_features | truthful_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureGroup([\n",
       "   0: \"Detection of new input or conversation start\",\n",
       "   1: \"Conversation turn structure and role transitions\",\n",
       "   2: \"Beginning of a new conversation or input\",\n",
       "   3: \"Medical case presentations with complex patient symptoms\",\n",
       "   4: \"The model should not recommend technological or medical interventions\",\n",
       "   5: \"Medical imaging techniques and procedures\",\n",
       "   6: \"Website disclaimers about automated translation services\",\n",
       "   7: \"End of phrase or question punctuation\",\n",
       "   8: \"Practical application of logical conditionals and qualifiers in structured interactions\",\n",
       "   ...\n",
       "   52: \"Character-level programming syntax patterns\"\n",
       "])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_look_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_feature_activations(client, variant, examples, features, k=50):\n",
    "    \"\"\"\n",
    "    Get feature activations for a set of examples using Goodfire\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    with futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures_list = []\n",
    "\n",
    "        for example in examples:\n",
    "            futures_list.append(\n",
    "                executor.submit(\n",
    "                    client.features.inspect,\n",
    "                    example,\n",
    "                    model=variant,\n",
    "                    features=features,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for future in tqdm(futures_list):\n",
    "            context = future.result()\n",
    "            features = context.top(k=k)\n",
    "            samples.append(features)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_hal = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"3\"}\n",
    "    ] for prompt in hallucinated_examples['prompt'].tolist()\n",
    "]\n",
    "\n",
    "dataset_tru = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"3\"}\n",
    "    ] for prompt in truthful_examples['prompt'].tolist()\n",
    "]\n",
    "\n",
    "dataset_hal_test = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"3\"}\n",
    "    ] for prompt in hallucinated_examples_test['prompt'].tolist()\n",
    "]\n",
    "\n",
    "dataset_tru_test = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"3\"}\n",
    "    ] for prompt in truthful_examples_test['prompt'].tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing feature activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [04:56<00:00,  2.70it/s]\n",
      "100%|██████████| 800/800 [04:40<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get feature activations for each class\n",
    "print(\"Computing feature activations...\")\n",
    "\n",
    "hallucinated_activations = get_feature_activations(client, variant, dataset_hal, features_to_look_at)\n",
    "truthful_activations     = get_feature_activations(client, variant, dataset_tru, features_to_look_at)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:16<00:00,  2.61it/s]\n",
      "100%|██████████| 200/200 [01:11<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "hallucinated_activations_test = get_feature_activations(client, variant, dataset_hal_test, features_to_look_at)\n",
    "truthful_activations_test     = get_feature_activations(client, variant, dataset_tru_test, features_to_look_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_matrix(feature_activations, features):\n",
    "    \"\"\"\n",
    "    Convert feature activations into a matrix for training\n",
    "    \"\"\"\n",
    "    def _select_feature_acts(features, row):\n",
    "        output = []\n",
    "        for feature in features:\n",
    "            found = False\n",
    "            for feature_act in row:\n",
    "                if feature_act.feature.uuid == feature.uuid:\n",
    "                    output.append(feature_act.activation)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                output.append(0.0)  # Default value if feature not found\n",
    "        return output\n",
    "\n",
    "    X = [_select_feature_acts(features, row) for row in feature_activations]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_hallucinate =  800\n",
      "Length of X_truthful  =  800\n",
      "Example of X_hallucinate:\n",
      "  [75.0, 87.0, 83.0, 0, 0, 0, 0, 0, 0.0, 0, 0.3743489583333333, 0, 1.109375, 0, 0, 0.50390625, 0, 0, 0.4147135416666667, 0.6178385416666666, 0, 0, 0.4599609375, 0, 0, 0.0, 0.291015625, 0, 0, 0, 0, 0, 0.447265625, 0, 0, 0, 0, 0, 0.2578125, 0, 0, 0, 0, 0, 0.267578125, 0.7265625, 0, 0, 0, 0, 0, 0, 0.0]\n",
      "Len of example:\n",
      "  53\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature matrix\n",
    "X_hallucinate = prepare_feature_matrix(hallucinated_activations, features_to_look_at)\n",
    "X_truthful    = prepare_feature_matrix(truthful_activations, features_to_look_at)\n",
    "\n",
    "X_hallucinate_test = prepare_feature_matrix(hallucinated_activations_test, features_to_look_at)\n",
    "X_truthful_test    = prepare_feature_matrix(truthful_activations_test, features_to_look_at)\n",
    "\n",
    "# view example, we expect 50 features\n",
    "print(\"Length of X_hallucinate = \", len(X_hallucinate))\n",
    "print(\"Length of X_truthful  = \", len(X_truthful))\n",
    "print(\"Example of X_hallucinate:\\n \", X_hallucinate[0])\n",
    "print(\"Len of example:\\n \", len(X_hallucinate[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressibility\n",
    "\n",
    "We have 100 samples of hallucinated and 100 truthful, but 53 predictors. Could overfit.\n",
    "\n",
    "What is compressibility of this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, NamedTuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class DatasetStats(NamedTuple):\n",
    "  position_variances: np.ndarray  # Variance at each position across all examples\n",
    "  top_variant_positions: List[int]  # Indices of positions with highest variance\n",
    "  position_activity: np.ndarray  # Percentage of non-zero values at each position\n",
    "  mean_vector: np.ndarray  # Mean value at each position\n",
    "  std_vector: np.ndarray  # Standard deviation at each position\n",
    "  sparsity: float  # Overall sparsity of the dataset\n",
    "\n",
    "def analyze_datasets(examples, n_top_positions = 5):\n",
    "  \"\"\"\n",
    "  Analyze multiple examples simultaneously to find the most variant positions.\n",
    "\n",
    "  Args:\n",
    "      examples: List of examples, where each example is a list of float values\n",
    "      n_top_positions: Number of top variant positions to identify\n",
    "\n",
    "  Returns:\n",
    "      DatasetStats containing analysis results\n",
    "  \"\"\"\n",
    "  # Convert to numpy array for efficient computation\n",
    "  data = np.array(examples)\n",
    "\n",
    "  # Calculate variance at each position\n",
    "  position_variances = np.var(data, axis=0)\n",
    "\n",
    "  # Get indices of positions with highest variance\n",
    "  top_variant_positions = np.argsort(position_variances)[-n_top_positions:].tolist()[::-1]\n",
    "\n",
    "  # Calculate percentage of non-zero values at each position\n",
    "  position_activity = np.mean(data != 0, axis=0) * 100\n",
    "\n",
    "  # Calculate mean and std at each position\n",
    "  mean_vector = np.mean(data, axis=0)\n",
    "  std_vector = np.std(data, axis=0)\n",
    "\n",
    "  # Calculate overall sparsity\n",
    "  sparsity = np.mean(data == 0) * 100\n",
    "\n",
    "  return DatasetStats(\n",
    "      position_variances=position_variances,\n",
    "      top_variant_positions=top_variant_positions,\n",
    "      position_activity=position_activity,\n",
    "      mean_vector=mean_vector,\n",
    "      std_vector=std_vector,\n",
    "      sparsity=sparsity\n",
    "  )\n",
    "\n",
    "def print_analysis_report(stats: DatasetStats, n_positions: int = 5):\n",
    "  \"\"\"\n",
    "  Print a comprehensive analysis report.\n",
    "\n",
    "  Args:\n",
    "      stats: DatasetStats object containing analysis results\n",
    "      n_positions: Number of top positions to show in detail\n",
    "  \"\"\"\n",
    "  print(f\"Dataset Analysis Report\")\n",
    "  print(\"=\" * 50)\n",
    "  print(f\"\\nOverall Statistics:\")\n",
    "  print(f\"Sparsity: {stats.sparsity:.2f}% zeros\")\n",
    "\n",
    "  print(f\"\\nTop {n_positions} Most Variant Positions:\")\n",
    "  print(\"-\" * 50)\n",
    "  print(f\"{'Position':^10} {'Variance':^12} {'Activity%':^12} {'Mean':^12} {'Std':^12}\")\n",
    "  print(\"-\" * 50)\n",
    "\n",
    "  for pos in stats.top_variant_positions[:n_positions]:\n",
    "      print(f\"{pos:^10} {stats.position_variances[pos]:^12.4f} \"\n",
    "            f\"{stats.position_activity[pos]:^12.2f} \"\n",
    "            f\"{stats.mean_vector[pos]:^12.4f} \"\n",
    "            f\"{stats.std_vector[pos]:^12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 800 examples for 'Hallucinated'\n",
      "There are 800 examples for 'Truthful'\n",
      "Therefore...\n",
      "Total length of X: 1600\n",
      "Total length of y: 1600\n",
      "\n",
      "\n",
      "Some random examples\n",
      "Element 1248:\n",
      "   X: [75.5, 87.5, 83.5, 0, 0, 1.964111328125, 0, 0, 0.0, 0, 0, 0, 1.1171875, 0.3291015625, 0.302734375, 0.701171875, 0, 0, 0, 0.49609375, 0.0, 0, 1.1328125, 0, 0, 1.765625, 0.4912109375, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.453125, 0, 0, 0.57568359375, 0, 0, 0, 0.3828125, 0, 0.3359375, 0.0]\n",
      "   y: 0\n",
      "Element 1471:\n",
      "   X: [75.5, 87.5, 83.5, 0.40625, 0, 0.431640625, 0, 1.32421875, 0.0, 0, 0, 0, 0.96484375, 0, 0.2900390625, 0.46875, 0, 0, 0.5064174107142857, 0.28515625, 0, 0, 0.5455729166666666, 0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0.63671875, 0, 0, 0, 0, 0, 0.837890625, 0, 0, 0, 0.478515625, 0, 0, 0.62109375, 0, 0.3193359375, 0, 0, 0, 1.3118489583333333, 0.0]\n",
      "   y: 0\n",
      "Element 602:\n",
      "   X: [75.0, 87.0, 83.0, 0, 0, 0, 0, 1.7265625, 0.0, 0, 0, 0, 1.109375, 0, 0, 0.34765625, 0, 0, 0, 2.03125, 0, 0, 0, 0, 0, 0.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.349609375, 0, 0.3466796875, 0, 0, 0, 0, 0, 0, 0.5677083333333334, 0, 0, 0, 0, 0, 1.40625, 0.0]\n",
      "   y: 1\n"
     ]
    }
   ],
   "source": [
    "# Combine Data, predictors (x) and targets (y)\n",
    "import random\n",
    "\n",
    "print(f\"There are {len(X_hallucinate)} examples for 'Hallucinated'\")\n",
    "print(f\"There are {len(X_truthful)} examples for 'Truthful'\")\n",
    "\n",
    "X = X_hallucinate + X_truthful \n",
    "y = ([1] * len(X_hallucinate)) + ([0] * len(X_truthful))  \n",
    "\n",
    "X_test = X_hallucinate_test + X_truthful_test\n",
    "y_test = ([1] * len(X_hallucinate_test)) + ([0] * len(X_truthful_test))  \n",
    "\n",
    "assert len(X) == len(y)\n",
    "\n",
    "print(\"Therefore...\")\n",
    "print(\"Total length of X:\", len(X))\n",
    "print(\"Total length of y:\", len(y))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Let's view a random sample\n",
    "indices = random.sample(range(len(X)), 3)\n",
    "print(\"Some random examples\")\n",
    "for i in indices:\n",
    "  print(f\"Element {i}:\")\n",
    "  print(\"   X:\", X[i])\n",
    "  print(\"   y:\", y[i])\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('X.npy', np.array(X))\n",
    "np.save('X_test.npy', np.array(X_test))\n",
    "np.save('y.npy', np.array(y))\n",
    "np.save('y_test.npy', np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Analysis Report\n",
      "==================================================\n",
      "\n",
      "Overall Statistics:\n",
      "Sparsity: 74.70% zeros\n",
      "\n",
      "Top 20 Most Variant Positions:\n",
      "--------------------------------------------------\n",
      " Position    Variance    Activity%       Mean         Std     \n",
      "--------------------------------------------------\n",
      "    1        17.1876       100.00      88.1520       4.1458   \n",
      "    2        16.7700       100.00      84.1412       4.0951   \n",
      "    0        13.4042       100.00      76.0342       3.6612   \n",
      "    25        0.4686       11.38        0.2272       0.6845   \n",
      "    5         0.3682       27.06        0.3016       0.6068   \n",
      "    7         0.2894       30.00        0.3188       0.5380   \n",
      "    19        0.2287       42.94        0.3467       0.4782   \n",
      "    22        0.1996       51.31        0.3844       0.4468   \n",
      "    27        0.1673       12.62        0.1329       0.4090   \n",
      "    51        0.1642       21.31        0.1862       0.4052   \n",
      "    29        0.1488       21.31        0.1700       0.3858   \n",
      "    18        0.1482       68.31        0.4615       0.3849   \n",
      "    15        0.1196       67.75        0.4167       0.3458   \n",
      "    32        0.0950       50.75        0.2770       0.3082   \n",
      "    38        0.0915       52.00        0.2753       0.3026   \n",
      "    12        0.0761       94.69        0.9947       0.2759   \n",
      "    26        0.0650       41.44        0.1980       0.2550   \n",
      "    47        0.0540       26.50        0.1281       0.2324   \n",
      "    45        0.0539       98.94        0.8667       0.2322   \n",
      "    42        0.0293       21.31        0.0842       0.1710   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Analyse compressibility\n",
    "n_top_positions = 20\n",
    "stats = analyze_datasets(X, n_top_positions)\n",
    "print_analysis_report(stats, n_top_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [87.0, 83.0, 75.0, 0.0, 0, 0, 0.6178385416666666, 0.4599609375, 0, 0, 0, 0.4147135416666667, 0.50390625, 0.447265625, 0.2578125, 1.109375, 0.291015625, 0, 0.7265625, 0]\n",
      "Y:\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "# get important locations in data\n",
    "X_compressed = [[x[i] for i in stats.top_variant_positions[0:20]] for x in X]\n",
    "X_compressed_test = [[x[i] for i in stats.top_variant_positions[0:20]] for x in X_test]\n",
    "# view example\n",
    "print(\"X:\\n\", X_compressed[0])\n",
    "print(\"Y:\\n\", y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Decision Tree\n",
    "\n",
    "For speed we'll sub divide the training set into train and test, then I can use objects already created...\n",
    "\n",
    "Lazy, I know, but in a hurry here...and justtrying to explore the territory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def train_tree(X, y, depth):\n",
    "    \"\"\"\n",
    "    Train a decision tree classifier\n",
    "    \"\"\"\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "    model = tree.DecisionTreeClassifier(\n",
    "        max_depth=depth,\n",
    "        min_samples_leaf=len(train_x) // 20,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "\n",
    "    accuracy = balanced_accuracy_score(test_y, pred)\n",
    "    score = f1_score(test_y, pred, average='weighted')\n",
    "\n",
    "    return model, pred, score, accuracy, (train_x, test_x, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training decision tree...\n",
      "Balanced Accuracy: 0.575\n",
      "F1 Score: 0.575\n"
     ]
    }
   ],
   "source": [
    "# Train model on original data\n",
    "print(\"Training decision tree...\")\n",
    "tree_raw, pred, score, accuracy, splits = train_tree(X, y, depth=6)\n",
    "\n",
    "print(f\"Balanced Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training decision tree...\n",
      "Balanced Accuracy: 0.590\n",
      "F1 Score: 0.592\n"
     ]
    }
   ],
   "source": [
    "# Train model on compressed data\n",
    "print(\"Training decision tree...\")\n",
    "tree_compressed, pred, score, accuracy, splits = train_tree(X_compressed, y, depth=6)\n",
    "\n",
    "print(f\"Balanced Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1 Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           balanced_accuracy_score, f1_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def train_evaluate_svm(X, y):\n",
    "    \"\"\"\n",
    "    Train and evaluate SVM classifier with balanced accuracy and F1 scores\n",
    "    \n",
    "    Args:\n",
    "    X: List of lists where each inner list has 53 float values\n",
    "    y: List of integers (1 for Hallucinated, 0 for truthful)\n",
    "    \"\"\"\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Create pipeline with preprocessing and model\n",
    "    svm_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            random_state=42,\n",
    "            probability=True\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    svm_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions for both training and test sets\n",
    "    y_train_pred = svm_pipeline.predict(X_train)\n",
    "    y_test_pred = svm_pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics for training set\n",
    "    train_balanced_acc = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Calculate metrics for test set\n",
    "    test_balanced_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(svm_pipeline, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Print comprehensive performance metrics\n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(\"-----------------\")\n",
    "    print(f\"Number of training examples: {len(X_train)}\")\n",
    "    print(f\"Number of test examples: {len(X_test)}\")\n",
    "    \n",
    "    print(f\"\\nClass distribution in training:\")\n",
    "    print(f\"Truthful (0): {sum(y_train == 0)}\")\n",
    "    print(f\"Hallucinated (1): {sum(y_train == 1)}\")\n",
    "    \n",
    "    print(\"\\nTraining Set Metrics:\")\n",
    "    print(f\"Balanced Accuracy: {train_balanced_acc:.3f}\")\n",
    "    print(f\"F1 Score: {train_f1:.3f}\")\n",
    "    \n",
    "    print(\"\\nTest Set Metrics:\")\n",
    "    print(f\"Balanced Accuracy: {test_balanced_acc:.3f}\")\n",
    "    print(f\"F1 Score: {test_f1:.3f}\")\n",
    "    \n",
    "    print(\"\\nCross-validation scores:\", cv_scores)\n",
    "    print(\"Average CV score: {:.3f} (+/- {:.3f})\".format(\n",
    "        cv_scores.mean(), cv_scores.std() * 2\n",
    "    ))\n",
    "    \n",
    "    print(\"\\nClassification Report (Test Set):\")\n",
    "    print(classification_report(y_test, y_test_pred, \n",
    "                              target_names=['Truthful', 'Hallucinated']))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix (Test Set):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    \n",
    "    # Return model and metrics dictionary\n",
    "    metrics = {\n",
    "        'train_balanced_accuracy': train_balanced_acc,\n",
    "        'train_f1': train_f1,\n",
    "        'test_balanced_accuracy': test_balanced_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'cv_scores_mean': cv_scores.mean(),\n",
    "        'cv_scores_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    return svm_pipeline, metrics\n",
    "\n",
    "def predict_new_examples(model, X_new):\n",
    "    \"\"\"\n",
    "    Predict classes for new examples\n",
    "    \n",
    "    Args:\n",
    "    model: Trained pipeline\n",
    "    X_new: List of lists, each inner list having 53 float values\n",
    "    \n",
    "    Returns:\n",
    "    predictions: Array of predicted labels (0 or 1)\n",
    "    probabilities: Array of prediction probabilities for each class\n",
    "    \"\"\"\n",
    "    X_new = np.array(X_new)\n",
    "    predictions = model.predict(X_new)\n",
    "    probabilities = model.predict_proba(X_new)\n",
    "    \n",
    "    return predictions, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "-----------------\n",
      "Number of training examples: 1280\n",
      "Number of test examples: 320\n",
      "\n",
      "Class distribution in training:\n",
      "Truthful (0): 640\n",
      "Hallucinated (1): 640\n",
      "\n",
      "Training Set Metrics:\n",
      "Balanced Accuracy: 0.794\n",
      "F1 Score: 0.808\n",
      "\n",
      "Test Set Metrics:\n",
      "Balanced Accuracy: 0.566\n",
      "F1 Score: 0.585\n",
      "\n",
      "Cross-validation scores: [0.609375   0.62890625 0.62890625 0.59765625 0.58203125]\n",
      "Average CV score: 0.609 (+/- 0.036)\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Truthful       0.57      0.52      0.54       160\n",
      "Hallucinated       0.56      0.61      0.59       160\n",
      "\n",
      "    accuracy                           0.57       320\n",
      "   macro avg       0.57      0.57      0.56       320\n",
      "weighted avg       0.57      0.57      0.56       320\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[83 77]\n",
      " [62 98]]\n",
      "\n",
      "Summary Metrics Dictionary:\n",
      "train_balanced_accuracy: 0.794\n",
      "train_f1: 0.808\n",
      "test_balanced_accuracy: 0.566\n",
      "test_f1: 0.585\n",
      "cv_scores_mean: 0.609\n",
      "cv_scores_std: 0.018\n"
     ]
    }
   ],
   "source": [
    "svm_raw, metrics = train_evaluate_svm(X, y)\n",
    "\n",
    "# Access specific metrics\n",
    "print(\"\\nSummary Metrics Dictionary:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name}: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "-----------------\n",
      "Number of training examples: 1280\n",
      "Number of test examples: 320\n",
      "\n",
      "Class distribution in training:\n",
      "Truthful (0): 640\n",
      "Hallucinated (1): 640\n",
      "\n",
      "Training Set Metrics:\n",
      "Balanced Accuracy: 0.684\n",
      "F1 Score: 0.706\n",
      "\n",
      "Test Set Metrics:\n",
      "Balanced Accuracy: 0.584\n",
      "F1 Score: 0.596\n",
      "\n",
      "Cross-validation scores: [0.57421875 0.58984375 0.58203125 0.58984375 0.55859375]\n",
      "Average CV score: 0.579 (+/- 0.023)\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Truthful       0.59      0.56      0.57       160\n",
      "Hallucinated       0.58      0.61      0.60       160\n",
      "\n",
      "    accuracy                           0.58       320\n",
      "   macro avg       0.58      0.58      0.58       320\n",
      "weighted avg       0.58      0.58      0.58       320\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[89 71]\n",
      " [62 98]]\n",
      "\n",
      "Summary Metrics Dictionary:\n",
      "train_balanced_accuracy: 0.684\n",
      "train_f1: 0.706\n",
      "test_balanced_accuracy: 0.584\n",
      "test_f1: 0.596\n",
      "cv_scores_mean: 0.579\n",
      "cv_scores_std: 0.012\n"
     ]
    }
   ],
   "source": [
    "svm_raw_compressed, metrics = train_evaluate_svm(X_compressed, y)\n",
    "\n",
    "# Access specific metrics\n",
    "print(\"\\nSummary Metrics Dictionary:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name}: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We'll use the raw model, not compressed, easier to work with features when model loaded from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model_and_features(model, features, output_path: str):\n",
    "    \"\"\"Save both the sklearn decision tree model and Goodfire features to a file.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained sklearn decision tree model\n",
    "        features: The Goodfire features used by the model\n",
    "        output_path: Path where to save the pickle file\n",
    "    \"\"\"\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'features': features\n",
    "    }\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model and best_features\n",
    "\n",
    "model_path = \"hallucination_classifier_svm.pkl\"\n",
    "save_model_and_features(svm_raw, features_to_look_at, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import goodfire\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "class SVMHallucinationClassifier:\n",
    "    def __init__(self, model_path: str, api_key: str):\n",
    "        \"\"\"\n",
    "        Initialize the hallucination classifier with a saved SVM model and features.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the saved pickle file containing both the model and features\n",
    "            api_key: Goodfire API key for accessing the service\n",
    "        \"\"\"\n",
    "        # Load the model and features\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "            self.model = model_data['model']\n",
    "            self.features = model_data['features']\n",
    "        self.client = goodfire.Client(api_key)\n",
    "        self.variant = goodfire.Variant(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "    def _format_prompt(self, question: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Format a question into the expected prompt structure.\"\"\"\n",
    "        introduction = (\"You are a medical expert and this is a multiple choice exam question. \"\n",
    "                      \"Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\")\n",
    "        return [{\"role\": \"user\", \"content\": f\"{introduction}\\n\\n{question}\"}]\n",
    "\n",
    "    def _get_feature_activations(self, prompt: List[Dict[str, str]]) -> List[float]:\n",
    "        \"\"\"Get feature activations for the input prompt.\"\"\"\n",
    "        context = self.client.features.inspect(\n",
    "            prompt,\n",
    "            model=self.variant,\n",
    "            features=self.features\n",
    "        )\n",
    "        \n",
    "        # Get activations for our specific features\n",
    "        activations = []\n",
    "        features_dict = {f.uuid: 0.0 for f in self.features}\n",
    "        \n",
    "        for feature_act in context.top(k=len(self.features)):\n",
    "            if feature_act.feature.uuid in features_dict:\n",
    "                features_dict[feature_act.feature.uuid] = feature_act.activation\n",
    "        \n",
    "        # Maintain order matching the original features\n",
    "        for feature in self.features:\n",
    "            activations.append(features_dict[feature.uuid])\n",
    "            \n",
    "        return activations\n",
    "\n",
    "    def predict(self, question: str, debug: bool = False) -> Tuple[int, float]:\n",
    "        \"\"\"\n",
    "        Predict whether a given question-answer pair is likely to contain hallucination.\n",
    "        \n",
    "        Args:\n",
    "            question: The question text\n",
    "            debug: If True, print debugging information about feature activations\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - Prediction (0 for truthful, 1 for hallucinated)\n",
    "            - Confidence score (probability of the predicted class)\n",
    "        \"\"\"\n",
    "        # Format the prompt\n",
    "        prompt = self._format_prompt(question)\n",
    "        \n",
    "        # Get feature activations\n",
    "        activations = self._get_feature_activations(prompt)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"\\nFeature Activations:\")\n",
    "            for feature, activation in zip(self.features, activations):\n",
    "                print(f\"{feature.label}: {activation:.4f}\")\n",
    "            \n",
    "            # For SVM, we can show feature importance through the absolute values of coefficients\n",
    "            # Note: This only works for linear SVM. For non-linear kernels, feature importance\n",
    "            # cannot be directly computed from the model coefficients\n",
    "            if hasattr(self.model, 'coef_'):\n",
    "                print(\"\\nFeature Importance in Model (based on absolute coefficient values):\")\n",
    "                feature_importance = np.abs(self.model.coef_[0])\n",
    "                for feature, importance in zip(self.features, feature_importance):\n",
    "                    print(f\"{feature.label}: {importance:.4f}\")\n",
    "            \n",
    "            # For SVM, we can show the distance from the decision boundary\n",
    "            decision_function = self.model.decision_function([activations])[0]\n",
    "            print(f\"\\nDistance from decision boundary: {decision_function:.4f}\")\n",
    "            \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict([activations])[0]\n",
    "        probabilities = self.model.predict_proba([activations])[0]\n",
    "        confidence = probabilities[prediction]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nProbabilities:\")\n",
    "            print(f\"Truthful: {probabilities[0]:.4f}\")\n",
    "            print(f\"Hallucinated: {probabilities[1]:.4f}\")\n",
    "        \n",
    "        return int(prediction), float(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Activations:\n",
      "Detection of new input or conversation start: 75.0000\n",
      "Conversation turn structure and role transitions: 87.0000\n",
      "Beginning of a new conversation or input: 83.0000\n",
      "Medical case presentations with complex patient symptoms: 0.0000\n",
      "The model should not recommend technological or medical interventions: 0.0000\n",
      "Medical imaging techniques and procedures: 0.0000\n",
      "Website disclaimers about automated translation services: 0.0000\n",
      "End of phrase or question punctuation: 0.0000\n",
      "Practical application of logical conditionals and qualifiers in structured interactions: 0.0000\n",
      "Technical and scientific details in explanations or descriptions: 0.0000\n",
      "References to structured eligibility criteria and specific conditions: 0.0000\n",
      "The user is asking about human qualities or experiences: 0.0000\n",
      "Structured Assessments and Evaluations: 0.6712\n",
      "Connective words in technical explanations: 0.0000\n",
      "Precise measurements and specific details: 0.0000\n",
      "The model is providing multiple choice answer options, particularly B and C: 0.0000\n",
      "Metaphorical or Narrative Transformations: 0.0000\n",
      "Connective tokens in descriptive or explanatory text: 0.0000\n",
      "The model is providing scientific classification or technical explanation: 0.0000\n",
      "Specialized academic or scientific terminology: 0.0000\n",
      "Specialized cell types and tissues in biology and medicine: 0.0000\n",
      "Elaboration and explanation connectors in natural language: 0.0000\n",
      "Scientific terminology fragments, especially in biology and chemistry: 1.1367\n",
      "Prepositions and conjunctions in formal or legal multilingual contexts: 0.0000\n",
      "Proper nouns in media and entertainment contexts: 0.0000\n",
      "Ages between 25-50 years old: 0.0000\n",
      "Punctuation in multiple choice questions: 0.0000\n",
      "Biological cells and cellular structures: 0.0000\n",
      "Connective and Punctuative Structuring in Explanatory and Technical Writing: 0.0000\n",
      "Networking protocols and standards (e.g. 802.11, IPv6): 0.0000\n",
      "HLS array partitioning directives: 0.0000\n",
      "Complex sentence structure and argumentative language: 0.0000\n",
      "Analytical 'of' in explanatory contexts: 0.7305\n",
      "Small organizational entities or units: 0.0000\n",
      "Spatial relationships and positioning in descriptive text: 0.0000\n",
      "Advanced materials and technical processes in science and engineering: 0.0000\n",
      "Recognition of Structured Data with Specific Entities: 0.0000\n",
      "Full-year earnings per share forecasts in financial reports: 0.2559\n",
      "Unclear or inconsistent activation pattern: 0.0000\n",
      "Start of a new user query or conversation: 0.0000\n",
      "Oxford University degree or prestigious academic credentials: 0.0000\n",
      "CSS media queries for responsive design breakpoints: 0.0000\n",
      "Well-known institutions, places, and sports teams: 0.0000\n",
      "Chess opening moves, especially knight and pawn movements: 0.0000\n",
      "Prepositions and relational words: 0.0000\n",
      "Detects newlines between items in structured text like quizzes or lists: 0.5430\n",
      "Real estate taxation and property ownership: 0.0000\n",
      "Initial letters of proper nouns and named entities: 0.0000\n",
      "Complex chemical compound names in IUPAC nomenclature: 0.0000\n",
      "End of explanatory sentences or clauses: 0.0000\n",
      "Code syntax elements, particularly variable assignments and function calls: 0.0000\n",
      "End of sentence or statement in structured text: 0.0000\n",
      "Character-level programming syntax patterns: 0.0000\n",
      "\n",
      "Distance from decision boundary: 0.8128\n",
      "\n",
      "Probabilities:\n",
      "Truthful: 0.3828\n",
      "Hallucinated: 0.6172\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../classifier/hallucination_classifier_svm.pkl\"\n",
    "\n",
    "prompt_example = \"this is my prompt\"\n",
    "\n",
    "classifier = SVMHallucinationClassifier(\n",
    "    model_path=model_path,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# get prediction\n",
    "# prediction = 1 indicates hallucinated\n",
    "# prediction = 0 indicates truthful\n",
    "prediction, confidence = classifier.predict(prompt_example, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model_predictions(classifier, truthful_examples, hallucinated_examples):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions across all examples.\n",
    "    \n",
    "    Args:\n",
    "        classifier: The HallucinationClassifier instance\n",
    "        truthful_examples: DataFrame containing truthful examples\n",
    "        hallucinated_examples: DataFrame containing hallucinated examples\n",
    "    \"\"\"\n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Process truthful examples\n",
    "    print(\"\\nProcessing truthful examples...\")\n",
    "    for idx, row in tqdm(truthful_examples.iterrows(), total=len(truthful_examples)):\n",
    "        prediction, confidence = classifier.predict(row['prompt'])\n",
    "        results.append({\n",
    "            'true_label': 'truthful',\n",
    "            'predicted': 'hallucinated' if prediction == 1 else 'truthful',\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    # Process hallucinated examples\n",
    "    print(\"\\nProcessing hallucinated examples...\")\n",
    "    for idx, row in tqdm(hallucinated_examples.iterrows(), total=len(hallucinated_examples)):\n",
    "        prediction, confidence = classifier.predict(row['prompt'])\n",
    "        results.append({\n",
    "            'true_label': 'hallucinated',\n",
    "            'predicted': 'hallucinated' if prediction == 1 else 'truthful',\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    total_predictions = len(results_df)\n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(f\"Total examples evaluated: {total_predictions}\")\n",
    "    \n",
    "    # Prediction distribution\n",
    "    pred_dist = results_df['predicted'].value_counts()\n",
    "    print(\"\\nPrediction Distribution:\")\n",
    "    for pred, count in pred_dist.items():\n",
    "        percentage = (count/total_predictions) * 100\n",
    "        print(f\"{pred}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    confusion = pd.crosstab(results_df['true_label'], results_df['predicted'])\n",
    "    print(confusion)\n",
    "    \n",
    "    # Calculate metrics by class\n",
    "    print(\"\\nMetrics by True Label:\")\n",
    "    for label in ['truthful', 'hallucinated']:\n",
    "        class_results = results_df[results_df['true_label'] == label]\n",
    "        correct = (class_results['true_label'] == class_results['predicted']).sum()\n",
    "        total = len(class_results)\n",
    "        accuracy = (correct/total) * 100\n",
    "        avg_confidence = class_results['confidence'].mean()\n",
    "        \n",
    "        print(f\"\\n{label.title()} Examples:\")\n",
    "        print(f\"Accuracy: {accuracy:.1f}%\")\n",
    "        print(f\"Average Confidence: {avg_confidence:.3f}\")\n",
    "    \n",
    "    # Return the results DataFrame for further analysis if needed\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing truthful examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:29<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing hallucinated examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:28<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Statistics:\n",
      "Total examples evaluated: 60\n",
      "\n",
      "Prediction Distribution:\n",
      "hallucinated: 34 (56.7%)\n",
      "truthful: 26 (43.3%)\n",
      "\n",
      "Confusion Matrix:\n",
      "predicted     hallucinated  truthful\n",
      "true_label                          \n",
      "hallucinated            20        10\n",
      "truthful                14        16\n",
      "\n",
      "Metrics by True Label:\n",
      "\n",
      "Truthful Examples:\n",
      "Accuracy: 53.3%\n",
      "Average Confidence: 0.585\n",
      "\n",
      "Hallucinated Examples:\n",
      "Accuracy: 66.7%\n",
      "Average Confidence: 0.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## TRAINING DATASET EVALUATION\n",
    "\n",
    "# Assuming classifier is already initialized\n",
    "results_df = evaluate_model_predictions(classifier, truthful_examples_test[0:30], hallucinated_examples_test[0:30])\n",
    "\n",
    "# You can do additional analysis on results_df if needed\n",
    "# For example, look at high confidence mistakes:\n",
    "high_conf_mistakes = results_df[\n",
    "    (results_df['true_label'] != results_df['predicted']) & \n",
    "    (results_df['confidence'] > 0.8)\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
