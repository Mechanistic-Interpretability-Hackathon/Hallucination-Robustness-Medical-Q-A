{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d48d84f06afa43d8a7b5f8b23da3ddfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78c73d3dbfa64bbd85aafbc2991e397f",
              "IPY_MODEL_5105c50d58e0487c8ce4575185c86113",
              "IPY_MODEL_da01c509317f45eba14698ceee5ae2b5"
            ],
            "layout": "IPY_MODEL_0a1fb6d22e664f35a07f5481c2927b89"
          }
        },
        "78c73d3dbfa64bbd85aafbc2991e397f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8265da7800d464388782de5fefe5973",
            "placeholder": "​",
            "style": "IPY_MODEL_9ccefcd06434492785c84f7527207581",
            "value": "README.md: 100%"
          }
        },
        "5105c50d58e0487c8ce4575185c86113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66db1bf4d435415b916daed735080a7c",
            "max": 4797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9529e82e8a234057a56398304a0e82a5",
            "value": 4797
          }
        },
        "da01c509317f45eba14698ceee5ae2b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a3d971b04664ea884a483b01ea59e6e",
            "placeholder": "​",
            "style": "IPY_MODEL_db19dee7e9b04e5e9a5a534bfcc0402a",
            "value": " 4.80k/4.80k [00:00&lt;00:00, 316kB/s]"
          }
        },
        "0a1fb6d22e664f35a07f5481c2927b89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8265da7800d464388782de5fefe5973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ccefcd06434492785c84f7527207581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66db1bf4d435415b916daed735080a7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9529e82e8a234057a56398304a0e82a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a3d971b04664ea884a483b01ea59e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db19dee7e9b04e5e9a5a534bfcc0402a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b1ef4caf4194e0ba96ece3968c9d8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_316399f633de4f6abfa38491316036f7",
              "IPY_MODEL_4bb8f11193ee43e9b79d76328e1b1b26",
              "IPY_MODEL_e1a8f22ec95449f98b9efcf361f4d63b"
            ],
            "layout": "IPY_MODEL_fb6414ccf933471c950d3219c20ce3e6"
          }
        },
        "316399f633de4f6abfa38491316036f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61a442fd0afe46449d4d25946754cad4",
            "placeholder": "​",
            "style": "IPY_MODEL_01c899ac93394352892b474d45d74324",
            "value": "reasoning_FCT/reasoning_FCT.csv: 100%"
          }
        },
        "4bb8f11193ee43e9b79d76328e1b1b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0e7a2220a06466da2f12837ef2dbc35",
            "max": 9925083,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7f8fc3b07454a50acb72b64e78aa42e",
            "value": 9925083
          }
        },
        "e1a8f22ec95449f98b9efcf361f4d63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b451893fa83848c1b0b3ab0c68ac2a92",
            "placeholder": "​",
            "style": "IPY_MODEL_38d12182345e401fa71baa17f66652e7",
            "value": " 9.93M/9.93M [00:01&lt;00:00, 7.25MB/s]"
          }
        },
        "fb6414ccf933471c950d3219c20ce3e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61a442fd0afe46449d4d25946754cad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c899ac93394352892b474d45d74324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0e7a2220a06466da2f12837ef2dbc35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7f8fc3b07454a50acb72b64e78aa42e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b451893fa83848c1b0b3ab0c68ac2a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d12182345e401fa71baa17f66652e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff904108a8074360b0569469bbe8962a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d52230552754a00b8574e4e168dbb07",
              "IPY_MODEL_6cf092475b2942c0bd788640e3018574",
              "IPY_MODEL_8a0dddca789b4dabad8f7e69f303e2a1"
            ],
            "layout": "IPY_MODEL_7d3747dcc95c432da8c93b662d5a975e"
          }
        },
        "7d52230552754a00b8574e4e168dbb07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fc7751a602e4f5091beb5eb310d880d",
            "placeholder": "​",
            "style": "IPY_MODEL_8c167dd59a1f438ab360b46e20e5da9a",
            "value": "Generating train split: 100%"
          }
        },
        "6cf092475b2942c0bd788640e3018574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_325a6a047a824931a8d7ee8ffef29d6a",
            "max": 18866,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_546d0d0341f54402b4f0a1244680c485",
            "value": 18866
          }
        },
        "8a0dddca789b4dabad8f7e69f303e2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4df1732356f443cbb8b2a4f5a8e3625",
            "placeholder": "​",
            "style": "IPY_MODEL_3319d82ba91b4beea877191eb456cb12",
            "value": " 18866/18866 [00:00&lt;00:00, 61679.39 examples/s]"
          }
        },
        "7d3747dcc95c432da8c93b662d5a975e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fc7751a602e4f5091beb5eb310d880d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c167dd59a1f438ab360b46e20e5da9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "325a6a047a824931a8d7ee8ffef29d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "546d0d0341f54402b4f0a1244680c485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4df1732356f443cbb8b2a4f5a8e3625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3319d82ba91b4beea877191eb456cb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Get Huggingface Data"
      ],
      "metadata": {
        "id": "fi6rGyFE9CWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install Huggingface datasets\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pm859At6cVN",
        "outputId": "39b53857-521b-4eea-f07e-f505f71da124"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Acquire the MedHALT Data, using the reasoning_FCT subset\n",
        "# This subset is for multiple choice questions\n",
        "# Source:\n",
        "# https://huggingface.co/datasets/openlifescienceai/Med-HALT/viewer/reasoning_FCT\n",
        "# There are 18.9k records to use, far more than we need\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "reasoning_FCT = load_dataset(\"openlifescienceai/Med-HALT\", \"reasoning_FCT\")\n",
        "train_data = reasoning_FCT['train']\n",
        "\n",
        "# How many records?\n",
        "print(\"No. of Records in Train:\", len(train_data))\n",
        "print(\"Example:\\n\")\n",
        "# see example\n",
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408,
          "referenced_widgets": [
            "d48d84f06afa43d8a7b5f8b23da3ddfe",
            "78c73d3dbfa64bbd85aafbc2991e397f",
            "5105c50d58e0487c8ce4575185c86113",
            "da01c509317f45eba14698ceee5ae2b5",
            "0a1fb6d22e664f35a07f5481c2927b89",
            "c8265da7800d464388782de5fefe5973",
            "9ccefcd06434492785c84f7527207581",
            "66db1bf4d435415b916daed735080a7c",
            "9529e82e8a234057a56398304a0e82a5",
            "8a3d971b04664ea884a483b01ea59e6e",
            "db19dee7e9b04e5e9a5a534bfcc0402a",
            "5b1ef4caf4194e0ba96ece3968c9d8a2",
            "316399f633de4f6abfa38491316036f7",
            "4bb8f11193ee43e9b79d76328e1b1b26",
            "e1a8f22ec95449f98b9efcf361f4d63b",
            "fb6414ccf933471c950d3219c20ce3e6",
            "61a442fd0afe46449d4d25946754cad4",
            "01c899ac93394352892b474d45d74324",
            "e0e7a2220a06466da2f12837ef2dbc35",
            "a7f8fc3b07454a50acb72b64e78aa42e",
            "b451893fa83848c1b0b3ab0c68ac2a92",
            "38d12182345e401fa71baa17f66652e7",
            "ff904108a8074360b0569469bbe8962a",
            "7d52230552754a00b8574e4e168dbb07",
            "6cf092475b2942c0bd788640e3018574",
            "8a0dddca789b4dabad8f7e69f303e2a1",
            "7d3747dcc95c432da8c93b662d5a975e",
            "7fc7751a602e4f5091beb5eb310d880d",
            "8c167dd59a1f438ab360b46e20e5da9a",
            "325a6a047a824931a8d7ee8ffef29d6a",
            "546d0d0341f54402b4f0a1244680c485",
            "a4df1732356f443cbb8b2a4f5a8e3625",
            "3319d82ba91b4beea877191eb456cb12"
          ]
        },
        "id": "ueP7hefy6fwq",
        "outputId": "49a6d4f0-a22d-4431-a7a2-a9dac4ef73c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/4.80k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d48d84f06afa43d8a7b5f8b23da3ddfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "reasoning_FCT/reasoning_FCT.csv:   0%|          | 0.00/9.93M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b1ef4caf4194e0ba96ece3968c9d8a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/18866 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff904108a8074360b0569469bbe8962a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of Records in Train: 18866\n",
            "Example:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'bc8659f4-3062-4f57-9e24-e32ad92a8d4e',\n",
              " 'dataset': 'headqa_en',\n",
              " 'question': 'Which of the following structural elements is characteristic of the ortopramide group drugs?',\n",
              " 'options': \"{'0': 'They are anilides with propyl group in ortho.', '1': 'They are benzamides with methoxy group in ortho.', '2': 'They are benzenesulfonamides with a methyl group in ortho.', '3': 'They are ortho-halogenated derivatives of phenothiazine.', 'correct answer': 'They are ortho-halogenated derivatives of phenothiazine.'}\",\n",
              " 'correct_answer': 'They are benzamides with methoxy group in ortho.',\n",
              " 'correct_index': 1,\n",
              " 'split_type': 'val',\n",
              " 'subject_name': 'pharmacology',\n",
              " 'topic_name': None,\n",
              " 'year': 2015.0,\n",
              " 'exam_name': 'Cuaderno_2015_1_F',\n",
              " 'student_answer': 'They are ortho-halogenated derivatives of phenothiazine.',\n",
              " 'student_index': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many subjects are there, and how many records per subject\n",
        "\n",
        "# Using pandas\n",
        "def count_subjects_pandas(records):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Convert list of records to DataFrame\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    # Group by subject_name and count\n",
        "    subject_counts = df.groupby('subject_name').size().reset_index(name='count')\n",
        "\n",
        "    # Sort by count in descending order\n",
        "    subject_counts = subject_counts.sort_values('count', ascending=False)\n",
        "\n",
        "    return subject_counts\n",
        "\n",
        "results_pandas = count_subjects_pandas(train_data)\n",
        "print(results_pandas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNhObXWGt2lK",
        "outputId": "b701ad27-a050-46bb-8859-20ca64ed21e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    subject_name  count\n",
            "3                         Dental   2401\n",
            "19                       Surgery    803\n",
            "23                      medicine    684\n",
            "24                       nursery    681\n",
            "25                  pharmacology    679\n",
            "6       Gynaecology & Obstetrics    679\n",
            "26                    psychology    678\n",
            "22                     chemistry    674\n",
            "21                       biology    672\n",
            "20                       Unknown    646\n",
            "11                     Pathology    601\n",
            "7                       Medicine    577\n",
            "14                    Physiology    528\n",
            "13                  Pharmacology    521\n",
            "2                   Biochemistry    477\n",
            "1                        Anatomy    427\n",
            "12                    Pediatrics    358\n",
            "18  Social & Preventive Medicine    353\n",
            "8                   Microbiology    268\n",
            "9                  Ophthalmology    209\n",
            "5              Forensic Medicine    182\n",
            "16                     Radiology    174\n",
            "4                            ENT    117\n",
            "0                    Anaesthesia     87\n",
            "17                          Skin     69\n",
            "15                    Psychiatry     20\n",
            "10                  Orthopaedics     18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above data we have the question, the options (which will be appended to the prompt) and we have the target, correct_index.\n",
        "\n",
        "To build the prompt and get the matching target, we will need a function, as follows...."
      ],
      "metadata": {
        "id": "msqNY0n6q1QJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data\n",
        "\n",
        "Aappending contexts to question"
      ],
      "metadata": {
        "id": "pHrgDhHx9iRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import ast\n",
        "\n",
        "def create_prompt(example):\n",
        "    \"\"\"\n",
        "    Creates a formatted prompt from a single example and extracts the correct index.\n",
        "\n",
        "    Args:\n",
        "        example (dict): Single example from the dataset\n",
        "\n",
        "    Returns:\n",
        "        tuple: (formatted_prompt, correct_index)\n",
        "    \"\"\"\n",
        "    # Define introduction\n",
        "    introduction = \"You are a medical expert and this is a multiple choice exam question. Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\"\n",
        "\n",
        "    # Get question\n",
        "    question = example['question']\n",
        "\n",
        "    # Parse options string to dict (assuming it's stored as string)\n",
        "    try:\n",
        "        # Handle case where options might be already a dict\n",
        "        if isinstance(example['options'], str):\n",
        "            options_dict = ast.literal_eval(example['options'])\n",
        "        else:\n",
        "            options_dict = example['options']\n",
        "\n",
        "        # Filter out 'correct answer' key\n",
        "        options_filtered = {k: v for k, v in options_dict.items() if k != 'correct answer'}\n",
        "\n",
        "        # Format options\n",
        "        options_formatted = \"Options: \" + json.dumps(options_filtered)\n",
        "\n",
        "    except (ValueError, SyntaxError) as e:\n",
        "        print(f\"Error parsing options: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Combine all parts\n",
        "    prompt = f\"{introduction}\\n\\n{question}\\n\\n{options_formatted}\"\n",
        "\n",
        "    # Get correct index\n",
        "    correct_index = example['correct_index']\n",
        "\n",
        "    return prompt, correct_index\n",
        "\n",
        "def process_dataset(data):\n",
        "    \"\"\"\n",
        "    Process entire dataset to create prompts and extract labels.\n",
        "\n",
        "    Args:\n",
        "        data (list): List of examples\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X, y)\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for example in data:\n",
        "        prompt, label = create_prompt(example)\n",
        "        if prompt is not None:  # Only add if parsing was successful\n",
        "            X.append(prompt)\n",
        "            y.append(label)\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "V3ELiyjaJs2O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "\n",
        "# Assuming your data is loaded into a list called 'dataset':\n",
        "X, y = process_dataset(train_data)\n",
        "\n",
        "# Print an example:\n",
        "print(\"Example prompt:\")\n",
        "print(X[0])\n",
        "print(\"\\nCorrect index:\", y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL7HsOvKJkAg",
        "outputId": "3eb07563-db04-405a-ecd8-43110f037c77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example prompt:\n",
            "You are a medical expert and this is a multiple choice exam question. Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\n",
            "\n",
            "Which of the following structural elements is characteristic of the ortopramide group drugs?\n",
            "\n",
            "Options: {\"0\": \"They are anilides with propyl group in ortho.\", \"1\": \"They are benzamides with methoxy group in ortho.\", \"2\": \"They are benzenesulfonamides with a methyl group in ortho.\", \"3\": \"They are ortho-halogenated derivatives of phenothiazine.\"}\n",
            "\n",
            "Correct index: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Goodfire\n"
      ],
      "metadata": {
        "id": "N6_Strt49aNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install goodfire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-MgBE-i39ZeH",
        "outputId": "e626d135-a3bb-4391-f603-904079a46818"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting goodfire\n",
            "  Downloading goodfire-0.2.39-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from goodfire) (0.27.2)\n",
            "Collecting ipywidgets<9.0.0,>=8.1.5 (from goodfire)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.10/dist-packages (from goodfire) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from goodfire) (2.9.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.2->goodfire) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.2->goodfire) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.2->goodfire) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.2->goodfire) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.2->goodfire) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.2->goodfire) (0.14.0)\n",
            "Collecting comm>=0.1.3 (from ipywidgets<9.0.0,>=8.1.5->goodfire)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets<9.0.0,>=8.1.5->goodfire)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9.0.0,>=8.1.5->goodfire) (3.0.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.2->goodfire) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.2->goodfire) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.2->goodfire) (4.12.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (4.9.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.2->goodfire) (1.2.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.5->goodfire) (0.2.13)\n",
            "Downloading goodfire-0.2.39-py3-none-any.whl (28 kB)\n",
            "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, jedi, comm, ipywidgets, goodfire\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed comm-0.2.2 goodfire-0.2.39 ipywidgets-8.1.5 jedi-0.19.2 widgetsnbextension-4.0.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ipywidgets"
                ]
              },
              "id": "a94032c61e344332831d660d9d05e7ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract features using Goodfire API\n",
        "\n",
        "submit pubmedqa prompts, get top_k features.\n",
        "\n",
        "Using top_k = 50, as per example at: https://docs.goodfire.ai/examples/decision_trees.html"
      ],
      "metadata": {
        "id": "SGOLRIum_YQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import goodfire\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Get API key\n",
        "api_key = userdata.get('GOODFIRE_API_KEY')\n",
        "\n",
        "# Initialize Goodfire\n",
        "client  = goodfire.Client(api_key)\n",
        "variant = goodfire.Variant(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
        "\n"
      ],
      "metadata": {
        "id": "hpd_RxEU_W8Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_first_valid_integer(text):\n",
        "    \"\"\"\n",
        "    Extract the first integer from text and validate if it's in [0,1,2,3].\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to search for integers\n",
        "\n",
        "    Returns:\n",
        "        int or None: First valid integer if it's in [0,1,2,3], else None\n",
        "    \"\"\"\n",
        "    # Pattern matches the first integer in the text\n",
        "    pattern = r'\\d+'\n",
        "    match = re.search(pattern, text)\n",
        "\n",
        "    if match:\n",
        "        number = int(match.group())\n",
        "        # Only return the number if it's in our valid set\n",
        "        if number in [0, 1, 2, 3]:\n",
        "            return number\n",
        "    return -1\n",
        "\n",
        "def get_integer_response(api_key, prompt):\n",
        "    \"\"\"\n",
        "    Submit prompt to Goodfire API and extract first valid integer from response.\n",
        "\n",
        "    Args:\n",
        "        api_key (str): Goodfire API key\n",
        "        prompt (str): Prompt to send to the model\n",
        "\n",
        "    Returns:\n",
        "        int or None: First valid integer found in response\n",
        "    \"\"\"\n",
        "    # Initialize Goodfire client\n",
        "    client = goodfire.Client(api_key)\n",
        "\n",
        "    # Initialize model variant (using Llama 3 as shown in docs)\n",
        "    variant = goodfire.Variant(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
        "\n",
        "    # Create chat completion with streaming disabled for simpler processing\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=variant,\n",
        "        stream=False,\n",
        "        max_completion_tokens=50  # Keep response short since we just need an integer\n",
        "    )\n",
        "\n",
        "    # Extract content from response based on the actual response structure\n",
        "    try:\n",
        "        content = response.choices[0].message['content']\n",
        "        return extract_first_valid_integer(content)\n",
        "    except (AttributeError, IndexError) as e:\n",
        "        print(f\"Error extracting content from response: {e}\")\n",
        "        print(f\"Full response: {response}\")\n",
        "        return -1"
      ],
      "metadata": {
        "id": "yGydlj6tNX59"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test the code...\n",
        "result = get_integer_response(api_key, X[0])\n",
        "print(f\"First valid integer found: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsn5vHFegjUH",
        "outputId": "95b4cbdb-c14f-400a-ea3a-51f2d5c4b145"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First valid integer found: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import asyncio\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, cohen_kappa_score\n",
        "from typing import List, Tuple, Optional\n",
        "import pandas as pd\n",
        "\n",
        "def sample_data(X, y, k, random_seed = None):\n",
        "    \"\"\"\n",
        "    Randomly sample k items from X and y, maintaining paired relationships.\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "\n",
        "    n = len(X)\n",
        "    indices = random.sample(range(n), k)\n",
        "    X_sampled = [X[i] for i in indices]\n",
        "    y_sampled = [y[i] for i in indices]\n",
        "\n",
        "    return X_sampled, y_sampled, indices\n",
        "\n",
        "def evaluate_model_concurrent(api_key, X_sample, y_sample, max_workers = 10):\n",
        "    \"\"\"\n",
        "    Non-async version of model evaluation using ThreadPoolExecutor.\n",
        "    \"\"\"\n",
        "    # Create ThreadPoolExecutor for concurrent API calls\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Create tasks for each prompt\n",
        "        futures = [\n",
        "            executor.submit(get_integer_response, api_key, prompt)\n",
        "            for prompt in X_sample\n",
        "        ]\n",
        "\n",
        "        # Gather predictions\n",
        "        y_pred = []\n",
        "        for future in futures:\n",
        "            try:\n",
        "                result = future.result()\n",
        "                y_pred.append(result if result is not None else -1)  # Use -1 for failed predictions\n",
        "            except Exception as e:\n",
        "                print(f\"Error in API call: {e}\")\n",
        "                y_pred.append(-1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    # Filter out failed predictions (where y_pred is -1)\n",
        "    valid_indices = [i for i, pred in enumerate(y_pred) if pred != -1]\n",
        "    y_pred_valid = [y_pred[i] for i in valid_indices]\n",
        "    y_sample_valid = [y_sample[i] for i in valid_indices]\n",
        "\n",
        "    # Calculate metrics only on valid predictions\n",
        "    if len(valid_indices) > 0:\n",
        "        accuracy = accuracy_score(y_sample_valid, y_pred_valid)\n",
        "        kappa = cohen_kappa_score(y_sample_valid, y_pred_valid)\n",
        "\n",
        "        # Create confusion matrix\n",
        "        conf_matrix = confusion_matrix(y_sample_valid, y_pred_valid, labels=[0,1,2,3])\n",
        "\n",
        "        # Calculate per-class metrics\n",
        "        per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "    else:\n",
        "        accuracy = 0\n",
        "        kappa = 0\n",
        "        per_class_accuracy = [0, 0, 0, 0]\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'prompt': X_sample,\n",
        "        'true_answer': y_sample,\n",
        "        'predicted_answer': y_pred,\n",
        "        'correct': [pred == true for pred, true in zip(y_pred, y_sample)]\n",
        "    })\n",
        "\n",
        "    # Print evaluation summary\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"Total samples: {len(X_sample)}\")\n",
        "    print(f\"Valid predictions: {len(valid_indices)}\")\n",
        "    print(f\"Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"Cohen's Kappa: {kappa:.3f}\")\n",
        "    print(\"\\nPer-class accuracy:\")\n",
        "    for i, acc in enumerate(per_class_accuracy):\n",
        "        print(f\"Class {i}: {acc:.3f}\")\n",
        "\n",
        "    return accuracy, kappa, results_df\n",
        "\n",
        "def run_evaluation(api_key, X, y, k, random_seed= None, max_workers = 10):\n",
        "    \"\"\"\n",
        "    Wrapper function to handle both notebook and app environments.\n",
        "    \"\"\"\n",
        "    # Sample data\n",
        "    X_sample, y_sample, indices = sample_data(X, y, k, random_seed)\n",
        "\n",
        "    # Run evaluation\n",
        "    return evaluate_model_concurrent(api_key, X_sample, y_sample, max_workers)\n"
      ],
      "metadata": {
        "id": "IFOov7QBThj7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Is LLM performance better than random?\n",
        "\n",
        "There are only 4 choices. Even random guessing will achieve some sort of score..."
      ],
      "metadata": {
        "id": "m0VjyQcZXpz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "\n",
        "def find_min_successes(n: int, p: float, alpha: float = 0.05) -> int:\n",
        "    \"\"\"\n",
        "    Find minimum number of successes needed for statistical significance.\n",
        "    Uses binary search to find critical value.\n",
        "    \"\"\"\n",
        "    left, right = int(n * p), n  # Start search from expected value\n",
        "\n",
        "    while left <= right:\n",
        "        mid = (left + right) // 2\n",
        "        p_value = scipy.stats.binomtest(mid, n, p, alternative='greater').pvalue\n",
        "\n",
        "        if p_value <= alpha:\n",
        "            # Try to find a smaller value that still works\n",
        "            if mid == left or scipy.stats.binomtest(mid - 1, n, p, alternative='greater').pvalue > alpha:\n",
        "                return mid\n",
        "            right = mid - 1\n",
        "        else:\n",
        "            left = mid + 1\n",
        "\n",
        "    return left\n",
        "\n",
        "def calculate_random_baseline(y_true: list) -> float:\n",
        "    \"\"\"\n",
        "    Calculate random baseline accuracy based on class distribution.\n",
        "    For perfectly balanced classes, this will return 0.25.\n",
        "    For imbalanced classes, returns sum of squared proportions.\n",
        "\n",
        "    Args:\n",
        "        y_true: List of true labels\n",
        "\n",
        "    Returns:\n",
        "        float: Expected random accuracy based on class distribution\n",
        "    \"\"\"\n",
        "    class_counts = Counter(y_true)\n",
        "    total = len(y_true)\n",
        "\n",
        "    # Calculate proportions for each class\n",
        "    proportions = {k: v/total for k, v in class_counts.items()}\n",
        "\n",
        "    # For random guessing with class imbalance,\n",
        "    # probability of correct guess is sum of squared proportions\n",
        "    random_baseline = sum(p*p for p in proportions.values())\n",
        "\n",
        "    return random_baseline, proportions\n",
        "\n",
        "\n",
        "def assess_performance(y_true: list, y_pred: list, alpha: float = 0.05) -> Dict:\n",
        "    \"\"\"\n",
        "    Assess if model performance is significantly better than random chance.\n",
        "    Uses binomial test and provides effect size metrics.\n",
        "\n",
        "    Args:\n",
        "        y_true: List of true labels\n",
        "        y_pred: List of predicted labels (use only valid predictions, no -1s)\n",
        "        alpha: Significance level for statistical test (default 0.05)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing test results and metrics\n",
        "    \"\"\"\n",
        "    # Calculate basic metrics\n",
        "    n_samples = len(y_true)\n",
        "    n_correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
        "    observed_accuracy = n_correct / n_samples\n",
        "\n",
        "    # Calculate random baseline based on class distribution\n",
        "    random_prob, class_proportions = calculate_random_baseline(y_true)\n",
        "\n",
        "    # Perform one-sided binomial test\n",
        "    p_value = scipy.stats.binomtest(n_correct, n_samples, p=random_prob, alternative='greater').pvalue\n",
        "\n",
        "    # Calculate effect size (Cohen's h)\n",
        "    h = 2 * (np.arcsin(np.sqrt(observed_accuracy)) - np.arcsin(np.sqrt(random_prob)))\n",
        "\n",
        "    # Interpreted results\n",
        "    is_significant = p_value < alpha\n",
        "\n",
        "    # Effect size interpretation\n",
        "    if abs(h) < 0.2:\n",
        "        effect_size = 'negligible'\n",
        "    elif abs(h) < 0.5:\n",
        "        effect_size = 'small'\n",
        "    elif abs(h) < 0.8:\n",
        "        effect_size = 'medium'\n",
        "    else:\n",
        "        effect_size = 'large'\n",
        "\n",
        "    # Find minimum correct needed for significance\n",
        "    min_correct = find_min_successes(n_samples, random_prob, alpha)\n",
        "    min_accuracy_needed = min_correct / n_samples\n",
        "\n",
        "    results = {\n",
        "        'better_than_random': is_significant,\n",
        "        'p_value': p_value,\n",
        "        'observed_accuracy': observed_accuracy,\n",
        "        'effect_size': h,\n",
        "        'effect_size_interpretation': effect_size,\n",
        "        'n_samples': n_samples,\n",
        "        'n_correct': n_correct,\n",
        "        'min_accuracy_needed': min_accuracy_needed,\n",
        "        'random_baseline': random_prob,\n",
        "        'class_distribution': class_proportions\n",
        "    }\n",
        "\n",
        "    # Create distribution description\n",
        "    dist_desc = \"\\nClass Distribution:\\n\"\n",
        "    for class_label, prop in sorted(class_proportions.items()):\n",
        "        count = int(prop * n_samples)\n",
        "        dist_desc += f\"Class {class_label}: {prop:.3f} ({count}/{n_samples} samples)\\n\"\n",
        "\n",
        "    # Create human-readable summary\n",
        "    summary = f\"\"\"\n",
        "Performance Assessment:\n",
        "----------------------\n",
        "Observed Accuracy: {observed_accuracy:.3f} ({n_correct}/{n_samples})\n",
        "Random Baseline: {random_prob:.3f} (based on class distribution)\n",
        "P-value: {p_value:.4f}\n",
        "Effect Size (Cohen's h): {h:.3f} ({effect_size})\n",
        "\n",
        "{dist_desc}\n",
        "Statistical Significance:\n",
        "The model {'is' if is_significant else 'is not'} performing significantly better than the random baseline\n",
        "(p{' < ' if p_value < alpha else ' = '}{p_value:.4f})\n",
        "\n",
        "For {n_samples} samples, needed {min_accuracy_needed:.3f} accuracy ({min_correct} correct)\n",
        "for statistical significance at α={alpha}\n",
        "\"\"\"\n",
        "\n",
        "    results['summary'] = summary\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage integrated with the evaluation function\n",
        "def run_evaluation_with_stats(api_key: str, X: list, y: list, k: int,\n",
        "                            random_seed: Optional[int] = None,\n",
        "                            max_workers: int = 10,\n",
        "                            alpha: float = 0.05) -> Tuple[float, float, pd.DataFrame, Dict]:\n",
        "    \"\"\"\n",
        "    Run evaluation and statistical analysis.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (accuracy, kappa, results DataFrame, statistical results)\n",
        "    \"\"\"\n",
        "    # Run basic evaluation\n",
        "    accuracy, kappa, results_df = run_evaluation(api_key, X, y, k, random_seed, max_workers)\n",
        "\n",
        "    # Get valid predictions for statistical analysis\n",
        "    valid_mask = results_df['predicted_answer'] != -1\n",
        "    y_true_valid = results_df.loc[valid_mask, 'true_answer'].tolist()\n",
        "    y_pred_valid = results_df.loc[valid_mask, 'predicted_answer'].tolist()\n",
        "\n",
        "    # Run statistical analysis\n",
        "    stats_results = assess_performance(y_true_valid, y_pred_valid, alpha)\n",
        "\n",
        "    # Print statistical summary\n",
        "    print(stats_results['summary'])\n",
        "\n",
        "    return accuracy, kappa, results_df, stats_results\n",
        "\n"
      ],
      "metadata": {
        "id": "w5DLeYy2xwPn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Example usage\n",
        "k = 300\n",
        "random_seed = 42\n",
        "accuracy, kappa, results, stats = run_evaluation_with_stats(\n",
        "    api_key, X, y, k, random_seed\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28vtmU_vxyGW",
        "outputId": "8db8c72b-11ae-4af9-a32d-5fb738bacc28"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Results:\n",
            "Total samples: 300\n",
            "Valid predictions: 300\n",
            "Accuracy: 0.450\n",
            "Cohen's Kappa: 0.240\n",
            "\n",
            "Per-class accuracy:\n",
            "Class 0: 0.238\n",
            "Class 1: 0.837\n",
            "Class 2: 0.462\n",
            "Class 3: 0.135\n",
            "\n",
            "Performance Assessment:\n",
            "----------------------\n",
            "Observed Accuracy: 0.450 (135/300)\n",
            "Random Baseline: 0.258 (based on class distribution)\n",
            "P-value: 0.0000\n",
            "Effect Size (Cohen's h): 0.405 (small)\n",
            "\n",
            "\n",
            "Class Distribution:\n",
            "Class 0: 0.280 (84/300 samples)\n",
            "Class 1: 0.287 (86/300 samples)\n",
            "Class 2: 0.260 (78/300 samples)\n",
            "Class 3: 0.173 (52/300 samples)\n",
            "\n",
            "Statistical Significance:\n",
            "The model is performing significantly better than the random baseline\n",
            "(p < 0.0000)\n",
            "\n",
            "For 300 samples, needed 0.303 accuracy (91 correct)\n",
            "for statistical significance at α=0.05\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to production code for github..."
      ],
      "metadata": {
        "id": "rJk5HKMwtNgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import json\n",
        "import ast\n",
        "import re\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from collections import Counter\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    cohen_kappa_score\n",
        ")\n",
        "import goodfire\n",
        "\n",
        "# Logging configuration\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DataHandler:\n",
        "    \"\"\"Handles loading and preprocessing of medical evaluation data.\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir: str = \".cache/med_eval\"):\n",
        "        \"\"\"\n",
        "        Initialize DataHandler with cache directory.\n",
        "\n",
        "        Args:\n",
        "            cache_dir (str): Directory for caching downloaded data\n",
        "        \"\"\"\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.data_path = self.cache_dir / \"med_halt_data.json\"\n",
        "\n",
        "    def load_data(self) -> Tuple[List[str], List[int], List[str]]:\n",
        "        \"\"\"\n",
        "        Load data, using cache if available, otherwise download fresh.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[List[str], List[int], List[str]]: Processed prompts, labels, and subject names\n",
        "        \"\"\"\n",
        "        if self.data_path.exists():\n",
        "            logger.info(\"Loading data from cache...\")\n",
        "            return self._load_from_cache()\n",
        "\n",
        "        logger.info(\"Downloading fresh data...\")\n",
        "        return self._download_and_cache()\n",
        "\n",
        "    def _load_from_cache(self) -> Tuple[List[str], List[int], List[str]]:\n",
        "        \"\"\"Load processed data from cache.\"\"\"\n",
        "        try:\n",
        "            with open(self.data_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            return data['prompts'], data['labels'], data['subject_names']\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading cached data: {e}\")\n",
        "            logger.info(\"Falling back to fresh download...\")\n",
        "            return self._download_and_cache()\n",
        "\n",
        "    def _download_and_cache(self) -> Tuple[List[str], List[int], List[str]]:\n",
        "        \"\"\"Download fresh data, process it, and cache the results.\"\"\"\n",
        "        try:\n",
        "            dataset = load_dataset(\"openlifescienceai/Med-HALT\", \"reasoning_FCT\")\n",
        "            train_data = dataset['train']\n",
        "\n",
        "            prompts, labels, subject_names = self._process_data(train_data)\n",
        "\n",
        "            # Cache the processed data\n",
        "            cache_data = {\n",
        "                'prompts': prompts,\n",
        "                'labels': labels,\n",
        "                'subject_names': subject_names\n",
        "            }\n",
        "            with open(self.data_path, 'w') as f:\n",
        "                json.dump(cache_data, f)\n",
        "\n",
        "            return prompts, labels, subject_names\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error downloading/processing data: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _process_data(self, dataset) -> Tuple[List[str], List[int], List[str]]:\n",
        "        \"\"\"Process raw dataset into prompts, labels, and subject names.\"\"\"\n",
        "        prompts = []\n",
        "        labels = []\n",
        "        subject_names = []\n",
        "\n",
        "        for example in dataset:\n",
        "            prompt, label = self._create_prompt(example)\n",
        "            if prompt is not None:\n",
        "                prompts.append(prompt)\n",
        "                labels.append(label)\n",
        "                subject_names.append(example.get('subject_name', ''))\n",
        "\n",
        "        return prompts, labels, subject_names\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_prompt(example: Dict) -> Tuple[Optional[str], Optional[int]]:\n",
        "        \"\"\"Create a formatted prompt from a single example.\"\"\"\n",
        "        try:\n",
        "            introduction = (\"You are a medical expert and this is a multiple choice exam question. \"\n",
        "                          \"Please respond with the integer index of the CORRECT answer only; [0,1,2,3].\")\n",
        "\n",
        "            question = example['question']\n",
        "\n",
        "            # Parse options\n",
        "            if isinstance(example['options'], str):\n",
        "                options_dict = ast.literal_eval(example['options'])\n",
        "            else:\n",
        "                options_dict = example['options']\n",
        "\n",
        "            options_filtered = {k: v for k, v in options_dict.items() if k != 'correct answer'}\n",
        "            options_formatted = \"Options: \" + json.dumps(options_filtered)\n",
        "\n",
        "            prompt = f\"{introduction}\\n\\n{question}\\n\\n{options_formatted}\"\n",
        "            return prompt, example['correct_index']\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating prompt: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def filter_by_subject(self, prompts: List[str], labels: List[int],\n",
        "                        subject_names: List[str], subject_name: Optional[str] = None) -> Tuple[List[str], List[int]]:\n",
        "        \"\"\"Filter data by subject name.\"\"\"\n",
        "        if not subject_name:\n",
        "            return prompts, labels\n",
        "\n",
        "        subject_name = subject_name.lower()\n",
        "        filtered_indices = [i for i, name in enumerate(subject_names)\n",
        "                          if name and name.lower() == subject_name]\n",
        "\n",
        "        return ([prompts[i] for i in filtered_indices],\n",
        "                [labels[i] for i in filtered_indices])\n",
        "\n"
      ],
      "metadata": {
        "id": "wNT5a6T-d4Dv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LLMEvaluator:\n",
        "    \"\"\"Handles evaluation of LLM performance on medical questions.\"\"\"\n",
        "\n",
        "    def __init__(self, client, variant):\n",
        "        \"\"\"\n",
        "        Initialize evaluator with API credentials.\n",
        "        \"\"\"\n",
        "        self.variant = variant\n",
        "        self.client = client\n",
        "\n",
        "    def evaluate(self,\n",
        "                X: List[str],\n",
        "                y: List[int],\n",
        "                k: int,\n",
        "                random_seed: Optional[int] = None,\n",
        "                max_workers: int = 10) -> Tuple[float, float, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Evaluate model performance on a sample of questions.\n",
        "\n",
        "        Args:\n",
        "            X (List[str]): List of prompts\n",
        "            y (List[int]): List of correct answers\n",
        "            k (int): Number of samples to evaluate\n",
        "            random_seed (Optional[int]): Random seed for reproducibility\n",
        "            max_workers (int): Maximum number of concurrent API calls\n",
        "\n",
        "        Returns:\n",
        "            Tuple[float, float, pd.DataFrame]: Accuracy, kappa, and detailed results\n",
        "        \"\"\"\n",
        "        X_sample, y_sample, indices = self._sample_data(X, y, k, random_seed)\n",
        "        return self._evaluate_concurrent(X_sample, y_sample, max_workers)\n",
        "\n",
        "    def _sample_data(self,\n",
        "                    X: List[str],\n",
        "                    y: List[int],\n",
        "                    k: int,\n",
        "                    random_seed: Optional[int] = None) -> Tuple[List[str], List[int], List[int]]:\n",
        "        \"\"\"Randomly sample k items from data.\"\"\"\n",
        "        if random_seed is not None:\n",
        "            random.seed(random_seed)\n",
        "\n",
        "        n = len(X)\n",
        "        indices = random.sample(range(n), k)\n",
        "        X_sampled = [X[i] for i in indices]\n",
        "        y_sampled = [y[i] for i in indices]\n",
        "\n",
        "        return X_sampled, y_sampled, indices\n",
        "\n",
        "    def _evaluate_concurrent(self,\n",
        "                           X_sample: List[str],\n",
        "                           y_sample: List[int],\n",
        "                           max_workers: int) -> Tuple[float, float, pd.DataFrame]:\n",
        "        \"\"\"Evaluate samples concurrently.\"\"\"\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = [\n",
        "                executor.submit(self._get_model_response, prompt)\n",
        "                for prompt in X_sample\n",
        "            ]\n",
        "\n",
        "            y_pred = []\n",
        "            for future in futures:\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    y_pred.append(result if result is not None else -1)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error in API call: {e}\")\n",
        "                    y_pred.append(-1)\n",
        "\n",
        "        return self._calculate_metrics(X_sample, y_sample, y_pred)\n",
        "\n",
        "    def _get_model_response(self, prompt: str) -> Optional[int]:\n",
        "        \"\"\"Get integer response from model.\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                model=self.variant,\n",
        "                stream=False,\n",
        "                max_completion_tokens=50\n",
        "            )\n",
        "            content = response.choices[0].message['content']\n",
        "            return self._extract_first_valid_integer(content)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting model response: {e}\")\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def _extract_first_valid_integer(text: str) -> int:\n",
        "        \"\"\"Extract first valid integer from text.\"\"\"\n",
        "        pattern = r'\\d+'\n",
        "        match = re.search(pattern, text)\n",
        "\n",
        "        if match:\n",
        "            number = int(match.group())\n",
        "            if number in [0, 1, 2, 3]:\n",
        "                return number\n",
        "        return -1\n",
        "\n",
        "    def _calculate_metrics(self,\n",
        "                         X_sample: List[str],\n",
        "                         y_sample: List[int],\n",
        "                         y_pred: List[int]) -> Tuple[float, float, pd.DataFrame]:\n",
        "        \"\"\"Calculate evaluation metrics.\"\"\"\n",
        "        valid_indices = [i for i, pred in enumerate(y_pred) if pred != -1]\n",
        "        y_pred_valid = [y_pred[i] for i in valid_indices]\n",
        "        y_sample_valid = [y_sample[i] for i in valid_indices]\n",
        "\n",
        "        if len(valid_indices) > 0:\n",
        "            accuracy = accuracy_score(y_sample_valid, y_pred_valid)\n",
        "            kappa = cohen_kappa_score(y_sample_valid, y_pred_valid)\n",
        "            conf_matrix = confusion_matrix(y_sample_valid, y_pred_valid, labels=[0,1,2,3])\n",
        "            per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "        else:\n",
        "            accuracy = 0\n",
        "            kappa = 0\n",
        "            per_class_accuracy = [0, 0, 0, 0]\n",
        "\n",
        "        results_df = pd.DataFrame({\n",
        "            'prompt': X_sample,\n",
        "            'true_answer': y_sample,\n",
        "            'predicted_answer': y_pred,\n",
        "            'correct': [pred == true for pred, true in zip(y_pred, y_sample)]\n",
        "        })\n",
        "\n",
        "        # Log results\n",
        "        logger.info(f\"\\nEvaluation Results:\")\n",
        "        logger.info(f\"Total samples: {len(X_sample)}\")\n",
        "        logger.info(f\"Valid predictions: {len(valid_indices)}\")\n",
        "        logger.info(f\"Accuracy: {accuracy:.3f}\")\n",
        "        logger.info(f\"Cohen's Kappa: {kappa:.3f}\")\n",
        "        logger.info(\"\\nPer-class accuracy:\")\n",
        "        for i, acc in enumerate(per_class_accuracy):\n",
        "            logger.info(f\"Class {i}: {acc:.3f}\")\n",
        "\n",
        "        return accuracy, kappa, results_df"
      ],
      "metadata": {
        "id": "qsnwYU4EthUQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class StatisticalAnalyzer:\n",
        "    \"\"\"Handles statistical analysis of model performance.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def analyze(y_true: List[int],\n",
        "                y_pred: List[int],\n",
        "                alpha: float = 0.05) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyze if model performance is significantly better than random.\n",
        "\n",
        "        Args:\n",
        "            y_true (List[int]): True labels\n",
        "            y_pred (List[int]): Predicted labels\n",
        "            alpha (float): Significance level\n",
        "\n",
        "        Returns:\n",
        "            Dict: Statistical analysis results\n",
        "        \"\"\"\n",
        "        # Basic metrics\n",
        "        n_samples = len(y_true)\n",
        "        n_correct = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
        "        observed_accuracy = n_correct / n_samples\n",
        "\n",
        "        # Calculate baselines\n",
        "        random_prob, class_proportions = StatisticalAnalyzer._calculate_random_baseline(y_true)\n",
        "\n",
        "        # Statistical test - using scipy.stats instead of stats\n",
        "        p_value = scipy.stats.binomtest(n_correct, n_samples, p=random_prob,\n",
        "                                alternative='greater').pvalue\n",
        "\n",
        "        # Effect size\n",
        "        h = 2 * (np.arcsin(np.sqrt(observed_accuracy)) -\n",
        "                np.arcsin(np.sqrt(random_prob)))\n",
        "\n",
        "        # Interpret results\n",
        "        is_significant = p_value < alpha\n",
        "        effect_size = StatisticalAnalyzer._interpret_effect_size(h)\n",
        "\n",
        "        # Minimum needed for significance\n",
        "        min_successes = StatisticalAnalyzer._find_min_successes(n_samples, random_prob, alpha)\n",
        "        min_accuracy = min_successes / n_samples\n",
        "\n",
        "        results = {\n",
        "            'better_than_random': is_significant,\n",
        "            'p_value': p_value,\n",
        "            'observed_accuracy': observed_accuracy,\n",
        "            'effect_size': h,\n",
        "            'effect_size_interpretation': effect_size,\n",
        "            'n_samples': n_samples,\n",
        "            'n_correct': n_correct,\n",
        "            'min_correct': min_successes,\n",
        "            'min_accuracy_needed': min_accuracy,\n",
        "            'random_baseline': random_prob,\n",
        "            'class_distribution': class_proportions\n",
        "        }\n",
        "\n",
        "        # Add human-readable summary\n",
        "        results['summary'] = StatisticalAnalyzer._create_summary(results, alpha)\n",
        "\n",
        "        return results\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_random_baseline(y_true: List[int]) -> Tuple[float, Dict[int, float]]:\n",
        "        \"\"\"Calculate random baseline accuracy based on class distribution.\"\"\"\n",
        "        class_counts = Counter(y_true)\n",
        "        total = len(y_true)\n",
        "\n",
        "        proportions = {k: v/total for k, v in class_counts.items()}\n",
        "        random_baseline = sum(p*p for p in proportions.values())\n",
        "\n",
        "        return random_baseline, proportions\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_min_successes(n: int, p: float, alpha: float) -> int:\n",
        "        \"\"\"Find minimum successes needed for significance.\"\"\"\n",
        "        left, right = int(n * p), n\n",
        "\n",
        "        while left <= right:\n",
        "            mid = (left + right) // 2\n",
        "            p_value = scipy.stats.binomtest(mid, n, p, alternative='greater').pvalue\n",
        "\n",
        "            if p_value <= alpha:\n",
        "                if mid == left or scipy.stats.binomtest(mid - 1, n, p,\n",
        "                                                alternative='greater').pvalue > alpha:\n",
        "                    return mid\n",
        "                right = mid - 1\n",
        "            else:\n",
        "                left = mid + 1\n",
        "\n",
        "        return left\n",
        "\n",
        "    @staticmethod\n",
        "    def _interpret_effect_size(h: float) -> str:\n",
        "        \"\"\"Interpret Cohen's h effect size.\"\"\"\n",
        "        if abs(h) < 0.2:\n",
        "            return 'negligible'\n",
        "        elif abs(h) < 0.5:\n",
        "            return 'small'\n",
        "        elif abs(h) < 0.8:\n",
        "            return 'medium'\n",
        "        else:\n",
        "            return 'large'\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_summary(results: Dict, alpha: float) -> str:\n",
        "        \"\"\"Create human-readable summary of results.\"\"\"\n",
        "        dist_desc = \"\\nClass Distribution:\\n\"\n",
        "        for class_label, prop in sorted(results['class_distribution'].items()):\n",
        "            count = int(prop * results['n_samples'])\n",
        "            dist_desc += (f\"Class {class_label}: {prop:.3f} \"\n",
        "                        f\"({count}/{results['n_samples']} samples)\\n\")\n",
        "\n",
        "        return f\"\"\"\n",
        "Performance Assessment:\n",
        "----------------------\n",
        "Observed Accuracy: {results['observed_accuracy']:.3f} ({results['n_correct']}/{results['n_samples']})\n",
        "Random Baseline: {results['random_baseline']:.3f} (based on class distribution)\n",
        "P-value: {results['p_value']:.4f}\n",
        "Effect Size (Cohen's h): {results['effect_size']:.3f} ({results['effect_size_interpretation']})\n",
        "\n",
        "{dist_desc}\n",
        "Statistical Significance:\n",
        "The model {'is' if results['better_than_random'] else 'is not'} performing significantly better than the random baseline\n",
        "(p{' < ' if results['p_value'] < alpha else ' = '}{results['p_value']:.4f})\n",
        "\n",
        "For {results['n_samples']} samples, needed {results['min_accuracy_needed']:.3f} accuracy ({results['min_correct']} correct)\n",
        "for statistical significance at α={alpha}\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "8jjLRIiztWoL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MedicalLLMEvaluator:\n",
        "  \"\"\"Main interface for evaluating LLM performance on medical questions.\"\"\"\n",
        "\n",
        "  def __init__(self, client, variant, cache_dir = \".cache/med_eval\"):\n",
        "      \"\"\"\n",
        "      Initialize the medical LLM evaluator.\n",
        "\n",
        "      Args:\n",
        "          api_key (str): Goodfire API key\n",
        "          model_name (str): Name of the model to use\n",
        "          cache_dir (str): Directory for caching downloaded data\n",
        "      \"\"\"\n",
        "      self.data_handler = DataHandler(cache_dir)\n",
        "      self.evaluator = LLMEvaluator(client, variant)\n",
        "      self.analyzer = StatisticalAnalyzer()\n",
        "\n",
        "  def run_evaluation(self,\n",
        "                    k: int,\n",
        "                    subject_name: Optional[str] = None,\n",
        "                    random_seed: Optional[int] = None,\n",
        "                    max_workers: int = 10,\n",
        "                    alpha: float = 0.05) -> Tuple[float, float, pd.DataFrame, Dict]:\n",
        "      \"\"\"Run complete evaluation including statistical analysis.\"\"\"\n",
        "      # Load data\n",
        "      prompts, labels, subject_names = self.data_handler.load_data()\n",
        "\n",
        "      # Filter by subject_name if provided\n",
        "      X, y = self.data_handler.filter_by_subject(prompts, labels, subject_names, subject_name)\n",
        "\n",
        "      if not X:  # Check if we have any data after filtering\n",
        "          raise ValueError(f\"No data found for subject: {subject_name}\")\n",
        "\n",
        "      if len(X) < k:\n",
        "          raise ValueError(f\"Not enough data for subject: {subject_name}\")\n",
        "\n",
        "      # Rest remains unchanged\n",
        "      accuracy, kappa, results_df = self.evaluator.evaluate(\n",
        "          X, y, k, random_seed, max_workers\n",
        "      )\n",
        "\n",
        "      valid_mask = results_df['predicted_answer'] != -1\n",
        "      y_true_valid = results_df.loc[valid_mask, 'true_answer'].tolist()\n",
        "      y_pred_valid = results_df.loc[valid_mask, 'predicted_answer'].tolist()\n",
        "\n",
        "      stats_results = self.analyzer.analyze(y_true_valid, y_pred_valid, alpha)\n",
        "\n",
        "      logger.info(stats_results['summary'])\n",
        "\n",
        "      return accuracy, kappa, results_df, stats_results\n",
        "\n"
      ],
      "metadata": {
        "id": "lp7-VTU2tagA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE USAGE\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import goodfire\n",
        "\n",
        "# Get API key\n",
        "api_key = userdata.get('GOODFIRE_API_KEY')\n",
        "\n",
        "# Initialize Goodfire\n",
        "client  = goodfire.Client(api_key)\n",
        "variant = goodfire.Variant(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = MedicalLLMEvaluator(client, variant)\n",
        "evaluator\n",
        "\n",
        "# Run evaluation\n",
        "accuracy, kappa, results, stats = evaluator.run_evaluation(\n",
        "    k=100              # number of samples\n",
        "    random_seed=42,    # for reproducibility\n",
        "    max_workers=10,    # concurrent API calls\n",
        "    subject_name=None  # enter subject_name, eg 'psychology', or leave as None\n",
        ")\n",
        "\n",
        "# Results are already logged by the evaluator\n",
        "# You can also access them programmatically:\n",
        "print(f\"\\nAccuracy: {accuracy:.3f}\")\n",
        "print(f\"Kappa: {kappa:.3f}\")\n",
        "print(\"\\nDetailed results available in results DataFrame\")\n",
        "print(\"\\nStatistical analysis results available in stats dictionary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4RFJ1zud6NM",
        "outputId": "f5eb2a8b-ae46-4c15-ca0f-4ea711de8f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.600\n",
            "Kappa: 0.394\n",
            "\n",
            "Detailed results available in results DataFrame\n",
            "\n",
            "Statistical analysis results available in stats dictionary\n"
          ]
        }
      ]
    }
  ]
}